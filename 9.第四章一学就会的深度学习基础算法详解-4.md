
对于此公式中的\(W_{ij}^{l}\)和\(\delta_{j}^{l + 1}\)以及所需要计算的目标\(\delta^{l}\)已经做了较为详尽的解释。但是对\(f'(a_{i}^{l})\)却一直没有做出介绍。

回到前面生物神经元的图示，传递进来的电信号通过神经元进行传递，由于神经元的突触强弱是有一定的敏感度的，因此只会对超过一定范围的信号进行反馈，即这个电信号必须大于某个阈值，神经元才会被激活引起后续的传递。

在训练模型中同样需要设置神经元的阈值，即神经元被激活的频率用于传递相应的信息，模型中这种能够确定是否为当前神经元节点的函数被称为激活函数，如图4 - 21所示。

![image](https://github.com/user-attachments/assets/4d9195f7-6861-417a-8d64-4d1bfd272216)


图4 - 21 激活函数示意图

激活函数代表生物神经元接收到的信号的强度，目前应用较广的是Sigmoid函数。因为其在运行过程中只接收一个值，所以输出也是一个经过公式计算后的的值，且其输出值的取值范围为0 - 1。

\[ y = \frac{1}{1 + e^{-x}} \]

Sigmoid激活函数图如图4 - 22所示。

图4 - 22 Sigmoid激活函数图

![image](https://github.com/user-attachments/assets/07b0a2cd-49ed-4802-b987-189e1afa876d)



其倒函数的求法也较为简单，即：

![image](https://github.com/user-attachments/assets/cdc0e0b0-2a0a-47fb-b139-e69ed0fe3308)


\[ y' = \frac{e^{-x}}{(1 + e^{-x})^{2}} \]

换一种表示方式为：

\[ f(x)' = f(x)\times(1 - f(x)) \]

Sigmoid函数可以将任意实值映射到0 - 1。对于较大值的负数，Sigmoid函数将其映射成0，而对于较大值的正数Sigmoid函数将其映射成1。

顺带讲一下，Sigmoid函数在神经网络模型中占据了很长时间的统治地位，但是目前已经不常使用，主要原因是其非常容易区域饱和，当输入开始非常大或者非常小的时候，Sigmoid会产生一个平缓区域，其中的梯度值几乎为0，而这又会造成梯度传播过程中产生接近0的传播梯度，这样在后续的传播中会造成梯度消散的现象，因此并不适合现代的神经网络模型使用。

除此之外，近年来涌现出了大量新的激活函数模型，例如Maxout、Tanh和ReLU模型，这些模型解决了传统的Sigmoid模型在更深度的神经网络上所产生的各种不良影响。

##### 4.3.5 反馈神经网络原理的Python实现

![image](https://github.com/user-attachments/assets/0ad6c3d6-f393-4918-880c-de5d4f600600)

本小节将使用Python语言实现神经网络的反馈算法。经过前面的讲解，读者对神经网络的算法和描述应该有了一定的理解，本小节将使用Python代码来实现一个自己的反馈神经网络。

为了简单起见，这里的神经网络被设置成3层，即只有一个输入层、一个隐藏层以及一个最终的输出层。

（1）**确定辅助函数**：

```python
def rand(a, b):
    return (b - a) * random.random() + a
def make_matrix(m,n,fill=0.0):
    mat = []
    for i in range(m):
        mat.append([fill] * n)
    return mat
def sigmoid(x):
    return 1.0 / (1.0 + math.exp(-x))
def sigmoid_derivate(x):
    return x * (1 - x)
```

代码首先定义了随机值，使用`random`包中的`random`函数生成了一系列随机数，之后的`make_matrix`函数生成了相对应的矩阵。`sigmoid`和`sigmoid_derivate`分别是激活函数和激活函数的导函数。这也是前文所定义的内容。

（2）**进入BP神经网络类的正式定义**，类的定义需要对数据内容进行设定。

```python
class BPNeuralNetwork:
    def __init__(self):
        self.input_n = 0
        self.hidden_n = 0
        self.output_n = 0
        self.input_cells = []
        self.hidden_cells = []
        self.output_cells = []
        self.input_weights = []
        self.output_weights = []
```

`init`函数的作用是对神经网络参数的初始化，即在其中设置了输入层、隐藏层以及输出层中节点的个数；各个`cell`是各个层中节点的数值；`weights`代表各个层的权重。

（3）**使用`setup`函数对`init`函数中设定的数据进行初始化**。

```python
def setup(self,ni,nh,no):
    self.input_n = ni + 1
    self.hidden_n = nh
    self.output_n = no
    self.input_cells = [1.0] * self.input_n
    self.hidden_cells = [1.0] * self.hidden_n
    self.output_cells = [1.0] * self.output_n
    self.input_weights = make_matrix(self.input_n,self.hidden_n)
    self.output_weights = make_matrix(self.hidden_n,self.output_n)
    # random activate
    for i in range(self.input_n):
        for h in range(self.hidden_n):
            self.input_weights[i][h] = rand(-0.2, 0.2)
    for h in range(self.hidden_n):
        for o in range(self.output_n):
            self.output_weights[h][o] = rand(-2.0, 2.0)
```

需要注意，输入层节点的个数被设置成`ni+1`，这是由于其中包含`bias`偏置数；各个节点与1.0相乘被初始节点的数值；各个层的权重值根据输入层、隐藏层以及输出层中节点的个数被初始化并被赋值。

（4）**定义完各个层的数目后，进入正式的神经网络内容的定义。对神经网络的前向计算如下**：

```python
def predict(self,inputs):
    for i in range(self.input_n - 1):
        self.input_cells[i] = inputs[i]
    for j in range(self.hidden_n):
        total = 0.0
        for i in range(self.input_n):
            total += self.input_cells[i] * self.input_weights[i][j]
        self.hidden_cells[j] = sigmoid(total)
    for k in range(self.output_n):
        total = 0.0
        for j in range(self.hidden_n):
            total += self.hidden_cells[j] * self.output_weights[j][k]
        self.output_cells[k] = sigmoid(total)
    return self.output_cells[:]
```

以上代码将数据输入函数中，通过隐藏层和输出层的计算，最终以数组的形式输出。案例的完整代码如下。

【程序4 - 3】

```python
import numpy as np
import math
import random

def rand(a, b):
    return (b - a) * random.random() + a

def make_matrix(m,n,fill=0.0):
    mat = []
    for i in range(m):
        mat.append([fill] * n)
    return mat

def sigmoid(x):
    return 1.0 / (1.0 + math.exp(-x))

def sigmoid_derivate(x):
    return x * (1 - x)

class BPNeuralNetwork:
    def __init__(self):
        self.input_n = 0
        self.hidden_n = 0
        self.output_n = 0
        self.input_cells = []
        self.hidden_cells = []
        self.output_cells = []
        self.input_weights = []
        self.output_weights = []

    def setup(self,ni,nh,no):
        self.input_n = ni + 1
        self.hidden_n = nh
        self.output_n = no
        self.input_cells = [1.0] * self.input_n
        self.hidden_cells = [1.0] * self.hidden_n
        self.output_cells = [1.0] * self.output_n
        self.input_weights = make_matrix(self.input_n,self.hidden_n)
        self.output_weights = make_matrix(self.hidden_n,self.output_n)
        # random activate
        for i in range(self.input_n):
            for h in range(self.hidden_n):
                self.input_weights[i][h] = rand(-0.2, 0.2)
        for h in range(self.hidden_n):
            for o in range(self.output_n):
                self.output_weights[h][o] = rand(-2.0, 2.0)

    def predict(self,inputs):
        for i in range(self.input_n - 1):
            self.input_cells[i] = inputs[i]
        for j in range(self.hidden_n):
            total = 0.0
            for i in range(self.input_n):
                total += self.input_cells[i] * self.input_weights[i][j]
            self.hidden_cells[j] = sigmoid(total)
        for k in range(self.output_n):
            total = 0.0
            for j in range(self.hidden_n):
                total += self.hidden_cells[j] * self.output_weights[j][k]
            self.output_cells[k] = sigmoid(total)
        return self.output_cells[:]

    def back_propagate(self,case,label,learn):
        self.predict(case)
        #计算输出层的误差
        output_deltas = [0.0] * self.output_n
        for k in range(self.output_n):
            error = label[k] - self.output_cells[k]
            output_deltas[k] = sigmoid_derivate(self.output_cells[k]) * error
        #计算隐藏层的误差
        hidden_deltas = [0.0] * self.hidden_n
        for j in range(self.hidden_n):
            error = 0.0
            for k in range(self.output_n):
                error += output_deltas[k] * self.output_weights[j][k]
            hidden_deltas[j] = sigmoid_derivate(self.hidden_cells[j]) * error
        #更新输出层的权重
        for j in range(self.hidden_n):
            for k in range(self.output_n):
                self.output_weights[j][k] += learn * output_deltas[k] * self.hidden_cells[j]
        #更新隐藏层的权重
        for i in range(self.input_n):
            for j in range(self.hidden_n):
                self.input_weights[i][j] += learn * hidden_deltas[j] * self.input_cells[i]
        error = 0.0
        for o in range(len(label)):
            error += 0.5 * (label[o] - self.output_cells[o]) ** 2
        return error

    def train(self,cases,labels,limit = 100,learn = 0.05):
        for i in range(limit):
            error = 0
            for i in range(len(cases)):
                label = labels[i]
                case = cases[i]
                error += self.back_propagate(case, label, learn)
        pass

    def test(self):
        cases = [
            [0, 0],
            [0, 1],
            [1, 0],
            [1, 1]
        ]
        labels = [[0], [1], [1], [0]]
        self.setup(2, 5, 1)
        self.train(cases, labels, 10000, 0.05)
        for case in cases:
            print(self.predict(case))

if __name__ == '__main__':
    nn = BPNeuralNetwork()
    nn.test()
```
#### 4.4 本章小结

本章完整介绍了BP神经网络的原理和实现。这是深度学习最基础的内容，可以说深度学习所有的后续发展都是基于对BP神经网络的修正而来的。

在后续章节中，将带领读者了解更多的神经网络。 
