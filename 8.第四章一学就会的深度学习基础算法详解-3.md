
\[ J(\theta)=\frac{1}{2m}\sum_{1}^{m}(h_{\theta}(x)-y)^{2} \]

![image](https://github.com/user-attachments/assets/d2c4037a-1664-4ed4-9e5c-b578e4f2a7ac)


其中参数解释如下：

- \(m\)是数据点总数。

- \(\frac{1}{2}\)是一个常量，在求梯度的时候，二次方微分后的结果与\(\frac{1}{2}\)抵消了，自然就没有多余的常数系数了，方便后续的计算，同时不会对结果产生影响。 

- \(y\)是数据集中每个点的真实\(y\)坐标的值。 

- 其中\(h_{\theta}(x)\)为预测函数，形式如下：

\[ h_{\theta}(x)=\theta_{0}+\theta_{1}x \]

根据每个输入\(x\)，都有一个经过参数计算后的预测值输出。

\(h_{\theta}(x)\)的Python实现如下：

```python
h_pred = np.dot(x,theta)
```

其中\(x\)是输入的维度为\([-1,2]\)的二维向量，\(-1\)的意思是维度不定。这里使用了一个技巧，即将\(h_{\theta}(x)\)的公式转化成矩阵相乘的形式，而\(\text{theta}\)是一个\([2,1]\)维度的二维向量。

依照最小二乘法实现的Python代码如下：

![image](https://github.com/user-attachments/assets/43cab98e-8f18-4700-8de3-8673d0c0bf43)


```python
def error_function(theta,x,y):
    h_pred = np.dot(x,theta)
    j_theta = (1./2*m) * np.dot(np.transpose(h_pred), h_pred)
    return j_theta
```

这里\(\text{j_theta}\)的实现同样是将原始公式转化成矩阵计算，即：

\[ (h_{\theta}(x)-y)^{2}=(h_{\theta}(x)-y)^{\mathrm{T}}\times(h_{\theta}(x)-y) \]

下面分析一下最小二乘法的公式\(J(\theta)\)，此时如果求\(J(\theta)\)的梯度，则需要对其中涉及的两个参数\(\theta_{0}\)和\(\theta_{1}\)进行微分：

\[ \nabla J(\theta)=\left[\frac{\partial J}{\partial \theta_{0}},\frac{\partial J}{\partial \theta_{1}}\right] \]

下面分别对这两个参数的求导公式进行求导：

\[ \frac{\partial J}{\partial \theta_{0}}=\frac{1}{2m}\times2\sum_{1}^{m}(h_{\theta}(x)-y)\times\frac{\partial (h_{\theta}(x))}{\partial \theta_{0}}=\frac{1}{m}\sum_{1}^{m}(h_{\theta}(x)-y) \]

\[ \frac{\partial J}{\partial \theta_{1}}=\frac{1}{2m}\times2\sum_{1}^{m}(h_{\theta}(x)-y)\times\frac{\partial (h_{\theta}(x))}{\partial \theta_{1}}=\frac{1}{m}\sum_{1}^{m}(h_{\theta}(x)-y)\times x \]

此时，将分开求导的参数合并可得新的公式：

\[ \frac{\partial J}{\partial \theta}=\frac{\partial J}{\partial \theta_{0}}+\frac{\partial J}{\partial \theta_{1}}=\frac{1}{m}\sum_{1}^{m}(h_{\theta}(x)-y)+\frac{1}{m}\sum_{1}^{m}(h_{\theta}(x)-y)\times x=\frac{1}{m}\sum_{1}^{m}(h_{\theta}(x)-y)\times(1 + x) \]

此时，公式最右边的常数\(1\)可以被去掉，公式变为：

\[ \frac{\partial J}{\partial \theta}=\frac{1}{m}\times(x)\times\sum_{1}^{m}(h_{\theta}(x)-y) \]

此时，依旧采用矩阵相乘的方式，使用矩阵相乘表示的公式为：

\[ \frac{\partial J}{\partial \theta}=\frac{1}{m}\times(x)^{\mathrm{T}}\times(h_{\theta}(x)-y) \]

这里\((x)^{\mathrm{T}}\times(h_{\theta}(x)-y)\)已经转换为矩阵相乘的表示形式。使用Python表示如下：

```python
def gradient_function(theta, X, y):
    h_pred = np.dot(X, theta) - y
    return (1./m) * np.dot(np.transpose(X), h_pred)
```

如果读者对`np.dot(np.transpose(X), h_pred)`理解有难度，可以将公式使用逐个\(x\)值的形式列出来看看如何，这里就不罗列了。

最后是梯度下降的Python实现，代码如下：

![image](https://github.com/user-attachments/assets/d2e9089f-731f-46b9-b41d-814a2852f896)


```python

def gradient_descent(X, y, alpha):
    theta = np.array([1, 1]).reshape(2, 1) #[2,1] 这里的theta是参数
    gradient = gradient_function(theta,X,y)
    for i in range(17):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, y)
    return theta
```

或者使用如下代码：

```python
def gradient_descent(X, y, alpha):
    theta = np.array([1, 1]).reshape(2, 1) #[2,1] 这里的theta是参数
    gradient = gradient_function(theta,X,y)
    while not np.all(np.absolute(gradient) <= 1e-4):#采用abs是因为gradient计算的是负梯度
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, y)
        print(theta)
    return theta
```

这两组程序段的区别在于，第一个代码段是固定循环次数，可能会造成欠下降或者过下降，而第二个代码段使用的是数值判定，可以设定阈值或者停止条件。

全部代码如下：

```python
import numpy as np
m = 20
# 生成数据集x，此时的数据集x是一个二维矩阵
x0 = np.ones((m, 1))
x1 = np.arange(1, m+1).reshape(m, 1)
x = np.hstack((x0, x1)) #【20,2】
y = np.array([
    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,
    11, 13, 13, 16, 17, 18, 17, 19, 21
]).reshape(m, 1)
alpha = 0.01
#这里的theta是一个[2,1]大小的矩阵，用来与输入x进行计算，获得计算的预测值y_pred，而y_pred是与y算的误差
def error_function(theta,x,y):
    h_pred = np.dot(x,theta)
    j_theta = (1./2*m) * np.dot(np.transpose(h_pred), h_pred)
    return j_theta
def gradient_function(theta, X, y):
    h_pred = np.dot(X, theta) - y
    return (1./m) * np.dot(np.transpose(X), h_pred)
def gradient_descent(X, y, alpha):
    theta = np.array([1, 1]).reshape(2, 1) #[2,1] 这里的theta是参数
    gradient = gradient_function(theta,X,y)
    while not np.all(np.absolute(gradient) <= 1e-6):
        theta = theta - alpha * gradient
        gradient = gradient_function(theta, X, y)
    return theta
theta = gradient_descent(x, y, alpha)
print('optimal:', theta)
print('error function:', error_function(theta, x, y)[0,0])
```

打印结果和拟合曲线请读者自行完成。

现在请读者回到前面的道士下山这个问题，这个下山的道士实际上就代表了反向传播算法，而要寻找的下山路径其实就代表着算法中一直在寻找的参数\(\theta\)，山上当前点最陡峭的方向实际上就是代价函数在这个点的梯度方向，场景中观察最陡峭方向所用的工具就是微分。

#### 4.3 反馈神经网络反向传播算法介绍

反向传播算法是神经网络的核心与精髓，在神经网络算法中拥有举足轻重的地位。

用通俗的话说，反向传播算法就是复合函数的链式求导法则的一个强大应用，而且实际上的应用比理论上的推导强大得多。本节将主要介绍反馈神经网络反向传播链式法则以及公式的推导，虽然整体过程比较简单，但这却是整个深度学习神经网络的理论基础。

##### 4.3.1 深度学习基础

机器学习在理论上可以看作是统计学在计算机科学上的一个应用。在统计学上，一个非常重要的内容就是拟合和预测，即基于以往的数据建立光滑的曲线模型来实现数据结果与数据变量的对应关系。

深度学习是统计学的应用，同样是为了寻找结果与影响因素的一一对应关系。只不过样本点由狭义的\(x\)和\(y\)扩展到向量、矩阵等广义的对应点。此时，由于数据变得复杂，对应关系模型的复杂度也随之增加，而不能使用一个简单的函数表达。

数学上通过建立复杂的高次多元函数解决复杂模型拟合的问题，但是大多数情况都会失败，因为过于复杂的函数式是无法求解的，也就是无法获取其公式。

基于前人的研究，科研工作人员发现可以通过神经网络来表示这样的一一对应关系，而神经网络本质就是一个多元复合函数，通过增加神经网络的层次和神经单元可以更好地表达函数的复合关系。

![image](https://github.com/user-attachments/assets/6cec82dd-d823-4260-b555-17c76b40edde)


图4 - 13是多层神经网络的图像表达方式，通过设置输入层、隐藏层与输出层可以形成多元函数用于求解相关问题。

图4 - 13 多层神经网络的表示

通过数学表达式将多层神经网络模型表达出来，公式如下：

\[ a_{1}=f(w_{11}\times x_{1}+w_{12}\times x_{2}+w_{13}\times x_{3}+b_{1}) \]

\[ a_{2}=f(w_{21}\times x_{1}+w_{22}\times x_{2}+w_{23}\times x_{3}+b_{2}) \]

\[ a_{3}=f(w_{31}\times x_{1}+w_{32}\times x_{2}+w_{33}\times x_{3}+b_{3}) \]

\[ h(x)=f(w_{11}\times a_{1}+w_{12}\times a_{2}+w_{13}\times a_{3}+b_{1}) \]

其中\(x\)是输入数值，\(w\)是相邻神经元之间的权重，也就是神经网络在训练过程中需要学习的参数。与线性回归类似，神经网络学习同样需要一个“损失函数”，即训练目标通过调整每个权重值\(w\)来使得损失函数最小。前面在讲解梯度下降算法的时候已经讲过，如果权重过大或者指数过大，直接求解系数是一件不可能的事情，因此梯度下降算法是求解权重问题的比较好的方法。

![image](https://github.com/user-attachments/assets/3d6a1b34-0a52-4527-8c4a-1e59a6918c74)


##### 4.3.2 链式求导法则

在前面介绍梯度下降算法时，没有对其背后的原理做出详细介绍。实际上，梯度下降算法就是链式法则的一个具体应用，如果把前面公式中的损失函数以向量的形式表示为：
\[ h(x)=f(w_{11},w_{12},w_{13},w_{14},\cdots,w_{l}) \]
那么其梯度向量为： 

![image](https://github.com/user-attachments/assets/f4568012-4f78-4525-a049-83212898f73d)

![image](https://github.com/user-attachments/assets/e1727c63-95ef-4d17-9d15-5f5ed183f839)

\[ \nabla h = \frac{\partial f}{\partial W_{11}}+\frac{\partial f}{\partial W_{12}}+\cdots+\frac{\partial f}{\partial W_{l}} \]

可以看到，其实所谓的梯度向量就是求出函数在每个向量上的偏导数之和。这也是链式法则擅长解决的问题。

下面以\(e=(a + b)\times(b + 1)\)，其中\(a = 2\)、\(b = 1\)为例，计算其偏导数，如图4 - 14所示。

图4 - 14 \(e=(a + b)\times(b + 1)\)示意图

本例中为了求得最终值\(e\)对各个点的梯度，需要将各个点与\(e\)联系在一起，例如期望求得\(e\)对输入点\(a\)的梯度，则只需要求得：

\[ \frac{\partial e}{\partial a}=\frac{\partial e}{\partial c}\times\frac{\partial c}{\partial a} \]

这样就把\(e\)与\(a\)的梯度联系在一起了，同理可得：

\[ \frac{\partial e}{\partial b}=\frac{\partial e}{\partial c}\times\frac{\partial c}{\partial b}+\frac{\partial e}{\partial d}\times\frac{\partial d}{\partial b} \]

用图表示如图4 - 15所示。

这样做的好处是显而易见的，求\(e\)对\(a\)的偏导数只要建立一个\(e\)到\(a\)的路径即可，图4 - 15中经过\(c\)，那么通过相关的求导链接就可以得到所需要的值。对于求\(e\)对\(b\)的偏导数，也只需要建立所有\(e\)到\(b\)路径中的求导路径，从而获得需要的值。

图4 - 15 链式法则的应用

![image](https://github.com/user-attachments/assets/bf8339b6-ede0-473e-82e1-71559f02b7aa)


##### 4.3.3 反馈神经网络的原理与公式推导

在求导过程中，可能有读者已经注意到，如果拉长了求导过程或者增加了其中的单元，就会大大增加其中的计算过程，即很多偏导数的求导过程会被反复计算，因此在实际应用中，对于权值达到十万甚至百万的神经网络来说，这样的重复冗余所导致的计算量是很大的。

同样是为了求得对权重的更新，反馈神经网络算法将训练误差\(E\)看作以权重向量每个元素为变量的高维函数，通过不断更新权重寻找训练误差的最低点，按误差函数梯度下降的方向更新权值。

提示：反馈神经网络算法的具体计算公式在本节后半部分进行推导。

首先求得最后的输出层与真实值之间的差距，如图4 - 16所示。

![image](https://github.com/user-attachments/assets/441326ef-0ac4-44e4-b6b2-428887edd32e)


图4 - 16 反馈神经网络最终误差的计算

之后以计算出的测量值与真实值为起点，反向传播到上一个节点，并计算出节点的误差值，如图4 - 17所示。

![image](https://github.com/user-attachments/assets/64f11cad-b620-43e5-8fcd-9d4d552526ab)


图4 - 17 反馈神经网络输出层误差的反向传播

![image](https://github.com/user-attachments/assets/98fa94f3-e2e2-40a3-aac4-e2f6c94bdbc9)


以后将计算出的节点误差重新设置为起点，依次向后传播误差，如图4 - 18所示。

![image](https://github.com/user-attachments/assets/12284152-c180-4b00-ae19-e536845c483d)


图4 - 18 反馈神经网络隐藏层误差的反向传播


注意：对于隐藏层，误差并不是像输出层一样由单个节点确定，而是由多个节点确定，因此对它的计算要求得所有的误差值之和。

通俗地解释，一般情况下，误差的产生是由于输入值与权重的计算产生了错误，而输入值往往是固定不变的，因此对于误差的调节，需要对权重进行更新。而权重的更新又以输入值与真实值的偏差为基础，当最终层的输出误差被反向一层一层地传递回来后，每个节点都会被相应地分配适合其在神经网络中地位的误差，即只需要更新其所需承担的误差量，如图4 - 19所示。

图4 - 19 反馈神经网络权重的更新

![image](https://github.com/user-attachments/assets/25ca4faa-48b6-40f8-885b-e1ffdb2fcf17)



也就是在每一层需要维护输出对当前层的微分值，该微分值相当于被复用于之前每一层中权值的微分计算，因此空间复杂度没有变化。同时，也没有重复计算，每个微分值都会在之后的迭代中使用。

下面介绍公式的推导。公式的推导需要使用一些高等数学的知识，读者可以自由选择学习。

从前文分析来看，对于反馈神经网络算法主要需要得到输出值与真实值之前的差值，之后再利用这个差值去对权重进行更新。而这个差值在不同的传递层有着不同的计算方法：

- 对于输出层单元，误差项是真实值与模型计算值之间的差值。 

- 对于隐藏层单元，由于缺少直接的目标值来计算隐藏层单元的误差，因此需要以间接的方式来计算隐藏层的误差项，并对受隐藏层单元影响的每个单元的误差进行加权求和。 

而在其后的权值更新部分，则主要依靠学习速率、该权值对应的输入以及单元的误差项来完成。

1. **前向传播算法**

![image](https://github.com/user-attachments/assets/4e099345-9772-48ef-917c-ccc2c071d107)


对于前向传播的值传递，隐藏层的输出值定义如下：

\[ a_{h}^{in}=W_{h}^{in}\times X_{i} \]

\[ b_{h}^{in}=f(a_{h}^{in}) \]


其中\(X_{i}\)是当前节点的输入值，\(W_{h}^{in}\)是连接到此节点的权重，\(a_{h}^{in}\)是输出值。\(f\)是当前节点的激活函数，\(b_{h}^{in}\)为当前节点的输入值经过计算后被激活的值。

而对于输出层，定义如下：

\[ a_{k}=\sum W_{hk}\times b_{h}^{in} \]

其中\(W_{hk}\)为输入的权重，\(b_{h}^{in}\)为将节点输入数据经过计算后的激活值作为输入值。这里对所有输入值进行权重计算后求得和值，作为神经网络的最后输出值\(a_{k}\)。

2. **反向传播算法**

与前向传播类似，首先需要定义两个值\(\delta_{k}\)与\(\delta_{h}^{in}\)：

\[ \delta_{k}=\frac{\partial L}{\partial a_{k}}=(Y - T) \]

\[ \delta_{h}^{in}=\frac{\partial L}{\partial a_{h}^{in}} \]

其中\(\delta_{k}\)为输出层的误差项，其计算值为真实值与模型计算值之间的差值。\(Y\)是计算值，\(T\)是真实值。\(\delta_{h}^{in}\)为输出层的误差。

提示：对于\(\delta_{k}\)与\(\delta_{h}^{in}\)来说，无论定义在哪个位置，都可以看作是当前的输出值对于输入值的梯度计算。

通过前面的分析可以知道，所谓的神经网络反馈算法，就是逐层地对最终误差进行分解，即每一层只与下一层打交道，如图4 - 20所示。据此可以假设每一层均为输出层的前一个层级，通过计算前一个层级与输出层的误差得到权重的更新。

![image](https://github.com/user-attachments/assets/fd03a685-aa44-4bdf-b902-9bad296712ae)

![image](https://github.com/user-attachments/assets/e677d0ed-42aa-4e08-b2b3-0f933ccaf244)


图4 - 20 权重的逐层反向传导

因此，反馈神经网络的计算公式如下：

\[
\begin{align*}
\delta_{h}^{in}&=\frac{\partial L}{\partial a_{h}^{in}}\\
&=\frac{\partial L}{\partial b_{h}^{in}}\times\frac{\partial b_{h}^{in}}{\partial a_{h}^{in}}\\
&=\frac{\partial L}{\partial b_{h}^{in}}\times f'(a_{h}^{in})\\
&=\frac{\partial L}{\partial a_{k}}\times\frac{\partial a_{k}}{\partial b_{h}^{in}}\times f'(a_{h}^{in})\\
&=\delta_{k}\times\sum W_{hk}\times f'(a_{h}^{in})\\
&=\sum W_{hk}\times\delta_{k}\times f'(a_{h}^{in})
\end{align*}
\]

![image](https://github.com/user-attachments/assets/731f06a6-f959-423c-ad5d-ea9fc68cbd7c)


也就是当前层输出值对误差的梯度可以通过下一层的误差与权重和输入值的梯度乘积获得。

公式\(\sum W_{hk}\times\delta_{k}\times f'(a_{h}^{in})\)中，若\(\delta_{k}\)为输出层，则可以通过\(\delta_{k}=\frac{\partial L}{\partial a_{k}}=(Y - T)\)求得\(\delta_{k}\)的值，若\(\delta_{k}\)为非输出层，则可以使用逐层反馈的方式求得\(\delta_{k}\)的值。

提示：这里千万要注意，对于\(\delta_{k}\)与\(\delta_{h}^{in}\)来说，其计算结果都是当前的输出值对于输入值的梯度计算，这是权重更新过程中一个非常重要的数据计算内容。

也可以换一种表述形式，将前面的公式表示为：

\[ \delta^{l}=\sum W_{ij}^{l}\times\delta_{j}^{l + 1}\times f'(a_{i}^{l}) \]

可以看到，通过更为泛化的公式，把当前层的输出对输入的梯度计算转换成求下一个层级的梯度计算值。

3. **权重的更新**

![image](https://github.com/user-attachments/assets/fc2c9f18-2035-414c-a567-db7c9cf9bbf4)


反馈神经网络计算的目的是对权重的更新，因此与梯度下降算法类似，其更新可以仿照梯度下降对权值的更新公式：

\[ \theta=\theta - a(f(\theta)-y_{i})x_{i} \]

即：

\[ W_{ji}=W_{ji}+a\times\delta_{j}^{l}\times x_{ji} \]

\[ b_{ji}=b_{ji}+a\times\delta_{j}^{l} \]

其中\(ji\)表示反向传播时对应的节点系数，通过对\(\delta_{j}^{l}\)的计算，就可以更新对应的权重值。\(W_{ji}\)的计算公式如上所示。而对于没有推导的\(b_{ji}\)，其推导过程与\(W_{ji}\)类似，但是在推导过程中输入值是被消去的，请读者自行学习。

##### 4.3.4 反馈神经网络原理的激活函数

![image](https://github.com/user-attachments/assets/94d910f7-cc80-4c40-9004-3e63dd6e1ba4)


现在回到反馈神经网络的函数：

\[ \delta^{l}=\sum W_{ij}^{l}\times\delta_{j}^{l + 1}\times f'(a_{i}^{l}) \] 

