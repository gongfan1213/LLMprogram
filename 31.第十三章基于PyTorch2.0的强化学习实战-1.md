### 第13章 基于PyTorch 2.0的强化学习实战
强化学习（Reinforcement Learning, RL）又称再励学习、评价学习或增强学习，是机器学习的范式和方法论之一，用于描述和解决智能体（Agent）在与环境的交互过程中，通过学习策略以达成回报最大化或实现特定目标的目的。

换句话说，强化学习是一种学习如何从状态映射到行为以使得获取的奖励最大的学习机制。这样的一个智能体需要不断地在环境中进行实验，通过环境给予的反馈（奖励）来不断优化状态 - 行为的对应关系。因此，反复实验（Trial and Error）和延迟奖励（Delayed Reward）是强化学习最重要的两个特征，如图13 - 1所示。

借助ChatGPT的成功，强化学习从原本的不太受重视一跃而起，成为协助ChatGPT登顶的一个重要辅助工具。本章将讲解强化学习方面的内容，使用尽量少的公式，而采用图示或者讲解的形式对其中的理论内容进行介绍。

![image](https://github.com/user-attachments/assets/8e12a619-3c6c-4146-b22b-32bba91ac191)


### 13.1 基于强化学习的火箭回收实战
我们也可以成为马斯克，这并不是天方夜谭。对于马斯克来说，他创立的SpaceX公司的猎鹰火箭回收技术处于世界领先水平。然而，火箭回收技术对于深度学习者来说，是一个遥不可及的梦想吗？答案是否定的。中国的老子说过“九层之台，起于累土；千里之行，始于足下”。接下来从头开始实现这个火箭回收技术。

#### 13.1.1 火箭回收基本运行环境介绍
前面章节介绍了强化学习的基本内容，本小节需要完成基于强化学习的火箭回收实战，也就是通过强化学习方案完成对火箭的控制，从而让其正常降落。

首先进行项目环境的搭建，在这里读者要有一定的深度学习基础以及相应的环境，即Python的运行环境Miniconda以及PyTorch 2.0框架。除此之外，还需要一个专用的深度学习框架Gym，本节会根据Gym来实现强化学习，在该游戏中，对系统的操作和更新都在Gym内部处理，读者只需要关注强化学习部分即可。因此，我们只需要考虑“状态”→“神经网络”→“动作”即可。

对Gym的安装如下。
```bash
pip install gym
pip install box2d box2d-kengz --user
```
这里需要注意，如果有报错，请读者自行查询相关的网络文章来解决。为了验证具体的安装情况，执行以下代码段：
```python
import gym
import time
# 环境初始化
env = gym.make('LunarLander-v2', render_mode='human')
if True:
    state = env.reset()
    while True:
        # 渲染画面
        env.render()
        # 从动作空间随机获取一个动作
        action = env.action_space.sample()
        # Agent与环境进行一步交互
        observation, reward, done, _, _ = env.step(action)
        print('state = {0}; reward = {1}'.format(state, reward))
        # 判断当前episode是否完成
        if done:
            print('游戏结束')
            break
        time.sleep(0.01)
env.close() # 环境结束
```
这是导入了Gym的运行环境，即完成了火箭回收的环境配置，读者通过运行此代码段可以看到如图13 - 2所示的界面。

![image](https://github.com/user-attachments/assets/bf3394e7-1c19-458c-b2ae-b9b0066cd0ac)


这是火箭回收的运行界面，在下方的输出框中有如图13 - 3所示的内容输出。

![image](https://github.com/user-attachments/assets/1cba90cd-0647-44db-bcdf-9b46c8bd168a)


#### 13.1.2 火箭回收参数介绍
13.1.1节打印了火箭回收的state参数，这是火箭回收过程中的环境参数值，也就是可以通过观测器获取到的火箭状态数值，分别如下：
- 水平坐标x。
- 垂直坐标y。
- 水平速度。
- 垂直速度。
- 角度。
- 角速度。
- 腿1触地。
- 腿2触地。

对于操作者来说，可以有4种离散的行动对火箭进行操作，分别说明如下。
- 0代表不采取任何行动。
- 2代表主引擎向下喷射。
- 1、3分别代表向左、右喷射。

除此之外，对于火箭还有一个最终的奖励，即对于每一步的操作都要额外计算分值，说明如下。
- 小艇坠毁得到 - 100分。
- 小艇在黄旗帜之间成功着地得100 - 140分。
- 喷射主引擎（向下喷火）每次得 - 0.3分。
- 小艇最终完全静止再得100分。
- “腿1”和“腿2”都落地得10分。

#### 13.1.3 基于强化学习的火箭回收实战
下面完成基于强化学习的火箭回收内容。完整代码如下（请读者运行本章源码中的“火箭回收”代码，第一次读者学会运行即可，部分代码段的详细讲解请参考13.1.4节的算法部分进行对照学习）：
```python
import matplotlib.pyplot as plt
import torch
from torch.distributions import Categorical
import gym
import time
import numpy as np
import random
from IPython import display

class Memory:
    def __init__(self):
        """初始化"""
        self.actions = []  # 行动(共4种)
        self.states = []  # 状态，由8个数字组成
        self.logprobs = []  # 概率
        self.rewards = []  # 奖励
        self.is_dones = []  # 游戏是否结束，is_terminals?

    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]
        del self.is_dones[:]

class Action(torch.nn.Module):
    def __init__(self, state_dim=8, action_dim=4):
        super().__init__()
        # actor
        self.action_layer = torch.nn.Sequential(
            torch.nn.Linear(state_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, action_dim),
            torch.nn.Softmax(dim=-1)
        )

    def forward(self, state):
        action_logits = self.action_layer(state)  # 计算4个方向的概率
        return action_logits

class Value(torch.nn.Module):
    def __init__(self, state_dim=8):
        super().__init__()
        # value
        self.value_layer = torch.nn.Sequential(
            torch.nn.Linear(state_dim, 128),
            torch.nn.ReLU(),
            torch.nn.Linear(128, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 1)
        )

    def forward(self, state):
        state_value = self.value_layer(state)
        return state_value

class PPOAgent:
    def __init__(self,state_dim,action_dim,n_latent_var,lr,betas,gamma, K_epochs,eps_clip):
        self.lr = lr  # 学习率
        self.betas = betas  # betas
        self.gamma = gamma  # gamma
        self.eps_clip = eps_clip  # 裁剪，限制值范围
        self.K_epochs = K_epochs  # 获取的每批次的数据作为训练使用的次数
        # actor
        self.action_layer = Action()
        # critic
        self.value_layer = Value()
        self.optimizer = torch.optim.Adam([{"params":self.action_layer.parameters()},{"params":self.value_layer.parameters()}], lr=lr, betas=betas)
        # 损失函数
        self.MseLoss = torch.nn.MSELoss()

    def evaluate(self, state, action):
        action_probs = self.action_layer(state)  # 这里输出的结果是4类别的[-1,4]
        dist = Categorical(action_probs)  # 转换成类别分布
        # 计算概率密度，log(概率)
        action_logprobs = dist.log_prob(action)
        # 计算信息熵
        dist_entropy = dist.entropy()
        # 评判，对当前的状态进行评判
        state_value = self.value_layer(state)
        # 返回行动概率密度、评判值、行动概率熵
        return action_logprobs, torch.squeeze(state_value), dist_entropy

    def update(self,memory):
        # 预测状态回报
        rewards = []
        discounted_reward = 0  # discounted = 不重要
        # 这里是不可以这样理解，当前步骤是决定未来的步骤，而模型需要根据当前步骤对未来的最终结果进行修正，如果遵循了当前步骤，就可以看到未来的结果如何
        # 这样未来的结果会很差，所以模型需要远离会造成坏的结果的步骤
        # 所以就反过来计算
        # print(len(self.memory.rewards),len(self.memory.is_dones))这里就是做成批次，1200批次数据做一次
        for reward, is_done in zip(reversed(memory.rewards), reversed(memory.is_dones)):
            # 回合结束:
            if is_done:
                discounted_reward = 0
            # 更新削减奖励(当前状态奖励 + 0.99*上一状态奖励)
            discounted_reward = reward + (self.gamma * discounted_reward)
            # 首插入
            rewards.insert(0, discounted_reward)
        # print(len(rewards))  #这里的长度就是根据batch_size的长度设置的
        # 标准化奖励
        rewards = torch.tensor(rewards, dtype=torch.float32)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)

        # print(len(self.memory.states),len(self.memory.actions), len(self.memory.logprobs))
        # 这里的长度就是根据batch_size的长度设置的
        # 张量转换
        # convert list to tensor
        old_states = torch.tensor(memory.states)
        old_actions = torch.tensor(memory.actions)
        old_logprobs = torch.tensor(memory.logprobs)
        # 迭代优化K次
        for _ in range(5):
            # Evaluating old actions and values ：新策略略重用旧样本进行训练
            logprobs, state_values, dist_entropy=self.evaluate(old_states, old_actions)
            ratios = torch.exp(logprobs - old_logprobs.detach())
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip,1 + self.eps_clip) * advantages
            loss =-torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy
            # take gradient step
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()

    def act(self,state):
        state = torch.from_numpy(state).float()
        # 计算4个方向的概率
        action_probs = self.action_layer(state)
        # 通过最大概率计算最终行动方向
        dist = Categorical(action_probs)
        action = dist.sample() #这个是根据action_probs做出符合分布action_probs的抽样结果
        return action.item(),dist.log_prob(action)

state_dim = 8  # 游戏的状态是一个8维向量
action_dim = 4  # 游戏的输出有4个取值
n_latent_var = 128  # 神经元个数
update_timestep = 1200  # 每1200步policy更新一次
lr = 0.002  # learning rate
betas = (0.9, 0.999)
gamma = 0.99  # discount factor
K_epochs = 5  # policy迭代更新次数
eps_clip = 0.2  # clip parameter for PPO 论文中表明0.2效果不错
random_seed = 929

agent = PPOAgent(state_dim,action_dim,n_latent_var,lr,betas,gamma,K_epochs,eps_clip)
memory = Memory()
agent.network.train()  # Switch network into training mode
EPISODE_PER_BATCH = 5  # update the agent every 5 episode
NUM_BATCH = 200  # totally update the agent for 400 time
avg_total_rewards, avg_final_rewards = [], []
env = gym.make('LunarLander-v2', render_mode='rgb_array')
rewards_list = []

for i in range(200):
    rewards = []
    # collect trajectory
    for episode in range(EPISODE_PER_BATCH):
        # 重开一把游戏
        state = env.reset()[0]
        while True:
            # 这里，agent做出act动作后，数据已经被储存了。另外，注意这里是使用old_policity_act做的
            with torch.no_grad():
                action,action_prob = agent.act(state)  # 按照策略网络输出的概率随机采样一个动作
                memory.states.append(state)
                memory.actions.append(action)
                memory.logprobs.append(action_prob)
                next_state, reward, done, _, _ = env.step(action)  # 与环境state进行交互，输出reward和环境next_state
                state = next_state
                rewards.append(reward)  # 记录每一个动作的reward
                memory.rewards.append(reward)
                memory.is_dones.append(done)
                if len(memory.rewards) >= 1200:
                    agent.update(memory)
                    memory.clear_memory()
                if done or len(rewards) > 1024:
                    rewards_list.append(np.sum(rewards))
                    #print('游戏结束')
                    break
    print(f"epoch: {i} ,rewards looks like ", rewards_list[-1])

plt.plot(range(len(rewards_list)),rewards_list)
plt.show()
plt.close()
env = gym.make('LunarLander-v2', render_mode='human')
for episode in range(EPISODE_PER_BATCH):
    # 重开一把游戏
    state = env.reset()[0]
    step = 0
    while True:
        step += 1
        # 这里，agent做出act动作后，数据已经被储存了。另外，注意这里是使用old_policity_act做的
        action,action_prob = agent.act(state)  # 按照策略网络输出的概率随机采样一个动作
        # agent与环境进行一步交互
        state, reward, terminated, truncated, info = env.step(action)
        #print('state = {0}; reward = {1}'.format(state, reward))
        # 判断当前episode是否完成
        if terminated or step >= 600:
            print('游戏结束')
            break
        time.sleep(0.01)
print(np.mean(rewards_list))
```
 
