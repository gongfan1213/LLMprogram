### 第8章 有趣的词嵌入
#### 从gensim.models import FastText
```python
model = FastText(min_count=5,vector_size=300,window=7,workers=10,
epochs=50,speed=17,sg=1,hs=1)
```
其中FastText的参数定义如下。
- **sentences (iterable of iterables, optional)**：供训练的句子，可以使用简单的列表，但是对于大语料库，建议直接从磁盘/网络流迭代传输句子。
- **vector_size (int, optional)**：词向量的维度。 
- **window (int, optional)**：一个句子中当前单词和被预测单词的最大距离。 
- **min_count (int, optional)**：忽略词频小于此值的单词。 
- **workers (int, optional)**：训练模型时使用的线程数。 
- **sg ({0, 1}, optional)**：模型的训练算法：1代表skip - gram；0代表CBOW。 
- **hs ({0, 1}, optional)**：1采用Hierarchical Softmax训练模型；0采用负采样。 
- **epochs**：模型迭代的次数。 
- **seed (int, optional)**：随机数发生器种子。 

在定义的FastText类中，依次设定了最低词频度、单词训练的最大距离、迭代数以及训练模型等。完整训练例子如下。
#### 【程序8 - 9】
```python
text = [
    "卷积神经网络在图像处理领域获得了极大成功，其结合特征提取和目标训练为一体的模型能够最好地利用已有的信息对结果进行反馈训练。",
    "对于文本识别的卷积神经网络来说，同样是充分利用特征提取时提取的文本特征来计算文本特征权值大小的，归一化处理需要处理的数据。",
    "这样使得原来的文本信息抽象成一个向量化的样本集，之后将样本集和训练好的模板输入卷积神经网络进行处理。",
    "本节将在上一节的基础上使用卷积神经网络实现文本分类的问题，这里将采用两种主要基于字符的和基于word embedding形式的词卷积神经网络处理方法。",
    "实际上无论是基于字符的还是基于word embedding形式的处理方式都是可以相互转换的，这里只介绍使用基本的使用模型和方法，更多的应用还需要读者自行挖掘和设计。"
]
import jieba
jieba_cut_list = []
for line in text:
    jieba_cut = jieba.lcut(line)
    jieba_cut_list.append(jieba_cut)
    print(jieba_cut)

from gensim.models import FastText
model = FastText(min_count=5,vector_size=300,window=7,workers=10,epochs=50,seed=17,sg=1,hs=1)
model.build_vocab(jieba_cut_list)
model.train(jieba_cut_list, total_examples=model.corpus_count, epochs=model.epochs)
```
这里使用作者给出的固定格式即可。
```python
model.save("./xiaohua_fasttext_model_jieba.model")
```
model中的build_vocab函数用于对数据建立词库，而train函数用于对model模型设定训练模式。这里使用作者给出的格式即可。

对于训练好的模型的存储，这里模型被存储在models文件夹中。
### 3. 使用训练好的FastText进行参数读取
使用训练好的FastText进行参数读取很方便，直接载入训练好的模型，之后将带测试的文本输入即可，代码如下：
```python
from gensim.models import FastText
model = FastText.load("./xiaohua_fasttext_model_jieba.model")
embedding = model.wv["设计"]  # “设计”这个词在上面提供的代文本中出现，并经过jieba分词后得到模型的训练
print(embedding)
```
代码中，“设计”这个词在作者的提供的代文本中出现，并经过jieba分词后得到模型的训练。与训练过程不同的是，这里FastText使用自带的load函数将保存的模型载入，之后使用类似于传统的list方式将已训练过的值打印出来。打印结果如图8 - 20所示。
```
[-1.85652229e-03 1.06951549e-04 -1.29939604e-03 -2.34862976e-03
 -6.68820925e-04 2.6710674e-03 -1.97672029e-03 -1.04239455e-03
 -8.38022737e-04 6.35023462e-05 9.9683461e-04 1.45770342e-03
  7.53837754e-04 5.64315473e-04 -1.27105368e-03 -8.11854668e-04
  1.84631464e-03 7.92989835e-04 2.69438024e-05 -2.72292834e-03
  1.66522607e-03 -1.27705897e-03 7.12231966e-04 6.97845593e-04
 -2.0309278e-03 6.80215948e-04 -5.58388012e-04 -2.13399762e-05
 -1.41401729e-03 -3.24102934e-04 6.30286346e-04 1.45770342e-03
  3.52950243e-04 9.67934118e-04 -7.11251458e-04 -1.24862022e-03]
```

![image](https://github.com/user-attachments/assets/7f9b0759-85d5-4573-bfa0-dbe44f0508c6)


#### 图8 - 20 打印结果
注意：FastText模型只能打印已训练过的词向量，而不能打印未经过训练的词。在上例中，模型输出的值是已经过训练的“设计”这个词。

打印输出值可维度如下：
```python
print(embedding.shape)
```
具体读者可自行决定。
### 4. 继续已有的FastText模型进行词嵌入训练
有时需要在训练好的模型上继续进行词嵌入训练，可以利用已训练好的模型或者利用计算机碎片时间进行迭代训练。理论上，数据集内容越多，训练时间越长，则训练精度越高。
```python
from gensim.models import FastText
model = FastText.load("./xiaohua_fasttext_model_jieba.model")
#embedding = model.wv["设计"]#“设计”这个词在上面提供的代文本中出现，并经过jieba分词后得到模型的训练
model.build_vocab(jieba_cut_list, update=True)
model.train(jieba_cut_list, total_examples=model.corpus_count, epochs=6)
model.min_count = 10
model.save("./xiaohua_fasttext_model_jieba.model")
```
在这里需要额外设置一些model的参数，读者仿照作者写的格式设置即可。
### 5. 提取FastText模型的训练结果作为预训练词嵌入数据（请读者一定注意位置对应关系）
训练好的FastText模型可以作为深度学习的预训练词嵌入输入模型中使用，相对于随机生成的向量，预训练的词嵌入数据带有部分位置和语义信息。

获取预训练好的词嵌入数据的代码如下：
```python
def get_embedding_model(Word2VecModel):
    vocab_list = [word for word in Word2VecModel.wv.key_to_index]  # 存储所有的词语
    word_index = {"": 0}  # 初始化[word : token], 后期tokenize语料库就是用该词典
    word_vector = {}  # 初始化[word : vector]字典
    # 初始化存储所有向量的大矩阵，留意其中多一位（首行），词向量全为0，用于padding补零
    # 行数为所有单词数+1，比如10000+1；列数为词向量维度，比如100
    embeddings_matrix = np.zeros((len(vocab_list) + 1, Word2VecModel.vector_size))
    # 填充上述的字典和大矩阵
    for i in range(len(vocab_list)):
        word = vocab_list[i]  # 每个词语
        word_index[word] = i + 1  # 词语：序号
        word_vector[word] = Word2VecModel.wv[word]  # 词语：词向量
        embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # 词向量矩阵
    # 这里的word_vector数据量较大时不好打印
    return word_index, word_vector, embeddings_matrix  # word_index和embeddings_matrix的作用在下文阐述
```
在示例代码中，首先通过迭代方法获取训练的词库列表，之后建立字典，使得词和序列号一一对应。

返回值是3个数值，分别word_index、word_vector和embeddings_matrix，这里word_index是词的序列，embeddings_matrix是生成的与词向量表所对应的embedding矩阵。在这里需要提示的是，实际上embedding可以根据传入的数据不同而对其位置进行修正，但是此修正必须伴随word_index一起改变位置。

使用输出的embeddings_matrix由以下函数完成：
```python
import torch
embedding = torch.nn.Embedding(num_embeddings=embeddings_matrix.shape[0],
embedding_dim=embeddings_matrix.shape[1])
embedding.weight.data.copy_(torch.tensor(embeddings_matrix))
```
在这里训练好的embeddings_matrix被作为参数传递给Embedding列表，读者只需要遵循这种写法即可。

有一个问题是PyTorch的Embedding中进行look_up查询时，传入的是每个字符的序号，因此需要一个编码器将字符编码为对应的序号。
```python
# tokenizer对输入文本中每个单词或字符进行序列化操作，并返回由每个单词或字符所对应的索引
# 组成的索引列表。这个只能对单个字使用，无法处理词语切词的情形
def tokenizer(texts, word_index):
    token_indexs = []
    for sentence in texts:
        new_txt = []
        for word in sentence:
            try:
                new_txt.append(word_index[word])  # 把句子中的词语转换为index
            except:
                new_txt.append(0)
        token_indexs.append(new_txt)
    return token_indexs
```
tokenizer函数用作对单词的序列化，这里根据上文生成的word_index对每个词语进行编号。具体应用请读者参考前面的内容自行尝试。
### 8.2.3 使用其他预训练参数来生成PyTorch 2.0词嵌入矩阵（中文）
无论是使用Word2Vec还是FastText作为训练基础都是可以的。但是对于个人用户或者规模不大的公司机构来说，做一个庞大的预训练项目是一个费时费力的工程。

他山之石，可以攻玉。我们可以借助其他免费的训练好的词向量作为使用基础，如图8 - 21所示。

![image](https://github.com/user-attachments/assets/43cc8c8f-bd6a-4502-9f74-562f1a4dc970)


#### 图8 - 21 预训练词向量

在中文部分，较为常用且免费的词嵌入预训练数据为腾讯的词向量，地址如下：

https://ai.tencent.com/ailab/nlp/embedding.html

![image](https://github.com/user-attachments/assets/8e3a675c-de71-41d9-b185-99e586b83294)


#### 图8 - 22 腾讯的词向量下载界面

可以使用以下代码载入预训练模型进行词矩阵的初始化：

```python
from gensim.models.word2vec import KeyedVectors
wv_from_text = KeyedVectors.load_word2vec_format(file, binary=False)
```

接下来的步骤与8.2.2节相似，读者可以自行编写完成。

### 8.3 针对文本的卷积神经网络模型简介——字符卷积

卷积神经网络在图像处理领域获得了极大成功，其结合特征提取和目标训练为一体的模型能够最好地利用已有的信息对结果进行反馈训练。

对于文本识别的卷积神经网络来说，同样是充分利用特征提取时提取的文本特征来计算文本特征权值大小的，归一化处理需要处理的数据。这样使得原来的文本信息抽象成一个向量化的样本集，之后将样本集和训练好的模板输入卷积神经网络进行处理。

本节将在上一节的基础上使用卷积神经网络实现文本分类的问题，这里将采用两种处理方法，分别是基于字符的处理方法和基于Word Embedding的处理方法。实际上，基于字符的和基于Word Embedding的处理方法是可以相互转换的，这里只介绍基本的模型和方法，更多的应用还需要读者自行挖掘和设计。
#### 8.3.1 字符（非单词）文本的处理
本小节将介绍基于字符的CNN处理方法。基于单词的卷积处理内容将在下一节介绍，读者可以循序渐进地学习。

任何一个英文单词都是由字母构成的，因此可以简单地将英文单词拆分成字母的表示形式：
```
hello -> ["h","e","l","l","o"]
```
这样可以看到一个单词hello被人为地拆分成“h”“e”“l”“l”“o”这5个字母。而对于Hello的处理有两种方法，即采用One - Hot的方式和采用词嵌入（按单字符分割）的方式处理。这样的话，hello这个单词就被转换成一个[5,n]大小的矩阵，本例采用One - Hot的方式处理。

使用卷积神经网络计算字符矩阵，对于每个单词拆分成的数据，根据不同的长度对其进行卷积处理，提取出高层抽象概念，这样做的好处是不需要使用预训练好的词向量和语法句法结构等信息。除此之外，字符级还有一个好处就是可以很容易地推广到所有语言。使用CNN处理字符文本分类的原理如图8 - 23所示。
#### 图8 - 23 使用CNN处理字符文本分类

![image](https://github.com/user-attachments/assets/838a6a57-317c-445e-be61-ecfc33e3ba42)


### 1. 标题文本的读取与转化
对于AG_news数据集来说，每个分类的文本条例既有对应的分类，又有标题和文本内容，对于文本内容的抽取，在8.1节的选学内容中介绍过了，这里采用直接使用标题文本的方法进行处理，如图8 - 24所示。

![image](https://github.com/user-attachments/assets/822b741a-6bba-4248-80a4-dbb09dd94c3f)



![image](https://github.com/user-attachments/assets/10fb937d-9ae4-4582-af43-6615af783edc)

#### 图8 - 24 AG_news标题文本
读取标题和label的程序请读者参考8.1节的内容自行完成。由于只是对文本标题进行处理，因此在进行数据清洗的时候不用处理停用词和进行词干还原，并且对于空格，由于是进行字符计算，因此不需要保留空格，直接将其删除即可。完整代码如下：
```python
def text_clearTitle(text):
    text = text.lower()  # 将文本转换成小写
    text = re.sub(r"[^a-z]", " ", text)  # 替换非标准字符，^是求反操作
    text = re.sub(r" +", " ", text)  # 替换多重空格
    text = text.strip()  # 取出首尾空格
    text = text + " eos"  # 添加结束符，请注意，eos前面有一个空格
    return text
```
这样获取的结果如图8 - 25所示。

![image](https://github.com/user-attachments/assets/f30edf75-124e-4082-b6a2-a0678fa6ef24)


#### 图8 - 25 AG_news标题文本抽取结果
可以看到，不同的标题被整合成一系列可能对于人类来说没有任何意义的字符。
### 2. 文本的One - Hot处理
对生成的字符串进行One - Hot处理，处理的方式非常简单，首先建立一个26个字母的字符表：
```
alphabet_title = "abcdefghijklmnopqrstuvwxyz"
```
针对不同的字符获取字符表对应位置进行提取，根据提取的位置将对应的字符位置设置成1，其他为0，例如字符c在字符表中第3个，那么获取的字符矩阵为：
```
[0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
```
其他的类似，代码如下：
```python
def get_one_hot(list):
    values = np.array(list)
    n_values = len(alphabet_title) + 1
    return np.eye(n_values)[values]
```
这段代码的作用是将生成的字符序列转换成矩阵，如图8 - 26所示。

![image](https://github.com/user-attachments/assets/2030fa33-e69e-4de9-9ce8-a1848a6442dc)


#### 图8 - 26 字符转换成矩阵示意图
然后将字符串按字符表中的顺序转换成数字序列，代码如下：
```python
def get_char_list(string):
    alphabet_title = "abcdefghijklmnopqrstuvwxyz"
    char_list = []
    for char in string:
        num = alphabet_title.index(char)
        char_list.append(num)
    return char_list
```
这样生成的结果如下：
```
hello -> [7, 4, 11, 11, 14]
```
将代码整合在一起，最终结果如下：
```python
def get_one_hot(list,alphabet_title = None):
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz"
    else:alphabet_title = alphabet_title
    values = np
