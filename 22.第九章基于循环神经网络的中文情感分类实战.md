### 第9章 基于循环神经网络的中文情感分类实战

前面的章节带领读者实现了图像降噪与图像识别等方面的内容，并且在第8章基于卷积神经网络完成了英文新闻分类的工作。相信读者学习到本章内容时，对使用PyTorch 2.0完成项目已经有了一定的把握。

但是在前期的学习过程中，主要以卷积神经网络为主，而较少讲解神经网络中的另一个非常重要的内容——循环神经网络。本章将讲解循环神经网络的基本理论，以其一个基本实现GRU为例来讲解循环神经网络的使用方法。

#### 9.1 实战：循环神经网络与情感分类

循环神经网络用来处理序列数据。

统的神经网络模型是从输入层到隐藏层再到输出层，层与层之间是全连接的，每层之间的节点是无连接的。但是这种普通的神经网络对于很多问题无能为力。例如，你要预测句子的下一个单词是什么，一般需要用到前面的单词，因为一个句子中的前后单词并不是独立的，即一个序列当前的输出与前面的输出也有关。

环神经网络的具体表现形式为：网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点不再是无连接的，而是有连接的，并且隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出，如图9 - 1所示。

![image](https://github.com/user-attachments/assets/e88d3e9c-d3ac-46e1-ae55-c8ce1d3518f2)


在讲解循环神经网络的理论知识之前，最好的学习方式就是通过实例实现并运行对应的项目，在这里首先带领读者完成循环神经网络的情感分类实战的准备工作。

1. **数据的准备**

首先是数据集的准备工作，在这里我们完成的是中文数据集的情感分类，因此准备了一套已完成情感分类的数据集，读者可以参考本书自带的dataset数据集中的chnSentiCorp.txt文件确认。此时，读者需要掌握数据的读取和准备工作，读取的代码如下：
```python
max_length = 80  #设置获取的文本长度为80
labels = []  #用以存放label
context = []  #用以存放汉字文本
vocab = set()
with open("../dataset/cn/ChnSentiCorp.txt", "mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")

        labels.append(int(line[0]))
        if int(line[0]) == 0:
            labels.append(0) #由于在后面直接采用PyTorch自带的crossentroy函数，因此这里直接输入0，否则输入[1,0]
        else:
            labels.append(1)
        text = "".join(line[1:])
        context.append(text)
        for char in text: vocab.add(char)  #建立vocab和vocab编号

vocab_list = list(sorted(vocab))
# print(len(vocab_list))
token_list = []
#下面对context内容根据vocab进行token处理
for text in context:
    token = [vocab_list.index(char) for char in text]
    token = token[:max_length] + [0] * (max_length - len(token))
    token_list.append(token)
```

2. **模型的建立**

下面根据需求建立需要的模型，在这里实现了一个带有单向GRU和一个双向GRU的循环神经网络，代码如下：

```python
class RNNModel(torch.nn.Module):
    def __init__(self,vocab_size = 128):
        super().__init__()
        self.embedding_table = torch.nn.Embedding(vocab_size,embedding_dim=312)
        self.gru = torch.nn.GRU(312,256)  # 注意这里输出有两个，分别是out与hidden，out是序列在模型运行后全部隐藏层的状态，而hidden是最后一个隐藏层的状态
        self.batch_norm = torch.nn.LayerNorm(256,256)

        self.gru2 = torch.nn.GRU(256,128,bidirectional=True)  # 注意这里输出有两个，分别是out与hidden，out是序列在模型运行后全部隐藏层的状态，而hidden是最后一个隐藏层的状态
    def forward(self,token):
        token_inputs = token
        embedding = self.embedding_table(token_inputs)
        gru_out,_ = self.gru(embedding)
        embedding = self.batch_norm(gru_out)
        out,hidden = self.gru2(embedding)

        return out
```

这里需要注意的是，对于GRU进行神经网络训练，无论是单向还是双向GUR，其结果输出都是两个隐藏层状态，分别是out与hidden。out是序列在模型运行后全部隐藏层的状态，而hidden是此序列最后一个隐藏层的状态。

这里使用的是两层的GRU，有读者可能会注意到，在对第二个GRU进行定义时，有一个额外的参数bidirectional，用于定义在循环神经网络中是单向计算还是双向计算，其具体形式如图9 - 2所示。



图9 - 2中可以很明显地看到，左右两个连续的模块并联构成了不同方向的循环神经网络单向计算层，这两个方向同时作用后生成了最终的隐藏层。

![image](https://github.com/user-attachments/assets/453fac13-6f27-4330-a098-abba9950221e)


3. **模型的实现**

9.1.1节完成了循环神经网络的数据准备和模型的定义，本小节对中文数据集进行情感分类，完整的代码如下：
```python
import numpy as np

max_length = 80  #设置获取的文本长度为80
labels = []  #用以存放label
context = []  #用以存放汉字文本
vocab = set()

with open("../dataset/cn/ChnSentiCorp.txt", "mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")

        labels.append(int(line[0]))
        if int(line[0]) == 0:
            labels.append(0) #由于在后面直接采用PyTorch自带的crossentroy函数，因此这里直接输入0，否则输入[1,0]
        else:
            labels.append(1)
        text = "".join(line[1:])
        context.append(text)
        for char in text: vocab.add(char)  #建立vocab和vocab编号

vocab_list = list(sorted(vocab))
# print(len(vocab_list))
token_list = []
#下面对context内容根据vocab进行token处理
for text in context:
    token = [vocab_list.index(char) for char in text]
    token = token[:max_length] + [0] * (max_length - len(token))
    token_list.append(token)

seed = 17
np.random.seed(seed);np.random.shuffle(token_list)
np.random.seed(seed);np.random.shuffle(labels)

dev_list = np.array(token_list[:170])
dev_labels = np.array(labels[:170])

token_list = np.array(token_list[170:])
labels = np.array(labels[170:])

import torch
class RNNModel(torch.nn.Module):
    def __init__(self,vocab_size = 128):
        super().__init__()
        self.embedding_table = torch.nn.Embedding(vocab_size,embedding_dim=312)
        self.gru = torch.nn.GRU(312,256)  # 注意这里输出有两个，分别是out与hidden，out是序列在模型运行后全部隐藏层的状态，而hidden是最后一个隐藏层的状态
        self.batch_norm = torch.nn.LayerNorm(256,256)

        self.gru2 = torch.nn.GRU(256,128,bidirectional=True)  # 注意这里输出有两个，分别是out与hidden，out是序列在模型运行后全部隐藏层的状态，而hidden是最后一个隐藏层的状态
    def forward(self,token):
        token_inputs = token
        embedding = self.embedding_table(token_inputs)
        gru_out,_ = self.gru(embedding)
        embedding = self.batch_norm(gru_out)
        out,hidden = self.gru2(embedding)

        return out
#这里使用顺序模型建立训练模型
def get_model(vocab_size = len(vocab_list),max_length = max_length):
    model = torch.nn.Sequential(
        RNNModel(vocab_size),
        torch.nn.Flatten(),
        torch.nn.Linear(2 * max_length * 128,2)
    )
    return model

device = "cuda"
model = get_model().to(device)
model = torch.compile(model)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
loss_func = torch.nn.CrossEntropyLoss()

batch_size = 128
train_length = len(labels)
for epoch in range(21):
    train_num = train_length // batch_size
    train_loss, train_correct = 0, 0
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size

        batch_input_ids = torch.tensor(token_list[start:end]).to(device)
        batch_labels = torch.tensor(labels[start:end]).to(device)

        pred = model(batch_input_ids)

        loss = loss_func(pred, batch_labels.type(torch.uint8))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_correct += ((torch.argmax(pred, dim=-1) ==
(batch_labels)).type(torch.float).sum().item() / len(batch_labels))

    train_loss /= train_num
    train_correct /= train_num
    print("train_loss:", train_loss, "train_correct:", train_correct)

    test_pred = model(torch.tensor(dev_list).to(device))
    correct = (torch.argmax(test_pred, dim=-1) ==
(torch.tensor(dev_labels).to(device))).type(torch.float).sum().item() / len(test_pred)
    print("test_acc:",correct)
    print("---------------------")
```
在这里使用顺序模型建立循环神经网络模型，在使用GUR对数据进行计算后，又使用Flatten对序列Embedding进行了平整化处理。最终的Linear是分类器，用于对结果进行分类。具体结果请读者自行测试查看。

#### 9.2 循环神经网络理论讲解

前面完成了循环神经网络对情感分类的实战工作，本节开始进入循环神经网络的理论讲解部分，还是以GRU为例进行介绍。

1. **9.2.1 什么是GRU**

在前面的实战过程中，使用GRU作为核心神经网络层，GRU是循环神经网络的一种，是为了解决长期记忆和反向传播中的梯度等问题而提出的一种神经网络结构，是一种用于处理序列数据的神经网络。

GRU更擅长处理序列变化的数据，比如某个单词的意思会因为上文提到的内容不同而有不同的含义，GRU就能够很好地解决这类问题。

![image](https://github.com/user-attachments/assets/4374cd0c-9c16-4915-80e0-50e3a662fc4e)


- **GRU的输入与输出结构**


GRU的输入与输出结构如图9 - 3所示。

通过GRU的输入与输出结构可以看到，在GRU中有一个当前的输入$x^t$，和上一个节点传递下来的隐藏状态（Hidden State）$h^{t - 1}$，这个隐藏状态包含之前节点的相关信息。

结合$x^t$和$h^{t - 1}$，GRU会得到当前隐藏节点的输出$y^t$和传递给下一个节点的隐藏状态$h^t$。

- **门 - GRU的重要设计**

一般认为，门是GRU能够替代传统的RNN的原因。先通过上一个传输下来的状态$h^{t - 1}$和当前节点的输入$x^t$来获取两个门控状态，如图9 - 4所示。

其中$r$用于控制重置的门控（Reset Gate），$z$则用于控制更新的门控（Update Gate）。而$\sigma$为Sigmoid函数，通过这个函数可以将数据变换为0 - 1范围内的数值，从而来充当门控信号。

得到门控信号之后，首先使用重置门控来得到重置之后的数据$h^{(t - 1)'}= h^{t - 1} \times r$，再将$h^{(t - 1)'}$与输入$x^t$进行拼接，通过一个Tanh激活函数来将数据缩放到 - 1~1的范围内，得到如图9 - 5所示的$h'$。

这里的$h'$主要是包含当前输入的$x^t$数据。有针对性地将$h'$添加到当前的隐藏状态，相当于“记忆了当前时刻的状态”。

![image](https://github.com/user-attachments/assets/ec8c08f0-9285-4b90-befb-3a41e49e616d)


- **GRU的结构**

最后介绍GRU最关键的一个步骤，可以称之为“更新记忆”阶段。在这个阶段，GRU同时进行了遗忘和记忆两个步骤，如图9 - 6所示。

使用了先前得到的更新门控$z$，从而能够获得新的更新，公式如下：

$h^t = zh^{t - 1}+(1 - z)h'$

![image](https://github.com/user-attachments/assets/c29b5b42-f9b6-49b3-a18f-c187a6aeb830)


公式说明如下：
  - $zh^{t - 1}$：表示对原本的隐藏状态选择性“遗忘”。这里的$z$可以想象成遗忘门（Forget Gate），忘记$h^{t - 1}$维度中一些不重要的信息。
  - $(1 - z)h'$：表示对包含当前节点信息的$h'$进行选择性“记忆”。与前面类似，这里的$1 - z$也会忘记$h'$维度中一些不重要的信息。或者，这里更应当看作是对$h'$维度中的一些信息进行选择。
  - 结上所述，整个公式的操作就是忘记传递下来的$h^{t - 1}$中的一些维度信息，并加入当前节点输入的一些维度信息。

可以看到，这里的遗忘$z$和选择（$1 - z$）是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，遗忘了多少权重（$z$），我们就会使用包含当前输入的$h'$中所对应的权重弥补（$1 - z$）的量，从而使得GRU的输出保持一种“恒定”状态。

2. **9.2.2 单向不行，那就双向**

在前面简单介绍了GRU中的参数bidirectional，bidirectional参数是双向传输的，其目的是将相同的信息以不同的方式呈现给循环网络，这样可以提高精度并缓解遗忘问题。双向GRU是一种常见的GRU变体，常用于自然语言处理任务。

GRU特别依赖于顺序或时间，它按顺序处理输入序列的时间步，而打乱时间步或反转时间步会完全改变GRU从序列中提取的表示。正是由于这个原因，如果顺序对问题很重要（比如室温预测等问题），GRU的表现就会很好。

双向GRU利用了这种顺序敏感性，每个GRU分别沿一个方向对输入序列进行处理（时间正序和时间逆序），然后将它们的表示合并在一起，如图9 - 7所示。通过沿这两个方向处理序列，双向GRU可以捕捉到可能被单向GRU所忽略的特征模式。

一般来说，按时间正序的模型会优于按时间逆序的模型。但是对应文本分类等问题，一个单词对理解句子的重要性通常并不取决于它在句子中的位置，即用正序序列和逆序序列，或者随机打断“词语（不是字）”出现的位置，之后将新的数据作为样本输入给GRU进行重新训练并评估，性能几乎相同。这证实了一个假设：虽然单词顺序对理解语言很重要，但使用哪种顺序并不重要。

$\vec{h}_t=\overrightarrow{\text{GRU}}(x_t),t \in [1,T]$

$\overleftarrow{h}_t=\overleftarrow{\text{GRU}}(x_t),t \in [T,1]$

双向循环层还有一个好处是，在机器学习中，如果一种数据表示不但有用，那么总是值得加以利用，这种表示与其他表示的差异越大越好，它们提供了查看数据的全新角度，抓住了数据中被其他方法忽略的内容，因此可以提高模型在某个任务上的性能。

![image](https://github.com/user-attachments/assets/a0e03134-5a01-4cab-819a-2c8621c42a38)

![image](https://github.com/user-attachments/assets/3fbf6c62-08bc-4847-bbb7-6751677958bb)


#### 9.3 本章小结
本章介绍了循环神经网络的基本用途与理论定义方法，可以看到循环神经网络能够较好地对序列的离散数据进行处理，这是一种较好的处理方法。但是在实际应用中读者应该会发现，这种模型训练的结果差强人意。
但是读者不用担心，因为每个深度学习模型设计人员都是从最基本的内容开始学习的，后续我们还会学习更为高级的PyTorch编程方法。 
