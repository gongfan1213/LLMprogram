### 8.1.4 文本主题的提取：基于TF-IDF

一般来说，文本的提取主要涉及以下几种：

- 基于TF-IDF的文本关键字提取。

- 基于TextRank的文本关键词提取。



当然，除此之外，还有很多模型和方法能够帮助进行文本抽取，特别是对于大文本内容。本书由于篇幅关系，对这方面的内容并不展开描写，有兴趣的读者可以参考相关教程。下面先介绍基于TF-IDF的文本关键字提取。

1. **TF-IDF简介**

目标文本经过文本清洗和停用词的去除后，一般认为剩下的都是有着目标含义的词。如果需要对其特征进行进一步的提取，那么提取的应该是那些能代表文章的元素，包括词、短语、句子、标点以及其他信息的词。从词的角度考虑，需要提取对文章表达贡献度大的词。


TF-IDF是一种用于资讯检索与咨询勘测的常用加权技术。TF-IDF是一种统计方法，用来衡量一个词对一个文件集的重要程度。字词的重要性与其在文件中出现的次数成正比，而与其在文件集中出现的次数成反比。该算法在数据挖掘、文本处理和信息检索等领域得到了广泛的应用，最为常见的应用是从文章中提取关键词。



TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来进行分类。其中TF（Term Frequency）表示词条在文章（Document）中出现的频率。
词频（TF）=$\frac{某个词在单个文本中出现的次数}{某个词在整个语料库中出现的次数}$




IDF（Inverse Document Frequency）的主要思想是：如果包含某个词（Word）的文档越少，那么这个词的区分度就越大，也就是IDF越大。
逆文档频率（IDF）=$log(\frac{语料库的文本总数}{语料库中包含该词的文本数 + 1})$

而TF-IDF的计算实际上就是TF×IDF。
TF - IDF = 词频×逆文档频率 = TF×IDF
2. **TF-IDF的实现**
首先计算IDF，代码如下：
```python
import math
def idf(corpus):  # corpus为输入的全部语料文本库文件
    idfs = {}
    d = 0.0
    # 统计词出现的次数
    for doc in corpus:
        d += 1
        counted = []
        for word in doc:
            if not word in counted:
                counted.append(word)
                if word in idfs:
                    idfs[word] += 1
                else:
                    idfs[word] = 1
    # 计算每个词的逆文档值
    for word in idfs:
        idfs[word] = math.log(d/float(idfs[word]))
    return idfs
```
然后使用计算好的IDF计算每个文档的TF-IDF值：
```python
idfs = idf(agnews_text)  # 获取计算好的文本中每个词的IDF词频
for text in agnews_text:  # 获取文档集中的每个文档
    word_tfidf = {}
    for word in text:  # 依次获取每个文档中的每个词
        if word in word_tfidf:  # 计算每个词的词频
            word_tfidf[word] += 1
        else:
            word_tfidf[word] = 1
    for word in word_tfidf:
        word_tfidf[word] *= idfs[word]  # 计算每个词的TF-IDF值
```
计算TF-IDF的完整代码如下。
```python
import math
def idf(corpus):
    idfs = {}
    d = 0.0
    # 统计词出现的次数
    for doc in corpus:
        d += 1
        counted = []
        for word in doc:
            if not word in counted:
                counted.append(word)
                if word in idfs:
                    idfs[word] += 1
                else:
                    idfs[word] = 1
    # 计算每个词的逆文档值
    for word in idfs:
        idfs[word] = math.log(d/float(idfs[word]))
    return idfs
idfs = idf(agnews_text)  # 获取计算好的文本中每个词的IDF词频，agnews_text是经过处理后的语料库文档，在8.1.1节中详细介绍过了
for text in agnews_text:  # 获取文档集中的每个文档
    word_tfidf = {}
    for word in text:  # 依次获取每个文档中的每个词
        if word in word_idf:  # 计算每个词的词频
            word_tfidf[word] += 1
        else:
            word_tfidf[word] = 1
    for word in word_tfidf:
        word_tfidf[word] *= idfs[word]  # word_tfidf为计算后的每个词的TF-IDF值
values_list = sorted(word_tfidf.items(), key=lambda item: item[1], reverse=True)  # 按value排序
values_list = [value[0] for value in values_list]  # 生成排序后的单个文档
```
3. **将重排的文档根据训练好的Word2Vec向量建立一个有限量的词矩阵**
请读者自行完成。
4. **将TF-IDF单独定义一个类**
将TF-IDF的计算函数单独整合到一个类中，这样方便后续使用，代码如下。
```python
class TFIDF_score:
    def __init__(self,corpus,model = None):
        self.corpus = corpus
        self.model = model
        self.idfs = self.__idf()
    def __idf(self):
        idfs = {}
        d = 0.0
        # 统计词出现的次数
        for doc in self.corpus:
            d += 1
            counted = []
            for word in doc:
                if not word in counted:
                    counted.append(word)
                    if word in idfs:
                        idfs[word] += 1
                    else:
                        idfs[word] = 1
        # 计算每个词的逆文档值
        for word in idfs:
            idfs[word] = math.log(d / float(idfs[word]))
        return idfs
    def __get_TFIDF_score(self, text):
        word_tfidf = {}
        for word in text:  # 依次获取每个文档中的每个词
            if word in word_tfidf:  # 计算每个词的词频
                word_tfidf[word] += 1
            else:
                word_tfidf[word] = 1
        for word in word_tfidf:
            word_tfidf[word] *= self.idfs[word]  # 计算每个词的TF-IDF值
        values_list = sorted(word_tfidf.items(), key=lambda word_tfidf: word_tfidf[1], reverse=True)  # 将TF-IDF数据按重要程度从大到小排序
        return values_list
    def get_TFIDF_result(self,text):
        values_list = self.__get_TFIDF_score(text)
        value_list = []
        for value in values_list:
            value_list.append(value[0])
        return (value_list)
```
使用方法如下：
```python
tfidf = TFIDF_score(agnews_text)  # agnews_text为获取的数据集
for line in agnews_text:
    value_list = tfidf.get_TFIDF_result(line)
    print(value_list)
    print(model[value_list])
```
其中agnews_text为从文档中获取的正文数据集，也可以使用标题或者文档进行处理。

### 8.1.5 文本主题的提取：基于TextRank
TextRank算法的核心思想来源于著名的网页排名算法PageRank。PageRank是Sergey Brin与Larry Page于1998年在WWW7会议上提出来的，用来解决链接分析中网页排名的问题。在衡量一个网页的排名时，可以根据感觉认为：
- 当一个网页被越多网页所链接时，其排名会越靠前。
- 排名高的网页应具有更大的表决权，即当一个网页被排名高的网页所链接时，其重要性也会对应提高。

TextRank算法与PageRank算法类似，其将文本拆分成最小组成单元，即词汇，作为网络节点，组成词汇网络图模型。TextRank在迭代计算词汇权重时与PageRank一样，理论上是需要计算边权的，但是为了简化计算，通常会默认使用相同的初始权重，并在分配相邻词汇权重时进行均分。

1. **TextRank前置介绍**
TextRank用于对文本关键词进行提取，步骤如下：
（1）把给定的文本按照完整句子进行分割。
（2）对于每个句子，进行分词和词性标注处理，并过滤掉停用词，只保留指定词性的单词，如名词、动词、形容词等。
（3）构建候选关键词图$G=(V,E)$，其中$V$为节点集，由每个词之间的相似度作为连接的边值。
（4）根据以下公式，迭代传播各节点的权重，直至收敛。
$WS(V_i)=(1 - d)+d\times\sum_{V_j\in In(V_i)}\frac{W_{ji}}{\sum_{V_k\in Out(V_j)}W_{jk}}WS(V_j)$
对节点权重进行倒序排序，作为按重要程度排列的关键词。
2. **TextRank类的实现**
整体TextRank的实现如下。
```python
class TextRank_score:
    def __init__(self,agnews_text):
        self.agnews_text = agnews_text
        self.filter_list = self.__get_agnews_text()
        self.win = self.__get_win()
        self.agnews_text_dict = self.__get_TextRank_score_dict()
    def __get_agnews_text(self):
        sentence = []
        for text in self.agnews_text:
            for word in text:
                sentence.append(word)
        return sentence
    def __get_win(self):
        win = {}
        for i in range(len(self.filter_list)):
            if self.filter_list[i] not in win.keys():
                win[self.filter_list[i]] = set()
            if i - 5 < 0:
                lindex = 0
            else:
                lindex = i - 5
            for j in self.filter_list[lindex:i + 5]:
                win[self.filter_list[i]].add(j)
        return win
    def __get_TextRank_score_dict(self):
        time = 0
        score = {w: 1.0 for w in self.filter_list}
        while (time < 50):
            for k, v in self.win.items():
                s = score[k] / len(v)
                score[k] = 0
                for i in v:
                    score[i] += s
            time += 1
        agnews_text_dict = {}
        for key in score:
            agnews_text_dict[key] = score[key]
        return agnews_text_dict
    def __get_TextRank_score(self, text):
        temp_dict = {}
        for word in text:
            if word in self.agnews_text_dict.keys():
                temp_dict[word] = (self.agnews_text_dict[word])
        values_list = sorted(temp_dict.items(), key=lambda word_tfidf: word_tfidf[1], reverse=False)  # 将TextRank数据按重要程度从大到小排序
        return values_list
    def get_TextRank_result(self,text):
        temp_dict = {}
        for word in text:
            if word in self.agnews_text_dict.keys():
                temp_dict[word] = (self.agnews_text_dict[word])
        values_list = sorted(temp_dict.items(), key=lambda word_tfidf: word_tfidf[1], reverse=False)
        value_list = []
        for value in values_list:
            value_list.append(value[0])
        return (value_list)
```
TextRank是另一种能够实现关键词抽取的方法。除此之外，还有基于相似度聚类以及其他方法。对于本书提供的数据集来说，对于文本的提取并不是必需的。本节为选学内容，有兴趣的读者可以自行学习。

### 8.2 更多的词嵌入方法——FastText和预训练词向量
在实际的模型计算过程中，Word2Vec一个最常用也是最重要的作用是将“词”转换成“词嵌入（Word Embedding）”。

对于普通的文本来说，供人类了解和掌握的信息传递方式并不能简易地被计算机所理解，因此词嵌入是目前来说解决向计算机传递文字信息的最好的方式。

随着研究人员对词嵌入的研究深入和计算机处理能力的提高，更多、更好的方法被提出，例如新的FastText和使用预训练的词嵌入模型来对数据进行处理。

本节延续8.1节，介绍FastText的训练和预训练词向量的使用方法。

#### 8.2.1 FastText的原理与基础算法
相对于传统的Word2Vec计算方法，FastText是一种更为快速和新的计算词嵌入的方法，其优点主要有以下几个方面：
- FastText在保持高精度的情况下加快了训练速度和测试速度。
- FastText对词嵌入的训练更加精准。
- FastText采用两个重要的算法：N-Gram和Hierarchical Softmax。
1. **N-Gram架构**
相对于Word2Vec中采用的CBOW架构，FastText采用的是N-Gram架构。

其中，$x_1,x_2,\cdots,x_{N - 1},x_N$表示一个文本中的N-Gram向量，每个特征是词向量的平均值。这里顺便介绍一下N-Gram的意义。
常用的N-Gram架构有3种：1-Gram、2-Gram和3-Gram，分别对应一元、二元和三元。
以“我想去成都吃火锅”为例，对其进行分词处理，得到下面的数组：["我", "想", "去", "成都", "吃", "火锅"]。这就是1-Gram，分词的时候对应一个滑动窗口，窗口大小为1，所以每次只取一个值。
同理，假设使用2-Gram，就会得到["我想", "想去", "去成都", "成都吃", "吃火锅"]。N-Gram模型认为词与词之间有关系的距离为N，如果超过N，则认为它们之间没有联系，所以就不会出现“我成都”“我去”这些词。
如果使用3-Gram，就是["我想去", "想去成都", "去成都吃", "成都吃火锅"]。N理论上可以设置为任意值，但是一般设置成上面3个类型就足够了。
2. **Hierarchical Softmax架构**
当语料类别较多时，使用Hierarchical Softmax(hs)以减轻计算量。FastText中的Hierarchical Softmax利用Huffman树实现，将词向量作为叶子节点，之后根据词向量构建Huffman树。

Hierarchical Softmax的算法较为复杂，这里就不过多阐述了，有兴趣的读者可以自行研究。

#### 8.2.2 FastText训练及其与PyTorch 2.0的协同使用
前面介绍了FastText架构和理论，本小节开始使用FastText。这里主要介绍中文部分的FastText处理。
1. **数据收集与分词**
为了演示FastText的使用，构造数据集。
text中是一系列的短句文本，以每个逗号为一句进行区分，一个简单的处理函数如下：
```python
import jieba
jieba_cut_list = []
for line in text:
    jieba_cut = jieba.lcut(line)
    jieba_cut_list.append(jieba_cut)
print(jieba_cut)
```
可以看到，其中每一行根据jieba分词模型进行分词处理，之后保存在每一行中的是已经被分过词的数据。
2. **使用Gensim中的FastText进行词嵌入计算**
gensim.models中除了包含前文介绍过的Word2Vec函数外，还包含FastText的专用计算类。 
