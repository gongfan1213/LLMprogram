### 第3章 从零开始学习PyTorch 2.0
#### 3.2 自定义神经网络框架的基本设计（续）
**3.2.2 自定义神经网络框架的具体实现（续）**
```python
@property
def name(self):
    return self.__class__.__name__

@property
def param_names(self):
    return ()

@property
def nt_param_names(self):
    return ()

def _init_params(self):
    for name in self.param_names:
        self.params[name] = self.initializers[name](self.shapes[name])
    self.is_init = True
```
下面实现一个基本的神经网络计算层——全连接层。关于全连接层的详细介绍，我们在后续章节中会讲解，在这里主要将其作为一个简单的计算层来实现。

在全连接层的计算过程中，`forward`接受上层的输入`inputs`实现$\omega x + b$的计算；`backward`正好相反，接受来自反向的梯度。具体实现如下：

```python
class Dense(Layer):
    """A dense layer operates 'outputs = dot(inputs, weight) + bias'
    :param num_out: A positive integer, number of output neurons
    :param w_init: Weight initializer
    :param b_init: Bias initializer
    """
    def __init__(self,
                 num_out,
                 w_init=XavierUniform(),
                 b_init=Zeros()):
        super().__init__()
        self.initializers = {"w": w_init, "b": b_init}
        self.shapes = {"w": [None, num_out], "b": [num_out]}

    def forward(self, inputs):
        if not self.is_init:
            self.shapes["w"][0] = inputs.shape[1]
            self._init_params()
        self.ctx = {"X": inputs}
        return inputs @ self.params["w"] + self.params["b"]

    def backward(self, grad):
        self.grads["w"] = self.ctx["X"].T @ grad
        self.grads["b"] = np.sum(grad, axis=0)
        return grad @ self.params["w"].T

@property
def param_names(self):
    return "w", "b"
```

在这里我们实现一个可以计算的`forward`函数，其目的是对输入的数据进行前向计算，具体计算结果如下：

```python
tensor = np.random.random(size=(10, 28, 28, 1))
tensor = np.reshape(tensor, newshape=[10,28*28])
res = Dense(512).forward(tensor)
```
上面代码生成了一个随机数据集，再通过`reshape`函数对其进行折叠，之后使用我们自定义的全连接层对其进行计算。最终结果请读者自行打印查看。

3. **激活层的基类与实现**

神经网络框架中的另一个重要的部分是激活函数。激活函数可以看作是一种网络层，同样需要实现`forward`和`backward`方法。我们通过继承`Layer`基类实现激活函数类，这里实现了常用的ReLU激活函数。`forward`和`backward`方法分别实现对应激活函数的正向计算和梯度计算，代码如下：

```python
#activity_layer
import numpy as np

class Layer(object):
    def __init__(self, name):
        self.name = name
        self.params, self.grads = None, None

    def forward(self, inputs):
        raise NotImplementedError

    def backward(self, grad):
        raise NotImplementedError

class Activation(Layer):
    """Base activation layer"""
    def __init__(self, name):
        super().__init__(name)
        self.inputs = None

    def forward(self, inputs):
        self.inputs = inputs
        return self.forward_func(inputs)

    def backward(self, grad):
        return self.backward_func(self.inputs) * grad

    def forward_func(self, x):
        raise NotImplementedError

    def backward_func(self, x):
        raise NotImplementedError

class ReLU(Activation):
    """ReLU activation function"""
    def __init__(self):
        super().__init__("ReLU")

    def forward(self, x):
        return np.maximum(x, 0.0)

    def backward_func(self, x):
        return x > 0.0
```

这里需要注意，对于具体的`forward`和`backward`实现函数，需要实现一个特定的需求对应的函数，从而完成对函数的计算。

4. **辅助网络更新的基类——Net**


对于神经网络来说，误差需要在整个模型中传播，即正向（Forward）传播和反向（Backward）传播。正向传播的实现方法很简单，按顺序遍历所有层，每层计算的输出作为下一层的输入；反向传播则逆序遍历所有层，将每层的梯度作为下一层的输入。

这一部分的具体实现需要建立一个辅助网络参数更新的网络基类，其作用是对每一层进行`forward`和`backward`计算，并更新各个层中的参数。为了达成这个目标，我们建立一个`model`基类，其作用是将每个网络层参数及其梯度保存下来。具体实现的`model`类如下：

```python
# Net
class Net(object):
    def __init__(self, layers):
        self.layers = layers

    def forward(self, inputs):
        for layer in self.layers:
            inputs = layer.forward(inputs)
        return inputs

    def backward(self, grad):
        all_grads = []
        for layer in reversed(self.layers):
            grad = layer.backward(grad)
            all_grads.append(layer.grads)
        return all_grads[::-1]

    def get_params_and_grads(self):
        for layer in self.layers:
            yield layer.params, layer.grads

    def get_parameters(self):
        return [layer.params for layer in self.layers]

    def set_parameters(self, params):
        for i, layer in enumerate(self.layers):
            for key in layer.params.keys():
                layer.params[key] = params[i][key]
```

5. **损失函数计算组件与优化器**

对于神经网络的训练来说，损失的计算与参数优化是必不可少的操作。对于损失函数组件来说，给定了预测值和真实值，需要计算损失值和关于预测值的梯度。我们分别使用`loss`和`grad`两个方法来实现。


具体而言，我们需要实现基类的损失（loss）函数与优化器（optimizer）函数。损失函数如下：

```python
# loss
class BaseLoss(object):
    def loss(self, predicted, actual):
        raise NotImplementedError

    def grad(self, predicted, actual):
        raise NotImplementedError
```
而优化器的基类需要实现根据当前的梯度，计算返回实际优化时每个参数改变的步长，代码如下：
```python
# optimizer
class BaseOptimizer(object):
    def __init__(self, lr, weight_decay):
        self.lr = lr
        self.weight_decay = weight_decay

    def compute_step(self, grads, params):
        step = list()
        # flatten all gradients
        flatten_grads = np.concatenate([np.ravel(v) for v in grads.values()])
        # compute step
        flatten_step = self._compute_step(flatten_grads)
        # reshape gradients
        p = 0
        for param in params:
            layer = dict()
            for k, v in param.items():
                block = np.prod(v.shape)
                _step = flatten_step[p:p+block].reshape(v.shape)
                _step -= self.weight_decay * v
                layer[k] = _step
                p += block
            step.append(layer)
        return step

    def _compute_step(self, grad):
        raise NotImplementedError
```
下面是对这两个类的具体实现。对于损失函数来说，我们最常用的也就是第2章所使用的多分类损失函数——多分类Softmax交叉熵。具体的数学形式如下（关于此损失函数的计算，读者可对3.1.2节有关CrossEntropy的计算进行学习）：
\[ \text{cross}(y_{\text{true}}, y_{\text{pred}}) = - \sum_{i=1}^{N} y(i) \times \log(y_{\text{pred}}(i)) \]
具体实现形式如下：

![image](https://github.com/user-attachments/assets/f98c50d9-50f8-4109-a8c2-c48773cc3725)



```python
class CrossEntropyLoss(BaseLoss):
    def loss(self, predicted, actual):
        m = predicted.shape[0]
        exps = np.exp(predicted - np.max(predicted, axis=1, keepdims=True))
        p = exps / np.sum(exps, axis=1, keepdims=True)
        nll = -np.log(np.sum(p * actual, axis=1, keepdims=True))
        return np.sum(nll) / m

    def grad(self, predicted, actual):
        m = predicted.shape[0]
        grad = np.copy(predicted)
        grad -= actual
        return grad / m
```

这里需要注意的是，我们在设计优化器时并没有进行归一化处理，因此在使用之前需要对分类数据进行one - hot表示，对其进行表示的函数如下：

```python
def get_one_hot(targets, nb_classes=10):
    return np.eye(nb_classes)[np.array(targets).reshape(-1)]
```

对于优化器来说，其公式推导较为复杂，我们在这里只实现常用的Adam优化器，具体数学推导部分有兴趣的读者可自行研究学习。

```python
class Adam(BaseOptimizer):
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8, weight_decay=0.0):
        super().__init__(lr, weight_decay)
        self.b1, self.b2 = beta1, beta2
        self.eps = eps
        self.t = 0
        self.m, self.v = 0, 0

    def _compute_step(self, grad):
        self.t += 1
        self.m = self.b1 * self.m + (1 - self.b1) * grad
        self.v = self.b2 * self.v + (1 - self.b2) * (grad ** 2)
        # bias correction
        _m = self.m / (1 - self.b1 ** self.t)
        _v = self.v / (1 - self.b2 ** self.t)
        return -self.lr * _m / (_v ** 0.5 + self.eps)
```

6. **整体model类的实现**
   
`Model`类实现了我们一开始设计的3个接口：`forward`、`backward`和`apply_grad`。在`forward`方法中，直接调用`net`的`forward`方法，在`backward`方法中，把`net`、`loss`、`optimizer`串联起来，首先计算损失（loss），然后进行反向传播得到梯度，接着由`optimizer`计算步长，最后通过`apply_grad`对参数进行更新，代码如下：
```python
class Model(object):
    def __init__(self, net, loss, optimizer):
        self.net = net
        self.loss = loss
        self.optimizer = optimizer

    def forward(self, inputs):
        return self.net.forward(inputs)

    def backward(self, preds, targets):
        loss = self.loss.loss(preds, targets)
        grad = self.loss.grad(preds, targets)
        grads = self.net.backward(grad)
        params = self.net.get_parameters()
        step = self.optimizer.compute_step(grads, params)
        return loss, step

    def apply_grad(self, grads):
        for grad, (param, _) in zip(grads, self.net.get_params_and_grads()):
            for k, v in param.items():
                param[k] += grad[k]
```
在`Model`类中，我们串联了损失函数、优化器以及对应的参数更新方法，从而将整个深度学习模型作为一个完整的框架进行计算。

7. **基于自定义框架的神经网络框架的训练**
下面进行最后一步，基于自定义框架的神经网络模型的训练。如果读者遵循作者的提示，在一开始对应train.py方法对模型的各个组件进行学习，那么相信在这里能够比较轻松地完成本小节的最后一步。完整的自定义神经网络框架训练如下：
```python
import numpy as np

def get_one_hot(targets, nb_classes=10):
    return np.eye(nb_classes)[np.array(targets).reshape(-1)]

train_x = np.load("../../dataset/mnist/x_train.npy")
train_x = np.reshape(train_x, [60000, 784])
train_y = get_one_hot(np.load("../../dataset/mnist/y_train_label.npy"))

import net, model, layer, loss, optimizer

net = net.Net([
    layer.Dense(200),
    layer.ReLU(),
    layer.Dense(100),
    layer.ReLU(),
    layer.Dense(70),
    layer.ReLU(),
    layer.Dense(30),
    layer.ReLU(),
    layer.Dense(10)
])

model = model.Model(net=net, loss=loss.SoftmaxCrossEntropy(),
                    optimizer=optimizer.Adam(lr=2e-4))

loss_list = list()
train_num = 60000 // 128
for epoch in range(20):
    train_loss = 0
    for i in range(train_num):
        start = i * 128
        end = (i + 1) * 128
        inputs = train_x[start:end]
        targets = train_y[start:end]
        pred = model.forward(inputs)
        loss, grads = model.backward(pred, targets)
        model.apply_grad(grads)
        if (i + 1) % 10 == 0:
            test_pred = model.forward(inputs)
            real_pred_idx = np.argmax(test_pred, axis=1)
            counter = 0
            for pre, rel in zip(real_pred_idx, np.argmax(targets, axis=1)):
                if pre == rel:
                    counter += 1
            print("train_loss:", round(loss, 2), "accuracy:", round(counter/128, 2))
```
最终训练结果如下：
```
train_loss: 1.52 accuracy: 0.73
train_loss: 0.78 accuracy: 0.84
train_loss: 0.54 accuracy: 0.88
train_loss: 0.34 accuracy: 0.91
train_loss: 0.33 accuracy: 0.88
train_loss: 0.38 accuracy: 0.85
train_loss: 0.28 accuracy: 0.93
train_loss: 0.23 accuracy: 0.94
train_loss: 0.31 accuracy: 0.9
train_loss: 0.18 accuracy: 0.95
```
可以看到，随着训练的深入进行，此时损失值在降低，而准确率随着训练次数的增加在不停地增高，具体请读者自行演示学习。

#### 3.3 本章小结
本章演示了使用PyTorch框架进行手写体数字识别的实战案例，我们完整地对MNIST手写体图片做了分类，同时讲解了模型的标签问题，以及后期常用的损失函数计算方面的内容。可以说CrossEntropy损失函数将会是深度学习最重要的损失函数，需要读者认真学习。

同时，本章通过自定义一个深度学习框架，完整地演示了深度学习框架的设计过程，并且讲解了各部分的工作原理以及最终组合在一起运行的流程，引导读者进一步熟悉深度学习框架。 
