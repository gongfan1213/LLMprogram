此时，火箭回收的最终得分图如图13 - 4所示。

![image](https://github.com/user-attachments/assets/80a7bdfb-69f4-40e9-a730-730574348de3)

### 13.1.4 强化学习的基本内容

在完成了强化学习的实战代码后，下面将讲解一些强化学习的基本理论，从而帮助读者加深对强化学习的理解。

![image](https://github.com/user-attachments/assets/73bd4784-ebe8-4873-a9e4-a33a43e67f9c)


1. **强化学习的总体思想**

强化学习背后的思想是，代理（Agent）将通过与环境（Environment）的动作（Action）交互，进而获得奖励（Reward）。

从与环境的交互中进行学习，这一思想来自于我们的自然经验，想象一下当你是个孩子的时候，看到一团火，并尝试接触它，如图13 - 5所示。

你觉得火很温暖，你感觉很开心（奖励 +1），你就会觉得火是个好东西，如图13 - 6所示。

然而，一旦你尝试去触摸它，就会狠狠地被教训，即火把你的手烧伤了（惩罚 -1），如图13 - 7所示。你才会明白只有与火保持一定距离，火才会产生温暖，才是个好东西，但如果太过靠近的话，就会烧伤自己。


这一过程是人类通过交互进行学习的方式。强化学习是一种可以根据行为进行计算的学习方式，如图13 - 8所示。

举个例子，思考如何训练Agent学会玩超级玛丽游戏。这一强化学习过程可以被建模为如下的一组循环过程：

- Agent从环境中接收到状态S₀。

- 基于状态S₀，Agent执行A操作。

- 环境转移至新状态S₁。

- 环境给予R₁奖励。

![image](https://github.com/user-attachments/assets/9953881f-f37f-4b04-a98a-aaae7399a8b4)



强化学习的整体过程中会循环输出状态、行为、奖励的序列，而整体的目标是最大化全局奖励的期望。

2. **强化学习的奖励与衰减**

奖励与衰减是强化学习的核心思想，在强化学习中，为了得到最好的行为序列，我们需要最大化累积奖励的期望，也就是奖励的最大化是强化学习的核心目标。

对于奖励的获取，每个时间步的累积奖励可以写作：

![image](https://github.com/user-attachments/assets/edafb964-0ee1-48b9-9a9f-3257a9b734aa)



\[ G_t = R_{t+1} + R_{t+2} + \cdots \]
等价于：
\[ G_t = \sum_{k=0}^{\infty} R_{t+k+1} \]

但是相对于长期奖励来说，更简单的是对短期奖励的获取，短期奖励来的很快，且发生的概率非常大，因此比起长期奖励，短期奖励更容易预测。

用图13 - 9所示的猫捉老鼠例子来说明，Agent是老鼠，对手是猫，目标是在被猫吃掉之前，先吃掉最多的奶酪。

从图13 - 9中可以看到，吃掉身边的奶酪要比吃掉猫旁边的奶酪要容易许多。

但是由于一旦被猫抓住，游戏就会结束，因此猫身边的奶酪奖励会有衰减，也要把这个因素考虑进去，对折扣的处理如下（定义Gamma为衰减比例，取值范围为0~1）：

- Gamma越大，带来的衰减越小。这意味着Agent的学习过程更关注长期的回报。

- Gamma越小，带来的衰减越大。这意味着Agent更关注短期的回报。

![image](https://github.com/user-attachments/assets/8609f16f-c3f9-4869-b1d3-58a8e32a63d8)




衰减后的累计奖励期望为：

\[ G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \text{ where } \gamma \in [0,1] \]

\[ = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} \cdots \]

每个时间步之间的奖励将与Gamma参数相乘，以获得衰减后的奖励值。随着时间步的增加，猫距离我们更近，因此未来的奖励概率将变得越来越小。

3. **强化学习的任务分类**

任务是强化学习问题中的基础单元，任务分为两类：事件型任务与持续型任务。

事件型任务指的是在这个任务中，有一个起始点和终止点（终止状态）。这会创建一个事件：一组状态、行为、奖励以及新奖励。对于超级玛丽来说，一个事件从游戏开始进行记录，直到角色被杀结束，如图13 - 10所示。

而持续型任务意味着任务不存在终止状态。在这个任务中，Agent将学习如何选择最好的动作，并与环境同步交互。例如，通过Agent进行自动股票交易，这个任务不存在起始点和终止状态，在我们主动终止之前，Agent将一直运行下去，如图13 - 11所示。

![image](https://github.com/user-attachments/assets/e4a15ef3-3405-4780-8081-cfc7ee2584e3)


4. **强化学习的基本处理方法**

![image](https://github.com/user-attachments/assets/44a2b52b-b69a-4173-877f-ca19835aba14)


对于一般的强化学习来说，其主要的学习与训练方法有两种，分别是基于值函数的学习方法与基于策略梯度的学习方法，另外还有把两者结合起来的AC算法。分别说明如下：

- 基于值函数的学习方法来学习价值函数，计算每个动作在当前环境下的价值，目标就是获取最大的动作价值，即每一步采取回报最大的动作和环境进行互动。

- 基于策略梯度的学习方法来学习策略函数，计算当前环境下每个动作的概率，目标是获取最大的状态价值，即该动作发生后期望回报越大越好。

- AC算法：融合了上述两种方法，将价值函数和策略函数一起进行优化。价值函数负责在环境学习并提升自己的价值判断能力，而策略函数则接受价值函数的评价，尽量采取在价值函数中可以得到高分的策略。

![image](https://github.com/user-attachments/assets/83b4d2e9-ef35-4eff-9ae9-ca0666eee69f)



读者可以参考走迷宫的例子来学习。走迷宫时每个步骤的评分是由基于值的算法在计算环境反馈后得出，如图13 - 12所示。在迷宫问题中，在每一步对周围环境进行打分，将选择得分最高的前进，即 -7、-6、-5等。而这里每个步骤的评分是由基于策略梯度的算法在计算实施动作后得出的，如图13 - 13所示。

这个过程中的每个动作（也就是其行进方向）是由模型决定的。

这两种方法一种施加在环境中，另一种施加在动作人上，各有利弊。因此，为了取长补短，研究人员提出了一种新的处理方法——AC算法，如图13 - 14所示。



AC算法将基于值函数和基于策略梯度的学习方法做了结合，同时对环境和施用者进行建模，从而可以获得更好的环境适配性。

AC算法分为两部分：Actor用的是Policy Gradient，它可以在连续动作空间内选择合适的动作；Critic用的是Q-Learning，它可以解决离散动作空间的问题。除此之外，又因为Actor只能在一个回合之后进行更新，导致学习效率较慢，Critic的加入就可以使用TD方法实现单步更新。这样两种算法相辅相成，形成了AC算法。

Actor对每个动作（Action）做出概率输出，而Critic会对输出进行评判，之后将评判结果返回给Actor，从而修正下一次Actor做出的结果。


![image](https://github.com/user-attachments/assets/79ca4f57-16d7-4d16-9c84-c1d3b25cb4a7)


### 13.2 强化学习的基本算法——PPO算法

一般强化学习过程中，一份数据只能进行一次更新，更新完就只能丢掉，再等待下一份数据。但是这种方式对深度学习来说是一种极大的浪费，尤其在强化学习中，数据的获取是弥足珍贵的。

因此，我们需要一种新的算法，能够通过获得完整的数据内容进行模型的多次更新，即需要对每次获取到的数据进行多次利用，而进行多次利用的算法，具有代表性的就是PPO（Proximal Policy Optimization，近线策略优化）算法。



#### 13.2.1 PPO算法简介

![image](https://github.com/user-attachments/assets/b3693cad-da75-4a64-84bf-52f940d793d0)


PPO算法是AC算法框架下的一种强化学习代表算法，在采样策略梯度算法训练的同时，还可以重复利用历史的采样数据进行网络参数更新，提升了策略梯度算法的效率。

PPO算法的突破在于对新旧策略函数进行约束，希望新的策略网络与旧的策略网络越接近越好，即实现近线策略优化的本质目的：新网络可以利用旧网络学习到的数据进行学习，不希望这两个策略相差很大。PPO损失函数的公式如下：

\[ L^{clip+vf+ent}(\theta) = E\left[ L^{clip}(\theta) - c_1 L^{vf}(\theta) + c_2 S[\pi_{\theta}](S_t) \right] \]

参数说明如下：

- \( L^{clip} \)：价值网络评分，即Critic网络评分，采用clip的方式使得新旧网络的差距不会过大。

- \( L^{vf} \)：价值网络预测的结果和真实环境的回报值越接近越好。

- \( S \)：策略网络的输出结果，这个值越大越好，目的是希望策略网络的输出分布概率不要太过于集中，提高了不同动作在环境中发生的可能。



#### 13.2.2 函数使用说明
在讲解PPO算法时，需要用到一些特定的函数，这些函数以前没有用过，我们先讲解一下。

1. **Categorical类**

Categorical类的作用是根据传递的数据概率建立相应的数据抽样分布，其使用如下：

```python
import torch
from torch.distributions import Categorical

action_probs = torch.tensor([0.3,0.7])  #首先人工建立一个概率值
dist = Categorical(action_probs)  #根据概率进行建立分布
c0 = 0
c1 = 1
for _ in range(10240):
    action = dist.sample()  #根据概率分布进行抽样
    if action == 0:
        c0 += 1  #对抽样结果进行存储
    else:
        c1 += 1
print("c0的概率为: ",c0/(c0 + c1))  #打印输出的结果
print("c1的概率为: ",c1/(c0 + c1))
```

首先人工建立一个概率值，之后Categorical类帮助我们建立依照这个概率构成的分布函数，sample的作用是依据存储的概率进行抽样。

从最终的打印结果可以看到，输出的结果可以反映人工概率的分布。
```
c0的概率为: 0.3014354066985646
c1的概率为: 0.6985645933014354
```

2. **log_prob函数**

log_prob(x)用来计算输入数据x的概率密度的对数值，读者可以通过如下代码段进行验证：
```python
import torch
from torch.distributions import Categorical

action_probs = torch.tensor([0.3,0.7])
#输出不同分布的log值
print(torch.log(action_probs))
#根据概率建立一个分布并抽样
dist = Categorical(action_probs)
action = dist.sample()
#获取抽样结果对应的分布log值
action_logprobs = dist.log_prob(action)
print(action_logprobs)
```
通过打印结果可以看到，首先输出了不同分布的log值，之后再反查出不同取值所对应的分布log值。
```
c0的概率为: 0.3014354066985646
c1的概率为: 0.6985645933014354
```
3. **entropy函数**

在前面讲解的过程中涉及交叉熵（crossEntropy）相关内容，而entropy用于计算数据中蕴含的信息量，在这里熵的计算如下：

```python
import torch
from torch.distributions import Categorical

action_probs = torch.tensor([0.3,0.7])
#自定义的entropy实现
def entropy(data):
    min_real = torch.min(data)
    logits = torch.clamp(data,min=min_real)
    p_log_p = logits * torch.log(data)
    return -p_log_p.sum(-1)

print(entropy(action_probs))
```
读者可以对这里自定义的entropy与PyTorch 2.0自带的entropy计算方式进行比较，代码如下：
```python
import torch
from torch.distributions import Categorical

action_probs = torch.tensor([0.3,0.7])
#根据概率建立一个分布并抽样
dist = Categorical(action_probs)
dist_entropy = dist.entropy()
print("dist_entropy: ",dist_entropy)

#自定义的entropy实现
def entropy(data):
    min_real = torch.min(data)
    logits = torch.clamp(data,min=min_real)
    p_log_p = logits * torch.log(data)
    return -p_log_p.sum(-1)

print("self_entropy: ",entropy(action_probs))
```
从最终结果可以看到，两者的计算结果是一致的。
```
dist_entropy: tensor(0.6109)
self_entropy: tensor(0.6109)
```


#### 13.2.3 一学就会的TD-error理论介绍

下面介绍ChatGPT中的一个非常重要的理论算法——TD-error，TD-error的作用是动态地解决后续数据量的估算问题，简单说就是：TD-error主要是让我们明确分段思维，而不能凭主观评价经验来对事物进行估量，如图13 - 15所示。

1. **项目描述与模型预估**

在图13 - 15右侧，一名司机需要驾车从NYC到Atlanta，中途有个中转站DC。按照现有的先验知识可以获得如下信息：

- NYC到Atlanta的距离为90千米，而DC中转站位于距离出发点NYC 300千米的中转站。

- 训练好的模型预估整体路途需要耗时1000分钟。

- 训练好的模型预估从NYC到DC耗时400分钟。

- 训练好的模型预估从DC到Atlanta耗时600分钟。


![image](https://github.com/user-attachments/assets/ff90f787-b13f-4465-959f-72fa2fb6a519)


这是对项目的描述，完整用到预估的知识内容。这里需要注意的是，整体1000分钟的耗时是由离线模型在出发前预先训练好的，而不能根据具体情况随时调整。

2. **到达DC后模型重新估算整体耗时**

当司机实际到达中转站DC时，发现耗时只有300分钟，此时如果模型进一步估算余下的路程时间，按照出发前的算法，剩余时间应该为1000 - 400 = 600分钟。此时进一步估算整体用时，可以使用公式如下：

\[ 900 = 300 + 600 \]

这是模型在DC估算的整体用时。其中300为出发点NYC到DC的耗时，而600为模型按原算法估算的DC到Atalanta的耗时。整体900为已训练模型在DC估算的总体耗时。

此时，如果模型在中转站重新进行时间评估的话，到达终点的整体耗费时间就会变为900。

3. **问题**

有读者可能会问，为什么不用按比例缩短的剩余时间进行估算，即剩下的时间变为：

\[ V_{future} = 600 \times 300 / 400 \]

这样做的问题在于，我们需要相信前期模型做出的预测是基于很好的训练做出来的一个可信度很高的值，不能人为随意地对整体的路途进行修正，即前一段路途可能因为种种原因（顺风、逆风等）造成了时间变更，但是并不能保证在后续的路途同样会遇到这样的情况。



因此，我们在剩余的这次模型拟合过程中，依旧需要假定模型对未来的原始拟合是正确的，而不能加入自己的假设。

有读者可能会继续问，如果下面再遇到一些状况，修正了原计划的路途，怎么办呢？一个非常好的解决办法就是以那个时间段为中转站重新训练整个模型。把前面路过的作为前部分，后面没有路过的作为后部分处理。

在这个问题中，我们把整体的路段分成了若干份，每隔一段就重新估算时间，这样使得最终的时间与真实时间的差值不会太大。

4. **TD-error**

此时模型整体估算的差值100 = 1000 - 900，这点相信读者很容易理解，即TD-error代表的是现阶段（也就是在DC位置）估算时间与真实时间的差值为100。

可以看到，这里的TD-error实际上就是根据现有的误差修正整体模型的预估结果，这样可以使得模型在拟合过程中更好地反映真实的数据。



#### 13.2.4 基于TD-error的结果修正
本小节会涉及PPO算法的一些细节部分。

1. **修正后的模型做出的预测不应该和未修正的模型做出的预测有太大的差别**

继续13.2.3节的例子，如果按原始的假设，对于总路程的拟合分析，在DC中转站估算的耗费时间为：

- 错误的模型估算时间：300 + 600×300 / 400 = 750

- 模型应该输入的时间：300 + 600 = 900



此时，除了在前面讲的加入训练人的主观因素外，还有一个比较重要的原因，是相对于原始的估算值1000，模型对于每次修正的幅度太大（错误的差距为250，而正确的差距为100），这样并不适合模型尽快地使用已有的数据重新拟合剩下的耗费时间。换算到模型输出，其决策器的输出跳跃比较大，很有可能造成模型失真的问题。

下面回到PPO算法的说明，对于每次做出动作的决定，决策器policy会根据更新生成一个新的分布，我们可以将其记作 \( p\theta'(at \mid st) \)，而对于旧的 \( p\theta(at \mid st) \)，这两个分布差距太大的话，也就是变化过大，会导致模型难以接受。读者可以参考下面两个分布的修正过程：

- \([0.1,0.2] \to [0.15,0.20] \to [0.25,0.30] \to [0.45,0.40]\)  一个好的分布修正过程

- \([0.1,0.2] \to [1


### 13.2.4 基于TD - error的结果修正（续）

![image](https://github.com/user-attachments/assets/8381c7ae-a3a1-4866-8e41-17efabab4cf9)


1. **修正后的模型做出的预测不应该和未修正的模型做出的预测有太大的差别（续）**


![image](https://github.com/user-attachments/assets/750097c1-7ce0-4f04-a425-c6592144be8d)



![image](https://github.com/user-attachments/assets/f9a275cb-f788-4390-bacb-6757aafa6d29)


\[ [0.1,0.2] \to [1.5,0.48] \to [1.2,4.3] \to [-0.1,0.7] \] 一个坏的分布修正过程

这部分的实现可以参考示例源码中的这条代码进行解读：

\[ \text{ratios} = \text{torch.exp}(\text{logprobs} - \text{old_logprobs.detach}()) \]

具体公式可以参考图13 - 16。

\[ \nabla \bar{R}_\theta = E_{\tau \sim p_\theta(\tau)}[R(\tau)\nabla \text{log}p_\theta(\tau)] \]

 - 使用 \(\pi_\theta\) 来收集数据。当 \(\theta\) 更新时，我们要能够再次使用训练数据。
 
 - 目标：使用来自 \(\pi_{\theta'}\) 的样本训练 \(\theta\)。\(\theta'\) 是固定的，所以我们可以重复使用样本数据。

\[ \nabla \bar{R}_\theta = E_{\tau \sim p_{\theta'}(\tau)}\left[\frac{p_\theta(\tau)}{p_{\theta'}(\tau)}R(\tau)\nabla \text{log}p_\theta(\tau)\right] \]

 - 对数据进行采样。
 
 - 使用这些数据多次训练 \(\theta\)。

![image](https://github.com/user-attachments/assets/396895a1-6193-4363-b0b8-cfe4992c8ae7)


2. **对于模型每次输出概率的权重问题**

对于以下公式：

 - 模型应该输入的时间：\(300 + 600 = 900\)
 
 - \( \text{TD - error} = 1000 - 900 = 100 \)
 
 - \( \text{Adventure} = \frac{100}{1000} = 0.1 \)

如果我们继续对下面的路径进行划分，对于不同的路径，可以得到如下的TD - error序列：
 - \( \text{TD - error}_1 = 80 \)
 - \( \text{TD - error}_2 = 50 \)
 - \( \text{TD - error}_3 = 20 \)

对于后期多次的模型拟合，输出新的动作概率时，需要一种连续的概率修正方法，即将当前具体的动作概率输出与不同的整体结果误差的修正联系在一起，具体实现如下：

![image](https://github.com/user-attachments/assets/53fd25ca-179a-4095-88af-36aa78deb72f)


![image](https://github.com/user-attachments/assets/e5b5b00e-4e48-44e0-9f5e-5709932f3f7c)


```python
ratios = torch.exp(logprobs - old_logprobs.detach())
advantages = rewards - state_values.detach()  #多个advantage组成的序列
surr1 = ratios * advantages
```
\(\text{advantages}\) 表示新的输出对原有输出的改变和修正，具体公式如图13 - 17所示。

\[ \text{Gradient for update} = E_{(s_t,a_t) \sim \pi_\theta}[A^\theta(s_t,a_t)\nabla \text{log}p_\theta(a_t|s_t^\pi)] \]

\[ = E_{(s_t,a_t) \sim \pi_{\theta'}} \left[ \frac{p_\theta(s_t,a_t)}{p_{\theta'}(s_t,a_t)} A^{\theta'}(s_t,a_t) \nabla \text{log}p_\theta(a_t|s_t^\pi) \right] \]

其中画线部分出现的就是离散后的 \(\text{advantages}\)，其作用是对输出的概率进行修正。

![image](https://github.com/user-attachments/assets/1cb52bac-898f-4241-a45a-ef79f7b39913)



#### 13.2.5 对于奖励的倒序构成的说明
关于奖励的构成方法，实现代码如下：


```python
# 预测状态回报
rewards = []
discounted_reward = 0  # discounted = 不重要
# 这里是不可以这样理解，当前步骤是决定未来的步骤，而模型需要根据当前步骤对未来的最终结果进行修正，如果遵循了当前步骤，就可以看到未来的结果如何
# 这样未来的结果会很差，所以模型需要远离会造成坏的结果的步骤
# 所以就反过来计算
# print(len(self.memory.rewards),len(self.memory.is_dones))这里就是做成批次，1200批次数据做一次
for reward, is_done in zip(reversed(memory.rewards), reversed(memory.is_dones)):
    # 回合结束:
    if is_done:
        discounted_reward = 0
    # 更新削减奖励(当前状态奖励 + 0.99*上一状态奖励)
    discounted_reward = reward + (self.gamma * discounted_reward)
    # 首插入
    rewards.insert(0, discounted_reward)
# print(len(rewards))  #这里的长度就是根据batch_size的长度设置的
# 标准化奖励
rewards = torch.tensor(rewards, dtype=torch.float32)
rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
```

在这段代码中，首先初始化一个空列表 \(\text{rewards}\) 用于存储处理后的奖励值，\(\text{discounted_reward}\) 初始化为0 。通过倒序遍历存储的奖励 \(\text{memory.rewards}\) 和游戏结束标志 \(\text{memory.is_dones}\) ：当遇到游戏结束标志（\(\text{is_done}\) 为真）时，将 \(\text{discounted_reward}\) 重置为0 ；否则，按照折扣因子 \(\text{self.gamma}\) 来更新 \(\text{discounted_reward}\) （即当前奖励加上折扣后的上一状态奖励 ），并将更新后的 \(\text{discounted_reward}\) 插入到 \(\text{rewards}\) 列表开头。最后，将 \(\text{rewards}\) 转换为张量并进行标准化处理（减去均值后除以标准差，为防止除零添加一个极小值 \(1e - 5\)  ） 。这样处理是为了在强化学习中更好地计算和利用奖励信号，引导智能体学习到更优的策略 。 



```
for reward, is_done in zip(reversed(memory.rewards), reversed(memory.is_dones)):
    # 回合结束
    if is_done:
        discounted_reward = 0
    # 更新削减奖励(当前状态奖励 + 0.99*上一状态奖励)
    discounted_reward = reward + (self.gamma * discounted_reward)
    # 首插入
    rewards.insert(0, discounted_reward)
# 标准化奖励
rewards = torch.tensor(rewards, dtype=torch.float32)
rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
```
可以看到，在这里对获取的奖励进行倒转，之后将奖励得分进行叠加，对于这部分的处理，读者可以这样理解：当前步骤是决定未来的步骤，而模型需要根据当前步骤对未来的最终结果进行修正，如果遵循了当前步骤，就可以看到未来的结果如何，如果未来的结果很差，模型就需要尽可能远离造成此结果的步骤，即对输出进行修正。

### 13.3 本章小结
本章是强化学习的实战部分，由于涉及较多的理论讲解，因此难度较大，但是相信通过本章的讲解，读者可以了解并掌握强化学习模型，并可以独立训练成功一个强化学习模型。


读者可以根据自身的需要继续强化学习模型的学习，本章选用的是一个比较简单的火箭回收例子，其作用只是抛砖引玉，向读者介绍强化学习的基本算法和训练方式。 



