# 第11章 站在巨人肩膀上的预训练模型 BERT


学看到这里，读者应该对使用深度学习框架PyTorch 2.0进行自然语言处理有了一个基础性的认识，如果按部就班地学习，那么你会觉得这部分内容也不是很难。

在第10章介绍了一种新的基于注意力模型的编码器，如果读者在学习第10章内容时注意到，作为编码器的encoder_layer与用于分类的dense_layer（全连接层）可以分开独立使用，那么一个自然而然的想法就是能否将编码器层和全连接层分开，利用训练好的模型作为编码器独立使用，并且可以根据具体项目接上不同的“尾端”，以便在预训练好的编码器上通过“微调”进行训练。

有了想法就要行动起来。



## 11.1 预训练模型BERT

BERT（Bidirectional Encoder Representation from Transformer）是2018年10月由Google AI研究院提出的一种预训练模型。其使用了第10章介绍的编码器结构的层级和构造方法，最大的特点是抛弃了传统的循环神经网络和卷积神经网络，通过注意力模型将任意位置的两个单词的距离转换成1，有效地解决了自然语言处理中棘手的文本长期依赖问题，如图11-1所示。

![image](https://github.com/user-attachments/assets/7b62ba88-37b8-4a2d-b3bc-7bb1c904edae)


BERT实际上是一种替代了Word Embedding的新型文字编码方案，是目前计算文字在不同文本中的语境而“动态编码”的最优方法。BERT被用来学习文本句子的语义信息，比如经典的词向量表示。BERT包括句子级别的任务（如句子推断、句子间的关系）和字符级别的任务（如实体识别）。

## 11.1.1 BERT的基本架构与应用

BERT的模型架构是一个多层的双向注意力结构的Encoder部分。本节先来看BERT的输入，再来看BERT的模型架构。
### 1. BERT的输入
BERT的输入的编码向量（长度是512）是3个嵌入特征的单位，如图11-2所示。

• 词嵌入（Token Embedding）：根据每个字符在“字表”中的位置赋予一个特定的Embedding值。

• 位置嵌入（Position Embedding）：将单词的位置信息编码成特征向量，是向模型中引入单词位置关系至关重要的一环。

• 分割嵌入（Segment Embedding）：用于区分两个句子，例如B是不是A的下文（对话场景、问答场景等）。对于句子对，第一个句子的特征值是0，第二个句子的特征值是1。

![image](https://github.com/user-attachments/assets/274a6fcc-0d5b-4530-b3a5-fe10a740759e)


### 2. BERT的模型架构
与第9章中介绍的编码器的结构相似，BERT实际上是由多个Encoder Block叠加而成的，通过使用注意力模型的多个层次来获得文本的特征提取，如图11-3所示。

![image](https://github.com/user-attachments/assets/48e5998d-2d1e-4612-8d1d-c671e3d6e3e9)


### 11.1.2 BERT预训练任务与微调


在介绍BERT的预训练任务前，首先介绍BERT在使用时的思路，即BERT在训练过程中将自己的训练任务和可替换的微调系统（Fine-Tuning）分离。

1. 开创性的预训练任务方案


Fine-Tuning的目的是根据具体任务的需求替换不同的后端接口，即在已经训练好的语言模型的基础上，加入少量任务专门的属性。例如，对于分类问题，在语言模型的基础上加一层Softmax网络，然后在新的语料上重新训练来进行Fine-Tuning。除了最后一层外，所有的参数都没有变化，如图11-4所示。

BERT在设计时将其作为预训练模型进行训练任务，为了更好地让BERT掌握自然语言的含义并加深对语义的理解，BERT采用了多任务的方式，包括遮蔽语言模型（Masked Language Model，MLM）和下一个句子预测（Next Sentence Prediction，NSP）。

![image](https://github.com/user-attachments/assets/fff61523-0ed3-40bc-9ca7-98238f38ed43)




#### 任务1：MLM

MLM是指在训练的时候随机从输入语料中遮蔽掉一些单词，然后通过上下文预测该单词，该任务非常像读者在中学时期经常做的完形填空。正如传统的语言模型算法和RNN匹配一样，MLM的这个性质和transformer的结构是非常匹配的。在BERT的实验中，15%的Embedding Token会被随机遮蔽掉。在训练模型时，一个句子会被多次“喂”到模型中用于参数学习，但是Google并没有每次都遮蔽这些单词，而是在确定要遮蔽的单词之后按一定比例进行处理：80%直接替换为[Mask]，10%替换为其他任意单词，10%保留原始Token。


• 80%：my dog is hairy -> my dog is [mask]。

• 10%：my dog is hairy -> my dog is apple。

• 10%：my dog is hairy -> my dog is hairy。
这么做的原因是，如果句子中的某个Token 100%被遮蔽，那么在Fine-Tuning的时候模型就会有一些没有见过的单词，如图11-5所示。

![image](https://github.com/user-attachments/assets/f8b53d95-93e0-469f-a4cd-1f502927794c)



加入随机Token的原因是，Transformer要保持对每个输入Token的分布式表征，否则模型就会记住这个[mask]是Token 'hairy'。至于单词带来的负面影响，因为一个单词被随机替换的概率只有15%×10% = 1.5%，所以这个负面影响其实是可以忽略不计的。
#### 任务2：NSP
NSP的任务是判断句子B是不是句子A的下文。如果是的话，就输出'IsNext'，否则输出'NotNext'，训练数据的生成方式是从平行语料中随机抽取连续的两句话，其中50%保留抽取的两句话，符合IsNext关系；剩下的50%随机从语料中提取，它们的关系是NotNext的。这个关系保存在图11-6中的【CLS]符号当中

![image](https://github.com/user-attachments/assets/e280012c-68e4-4b9d-9389-69244f124723)


![image](https://github.com/user-attachments/assets/c2094cc8-7172-4a2d-92ce-f9a426ae75ae)


### 2. BERT用于具体的NLP任务（Fine-Tuning）
在海量单语料上训练完BERT之后，便可以将其应用到NLP的各个任务中了。对于其他任务来说，我们也可以根据BERT的输出信息做出对应的预测。图11-7展示了BERT在11个不同任务中的模型，它们只需要在BERT的基础上再添加一个输出层便可以完成对特定任务的微调。这些任务类似于我们做过的文科试卷，其中有选择题、简答题等。 


