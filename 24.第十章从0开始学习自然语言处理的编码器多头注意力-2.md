### 图10 - 11 Layer Normalization函数与Batch Normalization函数的不同
有兴趣的读者可以展开学习，这里就不再过多阐述了。具体的使用如下（注意一定要显式声明归一化的维度）：
```python
embedding = torch.rand(size=(5, 80, 312))
print(torch.nn.LayerNorm(normalized_shape=[80, 312])(embedding).shape)  # 显式声明归一化的维度
```

#### 10.1.4 多头注意力

10.1.2节的最后实现了使用PyTorch 2.0自定义层编写自注意力模型。从中可以看到，除了使用自注意力核心模型以外，还额外加入了掩码层和点积的除法运算，以及为了整形所使用的Layer Normalization函数。实际上，这些都是为了使得整体模型在训练时更加简易和便捷而做出的优化。

读者应该发现了，前面无论是掩码计算、点积计算还是使用Layer Normalization，都是在一些细枝末节上的修补，有没有可能对注意力模型进行较大的结构调整，使其更加适应模型的训练？

下面在此基础上介绍一种较为大型的ticks，即多头注意力（Multi - Head Attention）架构，该架构在原始的自注意力模型的基础上做出了较大的优化。

多头注意力架构如图10 - 12所示，Query、Key、Value首先经过一个线性变换，之后计算相互之间的注意力值。相对于原始自注意计算方法，注意这里的计算要做h次（h为“头”的数目），其实也就是所谓的多头，每次算一个头，而每次Query、Key、Value进行线性变换的参数W是不一样的。

![image](https://github.com/user-attachments/assets/f93c4599-6632-4418-bfb3-f26e0ff31b47)


### 图10 - 12 多头注意力架构
将h次缩放点积注意力值的结果进行拼接，再进行一次线性变换，得到的值作为多头注意力的结果，如图10 - 13所示。

![image](https://github.com/user-attachments/assets/e06757cf-bd83-4225-b164-c768891d17bc)


### 图10 - 13 多头注意力的结果
可以看到，这样计算得到的多头注意力值的不同之处在于，进行了h次计算，而不只是计算一次。这样做的好处是可以允许模型在不同的表示子空间中学习到相关的信息，并且相对于单独的注意力模型，多头注意力模型的计算复杂度大大降低了。拆分多头模型的代码如下：
```python
def splite_tensor(tensor, h_head):
    embedding = elt.Rearrange("b l (h d) -> b l h d", h = h_head)(tensor)
    embedding = elt.Rearrange("b l h d -> b h l d", h=h_head)(embedding)
    return embedding
```
在此基础上，可以对注意力模型进行修正，新的多头注意力层代码如下：
**【程序10 - 3】**
```python
class Attention(torch.nn.Module):
    def __init__(self, embedding_dim = 312, hidden_dim = 312, n_head = 6):
        super().__init__()
        self.n_head = n_head
        self.query_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.key_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.value_layer = torch.nn.Linear(embedding_dim, hidden_dim)

    def forward(self, embedding, mask):
        input_embedding = embedding

        query = self.query_layer(input_embedding)
        key = self.key_layer(input_embedding)
        value = self.value_layer(input_embedding)

        query_splited = self.splite_tensor(query, self.n_head)
        key_splited = self.splite_tensor(key, self.n_head)
        value_splited = self.splite_tensor(value, self.n_head)

        key_splited = elt.Rearrange("b h l d -> b h d l")(key_splited)
        # 计算query与key之间的权重系数
        attention_prob = torch.matmul(query_splited, key_splited)

        # 使用softmax对权重系数进行归一化计算
        attention_prob += mask * -1e5  # 在自注意力权重的基础上加上掩码值
        attention_prob = torch.softmax(attention_prob, dim=-1)

        # 计算权重系数与value的值，从而获取注意力值
        attention_score = torch.matmul(attention_prob, value_splited)
        attention_score = elt.Rearrange("b h l d -> b l (h d)")(attention_score)
        return (attention_score)

    def splite_tensor(self, tensor, h_head):
        embedding = elt.Rearrange("b l (h d) -> b l h d", h = h_head)(tensor)
        embedding = elt.Rearrange("b l h d -> b h l d", h=h_head)(embedding)
        return embedding

if __name__ == '__main__':
    embedding = torch.rand(size=(5, 16, 312))
    mask = torch.ones((5, 1, 16, 1))  # 注意设计mask的位置，长度是16
    Attention()(embedding, mask)
```

相比较单一的注意力模型，多头注意力模型能够简化计算，并且在更多维的空间对数据进行整合。最新的研究表明，实际上使用“多头”注意力模型，每个“头”所关注的内容并不一致，有的“头”关注相邻之间的序列，而有的“头”会关注更远处的单词。

图10 - 14展示了一个8头注意力模型的架构，具体请读者自行实现。


![image](https://github.com/user-attachments/assets/98df1163-d5f2-4b50-8efe-fb16a9b56d1b)


### 图10 - 14 8头注意力模型的架构

#### 10.2 编码器的实现


本节开始介绍编码器的写法。

前面的章节对编码器的核心部件——注意力模型做了介绍，并且对输入端的词嵌入初始化方法和位置编码做了介绍，正如一开始所介绍的，本节将使用transformer的编码器方案来构建，这是目前最为常用的架构方案。

从图10 - 15中可以看到，一个编码器的构建分成3部分：初始向量层、注意力层和前馈层。 
