#### 10.2 编码器的实现

本节开始介绍编码器的写法。

前面的章节对编码器的核心部件——注意力模型做了介绍，并且对输入端的词嵌入初始化方法和位置编码做了介绍，正如一开始所介绍的，本节将使用transformer的编码器方案来构建，这是目前最为常用的架构方案。

从图10 - 15中可以看到，一个编码器的构建分成3部分：初始向量层、注意力层和前馈层。

![image](https://github.com/user-attachments/assets/9400112c-36ac-4967-b9d1-d40abb04b7d4)


### 图10 - 15 编码器的构建

初始向量层和注意力层在10.1节已经介绍完毕，本节将介绍最后一部分：前馈层。之后将使用这3部分构建本书的编码器架构。



#### 10.2.1 前馈层的实现

从编码器输入的序列经过一个自注意力层后，会传递到前馈神经网络中，这个神经网络被称为“前馈层” 。这个前馈层的作用是进一步整形通过注意力层获取的整体序列向量。


本书的解码器遵循的是transformer架构，因此参考transformer中解码器的构建，如图10 - 16所示。

![image](https://github.com/user-attachments/assets/cabceef8-d255-47ef-92a2-11c689f9e305)




### 图10 - 16 transformer中解码器的构建

所谓前馈神经网络，实际上就是加载了激活函数的全连接层神经网络（或者使用一维卷积实现的神经网络，这点不在这里介绍）。

既然了解了前馈神经网络，其实现也很简单，代码如下。

**【程序10 - 4】**
```python
import torch

class FeedForward(torch.nn.Module):
    def __init__(self,embedding_dim = 312,scale = 4):
        super().__init__()
        self.linear1 = torch.nn.Linear(embedding_dim,embedding_dim*scale)
        self.relu_1 = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(embedding_dim*scale,embedding_dim)
        self.relu_2 = torch.nn.ReLU()
        self.layer_norm = torch.nn.LayerNorm(normalized_shape=embedding_dim)
    def forward(self,tensor):
        embedding = self.linear1(tensor)
        embedding = self.relu_1(embedding)
        embedding = self.linear2(embedding)
        embedding = self.relu_2(embedding)
        embedding = self.layer_norm(embedding)
        return embedding
```
代码很简单，需要提醒读者的是，以上代码使用了两个全连接神经网络来实现前馈神经网络，然而实际上为了减少参数，减轻运行负担，可以使用一维卷积或者“空洞卷积”替代全连接层实现前馈神经网络，具体读者可以自行完成。

#### 10.2.2 编码器的实现
经过前面的分析可以得知，实现一个transformer架构的编码器并不困难，只需要按架构依次将其组合在一起即可。下面按步提供代码，读者可参考注释进行学习。
```python
import torch
import math
import einops.layers.torch as elt

class FeedForward(torch.nn.Module):
    def __init__(self,embedding_dim = 312,scale = 4):
        super().__init__()
        self.linear1 = torch.nn.Linear(embedding_dim,embedding_dim*scale)
        self.relu_1 = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(embedding_dim*scale,embedding_dim)
        self.relu_2 = torch.nn.ReLU()
        self.layer_norm = torch.nn.LayerNorm(normalized_shape=embedding_dim)
    def forward(self,tensor):
        embedding = self.linear1(tensor)
        embedding = self.relu_1(embedding)
        embedding = self.linear2(embedding)
        embedding = self.relu_2(embedding)
        embedding = self.layer_norm(embedding)
        return embedding

class Attention(torch.nn.Module):
    def __init__(self,embedding_dim = 312,hidden_dim = 312,n_head = 6):
        super().__init__()
        self.n_head = n_head
        self.query_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.key_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.value_layer = torch.nn.Linear(embedding_dim, hidden_dim)

    def forward(self,embedding,mask):
        input_embedding = embedding

        query = self.query_layer(input_embedding)
        key = self.key_layer(input_embedding)
        value = self.value_layer(input_embedding)

        query_splited = self.splite_tensor(query, self.n_head)
        key_splited = self.splite_tensor(key, self.n_head)
        value_splited = self.splite_tensor(value, self.n_head)

        key_splited = elt.Rearrange("b h l d -> b h d l")(key_splited)
        # 计算query与key之间的权重系数
        attention_prob = torch.matmul(query_splited, key_splited)

        # 使用softmax对权重系数进行归一化计算
        attention_prob += mask * -1e5  # 在自注意力权重的基础上加上掩码值
        attention_prob = torch.softmax(attention_prob, dim=-1)

        # 计算权重系数与value的值，从而获取注意力值
        attention_score = torch.matmul(attention_prob, value_splited)
        attention_score = elt.Rearrange("b h l d -> b l (h d)")(attention_score)

        return (attention_score)

    def splite_tensor(self,tensor,h_head):
        embedding = elt.Rearrange("b l (h d) -> b l h d", h = h_head)(tensor)
        embedding = elt.Rearrange("b l h d -> b h l d", h=h_head)(embedding)
        return embedding

class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model = 312, dropout = 0.05, max_len=80):
        """
        :param d_model: pe编码维度，一般与Word Embedding相同，方便相加
        :param dropout: drop out
        :param max_len: 语料库中最长句子的长度，即Word Embedding中的L
        """
        super(PositionalEncoding, self).__init__()
        # 定义drop out
        self.dropout = torch.nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model)  # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0, max_len).unsqueeze(1)  # 建个arange表示词的位置，以便使用公式计算，size=(max_len,1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *-(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加，意为batch维度下的操作
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x):
        # 输入的最终编码 = word_embedding + positional_embedding
        x = x + self.pe[:, :x.size(1)].clone().detach().requires_grad_(False)
        return self.dropout(x)  # size = [batch, L, d_model]

class Encoder(torch.nn.Module):
    def __init__(self, vocab_size = 1024,max_length = 80,embedding_size = 312,n_head = 6,scale = 4,n_layer = 3):
        super().__init__()
        self.n_layer = n_layer
        self.embedding_table = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_size)
        self.position_embedding = PositionalEncoding(max_len=max_length)
        self.attention = Attention(embedding_size,embedding_size,n_head)
        self.feedward = FeedForward()
    def forward(self,token_inputs):
        token = token_inputs
        mask = self.create_mask(token)

        embedding = self.embedding_table(token)
        embedding = self.position_embedding(embedding)
        for _ in range(self.n_layer):
            embedding = self.attention(embedding,mask)
            embedding = torch.nn.Dropout(0.1)(embedding)
            embedding = self.feedward(embedding)

        return embedding

    def create_mask(self,seq):
        mask = torch.not_equal(seq, 0).float()
        mask = torch.unsqueeze(mask, dim=-1)
        mask = torch.unsqueeze(mask, dim=1)
        return mask

if __name__ == '__main__':
    seq = torch.ones(size=(3,80),dtype=int)
    Encoder()(seq)
```
可以看到，真正实现一个编码器，从理论和架构上来说并不困难，只需要读者细心即可。

#### 10.3 实战编码器 （文档此处未完整展示相关内容 ） 
