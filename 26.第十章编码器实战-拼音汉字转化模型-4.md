### 10.3 实战编码器：拼音汉字转化模型
本节将结合前面两节的内容实战编码器，即使用编码器完成一个训练——拼音与汉字的转化，类似图10 - 17的效果。

### 图10 - 17 拼音和汉字

![image](https://github.com/user-attachments/assets/b20acfcd-c404-4cc6-863b-8fbbc6f15054)


#### 10.3.1 汉字拼音数据集处理

首先是数据集的准备和处理，在本例中准备了15万条汉字和拼音对应的数据。

1. **数据集展示**

汉字拼音数据集如下：

```js
A11_0 lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 ao4 si4 yue4 de lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran4 绿 是 阳春 烟 景 大 块 文章 的 底色 四月 的 林 密 更 是 绿 得 鲜 活 秀 媚 诗 意 盎 然
A11_1 tai1 jin3 ping2 yao1 hua4 de li4 liang4 zai4 yong3 dao4 shang4 xia4 fan1 teng2 yong3 dong4 she2 xing2 zhuang4 ru2 hai3 tun2 yi1 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1 他 仅 凭 腰 部 的 力 量 在 泳 道 上 下 翻 腾 蛹 动 蛇 行 状 如 海 豚 一 直 以 一 头 的 优 势 领 先
A11_10 pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3 ya2 shui1 de tuo1 qu4 yi1 fu2 guang1 bang3 zi zhong1 jind4 le shui3 cuan4 dong4 炮 眼 打 好 了 炸 药 怎 么 装 岳 正 才 咬 了 咬 牙 倏地 脱 去 衣 服 光 膀 子 冲 进 了 水 窜 洞
A11_100 ke3 shei2 zhi1 wen2 wan2 hou4 tai1 yi1 zhao4 jing4 zi zhi1 jian4 zuo3 xia4 yan3 jian3 de xian4 you4 cu4 you4 hei1 yu3 you4 ce4 ming2 xian3 bu4 dui4 cheng2 可 谁 知 纹 完 后 她 一 照 镜 子 只 见 左 下 眼 睑 的 线 又 粗 又 黑 与 右 侧 明 显 不 对 称
```
简单介绍一下。数据集中的数据被分成3部分，每部分使用特定的空格键隔开：
- 第一部分A11_i为序号，表示序列的条数和行号。
- 第二部分是拼音编号，这里使用的是汉语拼音，与真实的拼音标注不同的是，去除了拼音的原始标注，而使用数字1、2、3、4替代，分别代表当前读音的第一声到第四声，这点请读者注意。 
- 最后一部分是汉字序列，这里与第二部分的拼音部分一一对应。

![image](https://github.com/user-attachments/assets/3cad6a70-e00e-4180-b550-590cca41c993)



2. **获取字库和训练数据**

获取数据集中字库的个数是一个非常重要的问题，一个非常好的办法是使用set格式的数据读取全部字库中的不同字符。

创建字库和训练数据的完整代码如下：

```python
max_length = 64
with open("zh.tsv", errors="ignore", encoding="UTF-8") as f:
    context = f.readlines()  # 读取内容
    for line in context:
        line = line.strip().split(" ")  # 切分每行中的不同部分
        pinyin = ["GO"] + line[1].split(" ") + ["END"]  # 处理拼音部分，在头尾加上起止符号
        hanzi = ["GO"] + line[2].split(" ") + ["END"]  # 处理汉字部分，在头尾加上起止符号
        for _pinyin, _hanzi in zip(pinyin, hanzi):  # 创建字库
            pinyin_vocab.add(_pinyin)
            hanzi_vocab.add(_hanzi)
        pinyin = pinyin + ["PAD"] * (max_length - len(pinyin))
        hanzi = hanzi + ["PAD"] * (max_length - len(hanzi))
        pinyin_list.append(pinyin)  # 创建拼音列表
        hanzi_list.append(hanzi)  # 创建汉字列表
```

这里说明一下，首先context读取了全部数据集中的内容，之后根据空格将其分成3部分。对于拼音和汉字部分，将其转化成一个序列，并在前后分别加上起止符GO和END。这实际上可以不用加，为了明确地描述起止关系，从而加上了起止标注。

实际上还需要加上一个特定符号PAD，这是为了对单行序列进行补全操作，最终的数据如下：

['GO', 'lin2', 'yong3', '......', 'gan1', 'END', 'PAD', 'PAD', '......']
['GO', '柳', '水', '......', '感', 'END', 'PAD', 'PAD', '......']

pinyin_list和hanzi_list是两个列表，分别用来存放对应的拼音和汉字训练数据。最后不要忘记在字库中加上PAD符号。
```python
pinyin_vocab = ["PAD"] + list(sorted(pinyin_vocab))
hanzi_vocab = ["PAD"] + list(sorted(hanzi_vocab))
```

3. **根据字库生成Token数据**

获取的拼音标注和汉字标注的训练数据并不能直接用于模型训练，模型需要转化成token的一系列数字列表，代码如下：
```python
def get_dataset():
    pinyin_tokens_ids = []  # 新的拼音token列表
    hanzi_tokens_ids = []  # 新的汉字token列表
    for pinyin, hanzi in zip(tqdm(pinyin_list), hanzi_list):
        # 获取新的拼音token
        pinyin_tokens_ids.append([pinyin_vocab.index(char) for char in pinyin])
        # 获取新的汉字token
        hanzi_tokens_ids.append([hanzi_vocab.index(char) for char in hanzi])
    return pinyin_vocab, hanzi_vocab, pinyin_tokens_ids, hanzi_tokens_ids
```
代码中创建了两个新的列表，分别对拼音和汉字的token进行存储，获取的是根据字库序号编号后新的序列token。

#### 10.3.2 汉字拼音转化模型的确定

下面进行模型的编写。

实际上，单纯使用在10.2节提供的模型也是可以的，但是一般来说需要对其进行修正。因此，单纯使用一层编码器对数据进行编码，在效果上可能并没有多层编码器的准确率高，一个简单方法是增加更多层的编码器对数据进行编码，如图10 - 18所示。

![image](https://github.com/user-attachments/assets/a8a56332-4094-46df-848f-d7e5c7aee46f)


### 图10 - 18 使用多层编码器进行编码
代码如下。
**【程序10 - 5】**
```python
import torch
import math
import einops.layers.torch as elt

class FeedForward(torch.nn.Module):
    def __init__(self,embedding_dim = 312,scale = 4):
        super().__init__()
        self.linear1 = torch.nn.Linear(embedding_dim,embedding_dim*scale)
        self.relu_1 = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(embedding_dim*scale,embedding_dim)
        self.relu_2 = torch.nn.ReLU()
        self.layer_norm = torch.nn.LayerNorm(normalized_shape=embedding_dim)
    def forward(self,tensor):
        embedding = self.linear1(tensor)
        embedding = self.relu_1(embedding)
        embedding = self.linear2(embedding)
        embedding = self.relu_2(embedding)
        embedding = self.layer_norm(embedding)
        return embedding

class Attention(torch.nn.Module):
    def __init__(self,embedding_dim = 312,hidden_dim = 312,n_head = 6):
        super().__init__()
        self.n_head = n_head
        self.query_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.key_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.value_layer = torch.nn.Linear(embedding_dim, hidden_dim)

    def forward(self,embedding,mask):
        input_embedding = embedding

        query = self.query_layer(input_embedding)
        key = self.key_layer(input_embedding)
        value = self.value_layer(input_embedding)

        query_splited = self.splite_tensor(query, self.n_head)
        key_splited = self.splite_tensor(key, self.n_head)
        value_splited = self.splite_tensor(value, self.n_head)

        key_splited = elt.Rearrange("b h l d -> b h d l")(key_splited)
        # 计算query与key之间的权重系数
        attention_prob = torch.matmul(query_splited, key_splited)

        # 使用softmax对权重系数进行归一化计算
        attention_prob += mask * -1e5  # 在自注意力权重的基础上加上掩码值
        attention_prob = torch.softmax(attention_prob, dim=-1)

        # 计算权重系数与value的值，从而获取注意力值
        attention_score = torch.matmul(attention_prob, value_splited)
        attention_score = elt.Rearrange("b h l d -> b l (h d)")(attention_score)

        return (attention_score)

    def splite_tensor(self,tensor,h_head):
        embedding = elt.Rearrange("b l (h d) -> b l h d", h = h_head)(tensor)
        embedding = elt.Rearrange("b l h d -> b h l d", h=h_head)(embedding)
        return embedding

class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model = 312, dropout = 0.05, max_len=80):
        """
        :param d_model: pe编码维度，一般与Word Embedding相同，方便相加
        :param dropout: drop out
        :param max_len: 语料库中最长句子的长度，即Word Embedding中的L
        """
        super(PositionalEncoding, self).__init__()
        # 定义drop out
        self.dropout = torch.nn.Dropout(p=dropout)

        # 计算pe编码
        pe = torch.zeros(max_len, d_model)  # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0, max_len).unsqueeze(1)  # 建个arange表示词的位置，以便使用公式计算，size=(max_len,1)
        div_term = torch.exp(torch.arange(0, d_model, 2) *-(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加，意为batch维度下的操作
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x):
        # 输入的最终编码 = word_embedding + positional_embedding
        x = x + self.pe[:, :x.size(1)].clone().detach().requires_grad_(False)
        return self.dropout(x)  # size = [batch, L, d_model]

class Encoder(torch.nn.Module):
    def __init__(self, vocab_size = 1024,max_length = 80,embedding_size = 312,n_head = 6,scale = 4,n_layer = 3):
        super().__init__()
        self.n_layer = n_layer
        self.embedding_table = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_size)
        self.position_embedding = PositionalEncoding(max_len=max_length)
        self.attention = Attention(embedding_size,embedding_size,n_head)
        self.feedward = FeedForward()
    def forward(self,token_inputs):
        token = token_inputs
        mask = self.create_mask(token)

        embedding = self.embedding_table(token)
        embedding = self.position_embedding(embedding)
        for _ in range(self.n_layer):
            embedding = self.attention(embedding,mask)
            embedding = torch.nn.Dropout(0.1)(embedding)
            embedding = self.feedward(embedding)

        return embedding

    def create_mask(self,seq):
        mask = torch.not_equal(seq, 0).float()
        mask = torch.unsqueeze(mask, dim=-1)
        mask = torch.unsqueeze(mask, dim=1)
        return mask
```
这里相对于10.2.2节中的编码器构建示例，使用了多头自注意力层和前馈层，需要注意的是，这里只是在编码器层中加入了更多层的多头注意力层和前馈层，而不是直接加载了更多的编码器。

#### 10.3.3 模型训练部分的编写

剩下的是对模型的训练部分的编写。在这里采用简单的模型训练的方式完成代码的编写。

第一步：导入数据集和创建数据的生成函数。

对于数据的获取，由于模型在训练过程中不可能一次性将所有的数据导入，因此需要创建一个生成器，将获取的数据按批次发送给训练模型，在这里我们使用一个for循环来完成这个数据输入任务。

**【程序10 - 6】**

```python
pinyin_vocab,hanzi_vocab,pinyin_tokens_ids,hanzi_tokens_ids = get_data.get_dataset()
batch_size = 32
train_length = len(pinyin_tokens_ids)
for epoch in range(21):
    train_num = train_length // batch_size
    train_loss, train_correct = [], []
    for i in tqdm(range((train_num))):
        ...
```

这段代码是数据的生成工作，按既定的batch_size大小生成数据batch，之后在epoch的循环中对数据输入进行迭代。

下面是训练模型的完整实战，代码如下。

**【程序10 - 7】**

```python
import numpy as np
import torch
import attention_model
import get_data
max_length = 64
from tqdm import tqdm
char_vocab_size = 4462
pinyin_vocab_size = 1154

def get_model(embedding_dim = 312):
    model = torch.nn.Sequential(
        attention_model.Encoder(pinyin_vocab_size,max_length=max_length),
        torch.nn.Dropout(0.1),
        torch.nn.Linear(embedding_dim,char_vocab_size)
    )
    return model

device = "cuda"
model = get_model().to(device)
model = torch.compile(model)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)
loss_func = torch.nn.CrossEntropyLoss()

pinyin_vocab,hanzi_vocab,pinyin_tokens_ids,hanzi_tokens_ids = get_data.get_dataset()

batch_size = 32
train_length = len(pinyin_tokens_ids)
for epoch in range(21):
    train_num = train_length // batch_size
    train_loss, train_correct = [], []
    for i in tqdm(range((train_num))):
        model.zero_grad()
        start = i * batch_size
        end = (i + 1) * batch_size

        batch_input_ids = torch.tensor(pinyin_tokens_ids[start:end]).int().to(device)
        batch_labels = torch.tensor(hanzi_tokens_ids[start:end]).to(device)

        pred = model(batch_input_ids)

        batch_labels = batch_labels.to(torch.uint8)
        active_loss = batch_labels.gt(0).view(-1) == 1
        loss = loss_func(pred.view(-1, char_vocab_size)[active_loss],
                         batch_labels.view(-1)[active_loss])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (epoch +1) %10 == 0:
            state = {"net":model.state_dict(), "optimizer":optimizer.state_dict(),
                     "epoch":epoch}
            torch.save(state, "./saver/modelpara.pt")
```
通过将训练代码部分和模型组合在一起，即可完成模型的训练。而最后预测部分，即使用模型进行自定义实战拼音和汉字的转化，请读者自行完成。

#### 10.4 本章小结

首先，需要向读者说明的是，本章的模型设计并没有完全遵守transformer中编码器的设计，而是仅建立了多头注意力层和前馈层，这是与真实的transformer中解码器不一致的地方。

其次，在数据的设计上，本章直接将不同字符或者拼音作为独立的字符进行存储，这样做的好处在于可以使数据的最终生成更简单，但是增加了字符个数，增大了搜索空间，因此对训练要求更高。还有一种划分方法，即将拼音拆开，使用字母和
