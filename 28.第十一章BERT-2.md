
### 11.2 实战BERT：中文文本分类
前面介绍了BERT的结构与应用，本节将实战BERT的文本分类。

#### 11.2.1 使用Hugging Face获取BERT预训练模型

BERT是一个预训练模型，其基本架构和存档都有相应的服务公司提供下载服务，而Hugging Face是一家目前专门免费提供自然语言处理预训练模型的公司。

Hugging Face是一家总部位于纽约的聊天机器人初创服务商，开发的应用在青少年中颇受欢迎，相比于其他公司，Hugging Face更加注重产品带来的情感以及环境因素。在GitHub上开源的自然语言处理、预训练模型库Transformers提供了NLP领域大量优秀的预训练语言模型和调用框架。



**步骤01 安装依赖**

安装Hugging Face依赖的方法很简单，命令如下：

```js
pip install transformers
```

安装完成后，即可使用Hugging Face提供的预训练模型BERT。

**步骤02 使用Hugging Face提供的代码格式进行BERT的引入与使用**

代码如下：

```js
from transformers import BertTokenizer
from transformers import BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
pretrain_model = BertModel.from_pretrained("bert-base-chinese")
```

从网上下载该模型的过程如图11-8所示，模型下载完毕后即可使用。

![image](https://github.com/user-attachments/assets/335de5f9-5c8f-4b5d-a931-c01d75d44b3e)



下面的代码演示使用BERT编码器获取对应文本的Token。

**【程序11-1】**
```js
from transformers import BertTokenizer
from transformers import BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
pretrain_model = BertModel.from_pretrained("bert-base-chinese")
tokens = tokenizer.encode("春眠不觉晓",max_length=12,padding="max_length",truncation=True)
print(tokens)
print("---------------------")
print(tokenizer("春眠不觉晓",max_length=12,padding="max_length",truncation=True))
print("---------------------")

tokens = torch.tensor([tokens]).int()
print(pretrain_model(tokens))
```
![image](https://github.com/user-attachments/assets/a794f367-5043-4e23-bfa0-0a96d2afa206)


如果想打印使用BERT计算的对应文本的Embedding值，就使用如下代码。
**【程序11-2】**

```js
import torch
from transformers import BertTokenizer
from transformers import BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
pretrain_model = BertModel.from_pretrained("bert-base-chinese")

tokens = tokenizer.encode("春眠不觉晓",max_length=12,padding="max_length",truncation=True)
print(tokens)
print("---------------------")
print(tokenizer("春眠不觉晓",max_length=12,padding="max_length",truncation=True))
print("---------------------")

tokens = torch.tensor([tokens]).int()
print(pretrain_model(tokens))
```

打印结果如图11-9所示。最终获得一个维度为[1,12,768]大小的矩阵，用以表示输入的文本。

![image](https://github.com/user-attachments/assets/23d6a155-aa77-4112-98c3-0a3afb6bddc7)


#### 11.2.2 BERT实战文本分类
我们在第9章带领读者完成了基于循环神经网络的情感分类实战，但是当时的结果可能并不能令人满意，本小节通过预训练模型查看预测结果。

**步骤01 数据的准备**

这里使用与第1章相同的酒店评论数据集（见图1.1）。

**步骤02 数据的处理**

使用BERT自带的tokenizer函数将文本转换成需要的Token。完整代码如下：

```js
import numpy as np
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

max_length = 80  #设置获取的文本长度为80
labels = []  #用以存放label
context = []  #用以存放汉语文本
token_list = []

with open("../dataset/cn/CnSentiCorp.txt", mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split()

        # labels.append(int(line[0]))
        if int(line[0]) == 0:
            labels.append(0)  #由于在后面直接采用PyTorch自带的crossentroy函数，因此这里直接输入0，否则输入1
        else:
            labels.append(1)
        text = " ".join(line[1:])
        token = tokenizer.encode(text,max_length=max_length,padding="max_length",truncation=True)

        token_list.append(token)
        context.append(text)

seed = 828
np.random.seed(seed); np.random.shuffle(token_list)
np.random.seed(seed); np.random.shuffle(labels)

dev_list = np.array(token_list[:170]).astype(int)
dev_labels = np.array(labels[:170]).astype(int)

token_list = np.array(token_list[170:]).astype(int)
labels = np.array(labels[170:]).astype(int)
```

在这里首先通过BERT自带的tokenize对输入的文本进行编码处理，之后将其拆分成训练集与验证集。

**步骤03 模型的设计**

与第1章的不同之处在于，这里使用BERT作为文本的特征提取器，后面只使用了一个二分类层作为分类函数，需要说明的是，由于BERT的输入不同，这里将其拆分成两种模型，分别是simple版与标准版。Simple预训练模型代码如下：

```js
import torch
import torch.utils.data as Data
from transformers import BertModel
from transformers import BertTokenizer
from transformers import AdamW

# 定义下游任务模型
class ModelSimple(torch.nn.Module):
    def __init__(self, pretrain_model_name = "bert-base-chinese"):
        super().__init__()
        self.pretrain_model = BertModel.from_pretrained(pretrain_model_name)
        self.fc = torch.nn.Linear(768, 2)

    def forward(self, input_ids):
        with torch.no_grad():  # 上游的模型不进行梯度更新
            output = self.pretrain_model(input_ids=input_ids)  # input_ids: 编码之后的数字(Token) 
        output = self.fc(output[0][:, 0])  # 取出每个batch的第一列作为CLS，即(16, 768)
        output = output.softmax(dim=1)  # 通过softmax函数，使其在1维上进行缩放，使元素位于[0,1]范围内，总和为1
        return output
```

标准版预训练模型代码如下：

```js
class Model(torch.nn.Module):
    def __init__(self, pretrain_model_name = "bert-base-chinese"):
        super().__init__()
        self.pretrain_model = BertModel.from_pretrained(pretrain_model_name)
        self.fc = torch.nn.Linear(768, 2)

    def forward(self, input_ids,attention_mask,token_type_ids):
        with torch.no_grad():  # 上游的模型不进行梯度更新
            # input_ids: 编码之后的数字(Token)
            # attention_mask: 其中padding的位置是0，其他位置是1
            # token_type_ids: 第一个句子和特殊符号的位置是0，第二个句子的位置是1
            output = self.pretrain_model(input_ids=input_ids,
                                         attention_mask=attention_mask, token_type_ids=token_type_ids)
            # 取出每个batch的第一列作为CLS，即(16, 768)
            output = self.fc(output[0][:, 0])
            # 通过softmax函数，使其在1维上进行缩放，使元素位于[0,1]范围内，总和为1
            output = output.softmax(dim=1)
            return output
```
标准版和simple版的区别主要在于输入格式不同，对于不同的输入格式，有兴趣的读者可以在学完本章内容后自行尝试。


**步骤04 模型的训练**

完整代码如下。

**【程序11-3】**
```js
import torch
import model

device = "cuda"
model = model.ModelSimple().to(device)
model = torch.compile(model)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
loss_func = torch.nn.CrossEntropyLoss()

import get_data
token_list = get_data.token_list
labels = get_data.labels

dev_list = get_data.dev_list
dev_labels = get_data.dev_labels

batch_size = 128
train_length = len(labels)
for epoch in range(21):
    train_num = train_length // batch_size
    train_loss, train_correct = 0, 0
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size

        batch_input_ids = torch.tensor(token_list[start:end]).to(device)
        batch_labels = torch.tensor(labels[start:end]).to(device)
        pred = model(batch_input_ids)
        loss = loss_func(pred, batch_labels.type(torch.uint8))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_correct += ((torch.argmax(pred, dim=1) == 
(batch_labels).type(torch.float)).sum().item() / len(batch_labels))

    train_loss /= train_num
    train_correct /= train_num
    print("train_loss:", train_loss, "train_correct:", train_correct)

    test_pred = model(torch.tensor(dev_list).to(device))
    correct = ((torch.argmax(test_pred, dim=1) == 
(torch.tensor(dev_labels).to(device)).type(torch.float)).sum().item() / len(test_pred))
    print("test_acc:", correct)
```
上面的代码较为简单，这里就不再过多阐述了。需要注意的是，使用BERT增大了显存的消耗，这里batch_size被设置成128，对于不同的显存，其结果有着不同的输入大小。最终结果如图11-10所示。

这里展示了10个epoch中后面6个epoch的过程，最终准确率达到了0.9176。另外，由于这里设置的训练时间与学习率的关系，该结果并不是最优的结果，读者可以自行尝试完成。

### 11.3 更多的预训练模型
Hugging Face除了提供BERT预训练模型下载之外，还提供了更多的预训练模型下载，打开Hugging Face主页。

单击主页顶端的Models菜单之后，出现预训练模型的选择界面。

左侧依次是Tasks、Libraries、DataSets、Languages、Licenses、Other选项卡，单击Libraries选项卡，在其下选择我们使用的PyTorch与zh标签，即使用PyTorch构建的中文数据集，右边会呈现对应的模型。

图中右侧为Hugging Face提供的基于PyTorch框架的中文预训练模型，刚才我们所使用的BERT模型也在其中。我们可以选择另一个模型进行模型训练，比如基于“全词遮蔽”的GPT-2模型进行训练。

这里首先复制Hugging Face所提供的预训练模型全名：

```js
model_name = "uer/gpt2-chinese-ancient"
```

注意，需要保留“/”和前后的名称。替换不同的预训练模型只需要替换说明字符，代码如下：
```js
from transformers import BertTokenizer,GPT2Model
model_name = "uer/gpt2-chinese-ancient"
tokenizer = BertTokenizer.from_pretrained(model_name)
pretrain_model = GPT2Model.from_pretrained(model_name)

tokens = tokenizer.encode("春眠不觉晓",max_length=12,padding="max_length",truncation=True)
print(tokens)
print("---------------------")
print(tokenizer("春眠不觉晓",max_length=12,padding="max_length",truncation=True))
print("---------------------")

tokens = torch.tensor([tokens]).int()
print(pretrain_model(tokens))
```
剩下的内容与11.2节中的方法一致，有兴趣的读者可以自行完成验证。
最终结果与普通的BERT预训练模型相比可能会有出入，原因是多种多样的，这不在本书的评判范围，有兴趣的读者可以自行研究更多模型的使用方法。

### 11.4 本章小结
本章介绍了预训练模型的使用，以经典的预训练模型BERT为例演示了文本分类的方法。
除此之外，对于使用的预训练模型来说，使用每个序列中的第一个Token可以较好地表示完整序列的功能，这在某些任务中有较好的作用。
Hugging Face网站提供了很多预训练模型下载，本章也介绍了多种使用预训练模型的方法，有兴趣的读者自行学习和比较训练结果。 


### ④ 模型的训练
完整代码如下。

**【程序11-3】**
```python
import torch
import model

device = "cuda"
model = model.ModelSimple().to(device)
model = torch.compile(model)
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)
loss_func = torch.nn.CrossEntropyLoss()

import get_data
token_list = get_data.token_list
labels = get_data.labels

dev_list = get_data.dev_list
dev_labels = get_data.dev_labels

batch_size = 128
train_length = len(labels)
for epoch in range(21):
    train_num = train_length // batch_size
    train_loss, train_correct = 0, 0
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size

        batch_input_ids = torch.tensor(token_list[start:end]).to(device)
        batch_labels = torch.tensor(labels[start:end]).to(device)
        pred = model(batch_input_ids)
        loss = loss_func(pred, batch_labels.type(torch.uint8))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        train_correct += ((torch.argmax(pred, dim=1) == 
(batch_labels).type(torch.float)).sum().item() / len(batch_labels))

    train_loss /= train_num
    train_correct /= train_num
    print("train_loss:", train_loss, "train_correct:", train_correct)

    test_pred = model(torch.tensor(dev_list).to(device))
    correct = ((torch.argmax(test_pred, dim=1) == 
(torch.tensor(dev_labels).to(device)).type(torch.float)).sum().item() / len(test_pred))
    print("test_acc:", correct)


 print("---------------")
``` 

上面的代码较为简单，这里就不再过多阐述了。需要注意的是，使用BERT增大了显存的消耗，这里batch_size被设置成128，对于不同的显存，其结果有着不同的输入大小。最终结果如图11-10所示。

|train_loss|train_correct|test_acc|
| ---- | ---- | ---- |
|0.463061871872817|0.857786018491526|0.9176470588235294|
|0.459601732905445|0.859777542372882|0.9176470588235294|
|0.4580921763086615|0.860691525423728|0.9176470588235294|
|0.45672218009915|0.862502118646468|0.9176470588235294|
|0.4555700853156375|0.863621288135592|0.9176470588235294|
|0.4544448391370847|0.865333686464678|0.9176470588235294|

![image](https://github.com/user-attachments/assets/80fe8dde-2789-44bf-94c4-b8db5bb53133)


**图11-10 10个epoch的过程**
这里展示了10个epoch中后面6个epoch的过程，最终准确率达到了0.9176。另外，由于这里设置的训练时间与学习率的关系，该结果并不是最优的结果，读者可以自行尝试完成。

### 11.3 更多的预训练模型
Hugging Face除了提供BERT预训练模型下载之外，还提供了更多的预训练模型下载，打开Hugging Face主页，如图11-11所示。

![image](https://github.com/user-attachments/assets/44c3812c-baa4-4f69-bb9e-6b9d3111fc95)


**图11-11 Hugging Face主页**

The AI community building the future.

单击主页顶端的Models菜单之后，出现预训练模型的选择界面，如图11-12所示。

![image](https://github.com/user-attachments/assets/e85eb5c6-b8f0-4e4a-b8ac-1108fb6c18cf)


**图11-12 预训练模型的选择界面**

左侧依次是Tasks、Libraries、DataSets、Languages、Licenses、Other选项卡，单击Libraries选项卡，在其下选择我们使用的PyTorch与zh标签，即使用PyTorch构建的中文数据集，右边会呈现对应的模型，如图11-13所示。

![image](https://github.com/user-attachments/assets/1b518ab4-488a-47e9-927c-6c8e7ea0b051)


**图11-13 选择我们需要的模型**

图11-13右侧为Hugging Face提供的基于PyTorch框架的中文预训练模型，刚才我们所使用的BERT模型也在其中。我们可以选择另一个模型进行模型训练，比如基于“全词遮蔽”的GPT-2模型进行训练，如图11-14所示。

![image](https://github.com/user-attachments/assets/cf99df8f-b8d0-4bf0-a9ee-83769e77f3f8)


**图11-14 选择中文的PyTorch的BERT模型**

这里首先复制Hugging Face所提供的预训练模型全名：
```python
model_name = "uer/gpt2-chinese-ancient"
```
注意，需要保留“/”和前后的名称。替换不同的预训练模型只需要替换说明字符，代码如下：
```python
from transformers import BertTokenizer, GPT2Model
model_name = "uer/gpt2-chinese-ancient"
tokenizer = BertTokenizer.from_pretrained(model_name)
pretrain_model = GPT2Model.from_pretrained(model_name)

tokens = tokenizer.encode("春眠不觉晓", max_length=12, padding="max_length", truncation=True)
print(tokens)
print("---------------------")
print(tokenizer("春眠不觉晓", max_length=12, padding="max_length", truncation=True))
print("---------------------")

tokens = torch.tensor([tokens]).int()
print(pretrain_model(tokens))
```
剩下的内容与11.2节中的方法一致，有兴趣的读者可以自行完成验证。

最终结果与普通的BERT预训练模型相比可能会有出入，原因是多种多样的，这不在本书的评判范围，有兴趣的读者可以自行研究更多模型的使用方法。 

### 11.4 本章小结
本
章介绍了预训练模型的使用，以经典的预训练模型BERT为例演示了文本分类的方法。

除此之外，对于使用的预训练模型来说，使用每个序列中的第一个Token可以较好地表示完整序列的功能，这在某些任务中有较好的作用。

Hugging Face网站提供了很多预训练模型下载，本章也介绍了多种使用预训练模型的方法，有兴趣的读者自行学习和比较训练结果。 
