### 第6章 可视化的PyTorch数据处理与模型展示

前面带领读者完成了基于PyTorch 2.0模型与训练方面的学习，相信读者已经可以较好地完成一定基础的深度学习应用项目。读者也可能感觉到在前期的学习中，更多的是对PyTorch 2.0模型本身的了解，而对其他部分介绍较少。特别是数据处理部分，一直使用NumPy计算包对数据进行处理，因此缺乏一个贴合PyTorch自身的数据处理器。 

针对这个问题，PyTorch在2.0版本中为我们提供了专门的数据下载和数据处理包，集中在torch.utils.data这个工具包中，使用该包中的数据处理工具可以极大地提高开发效率及质量，帮助提高使用者在数据预处理，数据加载模块的边界与效率，如图6-1所示。

![image](https://github.com/user-attachments/assets/53006886-5042-4885-8d70-a1050cbd1bbb)


#### 图6-1 torch.utils.data包中提供的数据处理工具箱

可以看到，图6-1展示的是基于PyTorch 2.0的数据处理工具箱总体框架，主要由以下3部分构成。

- **Dataset**：一个抽象类，其他数据需要继承这个类，并且覆写其中的两个方法__getitem__和__len__。

- **DataLoader**：定义一个新的迭代器，实现批量读取、打乱数据以及提供并行加速等功能。 

- **Sample**：提供多种采样方法的函数。

下面我们将基于PyTorch 2.0的工具箱依次对其进行讲解。

### 6.1 用于自定义数据集的torch.utils.data工具箱使用详解
本章开头提到torch.utils.data工具箱中提供了3个类用于对数据进行处理和采样，但是Dataset在输出时每次只能输出一个样本，而DataLoader可以弥补这一缺陷，实现批量乱序输出样本，如图6-2所示。

![image](https://github.com/user-attachments/assets/293ae048-c03c-4b68-865d-9ed2492e3af5)



#### 图6-2 DataLoader

#### 6.1.1 使用torch.utils.data.Dataset封装自定义数据集

本小节从自定义数据集开始介绍。在PyTorch 2.0中，数据集的自定义使用需要继承torch.utils.data.Dataset类，之后实现其中的__getitem__、__len__方法。基本的Dataset类架构如下：
```python
class Dataset():
    def __init__(self, transform=None):  #注意transform参数会在6.1.2节进行介绍
        super(Dataset, self).__init__()

    def __getitem__(self, index):
        pass

    def __len__(self):
        pass
```

可以很清楚地看到，Dataset除了基本的init函数外，还需要填充两个额外的函数：__getitem__与__len__。这是仿照Python中数据list的写法对其进行定义，其使用方法如下：

```python
data = Customer(Dataset)[index]  #打印出index序号对应的数据
length = len(Customer(Dataset))  #打印出数据集总行数
```

下面以前面章节中一直使用的MNIST数据集为例进行介绍。

1. **init的初始化方法**

在对数据进行输出之前，首先将数据加载到Dataset这个类中，加载的方法直接按数据读取的方案使用NumPy进行载入。当然，读者也可以使用任何对数据读取的技术获取数据本身。在这里，所使用的数据读取代码如下：

```python
def __init__(self, transform=None):  #注意transform参数会在6.1.2节进行介绍
    super(MNIST_Dataset, self).__init__()
    # 载入数据
    self.x_train = np.load("../dataset/mnist/x_train.npy")
    self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")
```

2. **__getitem__与__len__方法**

首先是对数据的获取方式，__getitem__是Dataset父类中内置的数据迭代输出的方法。在这里，我们只需要显式地提供此方法的实现即可，代码如下：

```python
def __getitem__(self, item):
    image = (self.x_train[item])
    label = (self.y_train_label[item])
    return image,label
```

而__len__方法用于获取数据的长度，在这里直接返回标签的长度即可，代码如下：

```python
def __len__(self):
    return len(self.y_train_label)
```
完整的自定义MNIST_Dataset数据输出代码如下：
```python
class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self):
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")

    def __getitem__(self, item):
        image = self.x_train[item]
        label = self.y_train_label[item]
        return image,label

    def __len__(self):
        return len(self.y_train_label)
```
读者可以将上面代码中定义的MNIST_Dataset类作为模板尝试更多的自定义数据集。

#### 6.1.2 改变数据类型的Dataset类中的transform的使用

我们获取的输入数据对于PyTorch 2.0来说并不能够直接使用，因此最少需要一种转换的方法，将初始化载入的数据转化成我们所需要的样式。

1. **将自定义载入的参数转化为PyTorch 2.0专用的tensor类**

这一步的编写方法很简单，我们只需要额外提供对于输入输出类的处理方法即可，代码如下：

```python
class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        return torch.tensor(inputs), torch.tensor(targets)
```

这里我们所提供的ToTensor类的作用是对输入的数据进行调整。需要注意的是，这个类的输入输出数据结构和类型需要与自定义Dataset类中的def __getitem__()方法的数据结构和类型相一致。

2. **新的自定义的Dataset类**

对于原本的自定义数据Dataset类的定义，需要对其做出修正，新的数据读取类的定义如下：

```python
class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self,transform = None):  #在定义时需要定义transform的参数
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")

        self.transform = transform  #需要显式地提供transform类

    def __getitem__(self, index):
        image = (self.x_train[index])
        label = (self.y_train_label[index])

        #通过判定transform类的存在对其进行调用
        if self.transform:
            image,label = self.transform(image,label)
        return image,label

    def __len__(self):
        return len(self.y_train_label)
```

在这里读者需要显式地在MNIST_Dataset类中提供transform的定义、具体使用位置和操作。因此，在这里特别注意，我们自己定义的transform类需要与getitem函数的输出结构相一致。

完整的带有transform的自定义MNIST_Dataset类使用如下：

```python
import numpy as np
import torch

class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        return torch.tensor(inputs), torch.tensor(targets)

class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self,transform = None):  #在定义时需要定义transform的参数
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")

        self.transform = transform  #需要显式地提供transform类

    def __getitem__(self, index):
        image = (self.x_train[index])
        label = (self.y_train_label[index])

        #通过判定transform类的存在对其进行调用
        if self.transform:
            image,label = self.transform(image,label)
        return image,label

    def __len__(self):
        return len(self.y_train_label)

mnist_dataset = MNIST_Dataset()
image,label = (mnist_dataset[1024])
print(type(image), type(label))
print("---------------------------")
mnist_dataset = MNIST_Dataset(transform=ToTensor())
image,label = (mnist_dataset[1024])
print(type(image), type(label))
```
在这里我们做了尝试，对同一个MNIST_Dataset类做了无传入和有transform传入的比较，最终结果如图6-3所示。



#### 图6-3 无传入和有transform传入的比较
```
<class 'numpy.ndarray'> <class 'numpy.uint8'>
---------------------------
<class 'torch.Tensor'> <class 'torch.Tensor'>
```
可以清楚地看到，对于传入后的数据，由于transform的存在，其数据结构有了很大的变化。

3. **修正数据输出的维度**

在transform类中，我们还可以进行更为复杂的操作，例如对维度进行转换，代码如下：

```python
class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        inputs = np.reshape(inputs,[28*28])
        return torch.tensor(inputs), torch.tensor(targets)
```
可以看到，我们根据输入大小的维度进行折叠操作，从而为后续的模型输出提供合适的数据维度格式。此时，读者可以使用如下方法打印出新的输出数据维度，代码如下：
```python
mnist_dataset = MNIST_Dataset(transform=ToTensor())
image,label = (mnist_dataset[1024])
print(type(image), type(label))
print(image.shape)
```

4. **依旧无法使用自定义的数据对模型进行训练**

当读者学到此部分时，一定信心满满地想将刚学习到的内容应用到我们的深度学习训练中。但是遗憾的是，到目前为止，使用自定义数据集的模型还无法运行，这是由于PyTorch 2.0在效能方面以及损失函数的计算方式上对此进行了限制。读者可以先运行程序并参考本小节结尾的提示，尝试解决这个问题，我们在6.1.3节也提供了一种PyTorch 2.0官方建议的解决方案。

```python
#注意下面这段代码无法正常使用，仅供演示
import numpy as np
import torch

#device = "cpu"  #PyTorch的特性，需要指定计算的硬件，如果没有GPU的存在，就使用CPU进行计算
device = "cuda"  #在这里默认使用GPU，如果出现运行问题，可以将其改成CPU模式

class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        inputs = np.reshape(inputs,[1,-1])
        targets = np.reshape(targets, [1, -1])
        return torch.tensor(inputs), torch.tensor(targets)

#注意下面这段代码无法正常使用，仅供演示
class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self,transform = None):  #在定义时需要定义transform的参数
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")

        self.transform = transform  #需要显式地提供transform类

    def __getitem__(self, index):
        image = (self.x_train[index])
        label = (self.y_train_label[index])

        #通过判定transform类的存在对其进行调用
        if self.transform:
            image,label = self.transform(image,label)
        return image,label

    def __len__(self):
        return len(self.y_train_label)

#注意下面这段代码无法正常使用，仅供演示
mnist_dataset = MNIST_Dataset(transform=ToTensor())

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0' #指定GPU编码
import torch
import numpy as np

batch_size = 320  #设定每次训练的批次数
epochs = 1024  #设定训练次数

#设定的多层感知机网络模型
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(28*28, 312),
            torch.nn.ReLU(),
            torch.nn.Linear(312, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )

    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
model = model.to(device)  #将计算模型传入GPU硬件等待计算
torch.save(model, './model.pth')
model = torch.compile(model)  #PyTorch 2.0的特性，加速计算速度
loss_fu = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)  #设定优化函数

#注意下面这段代码无法正常使用，仅供演示
#开始计算
for epoch in range(20):
    train_loss = 0
    for sample in (mnist_dataset):
        image = sample[0];label = sample[1]
        train_image = image.to(device)
        train_label = label.to(device)

        pred = model(train_image)
        loss = loss_fu(pred,train_label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()  # 记录每个批次的损失值

    # 计算并打印损失值
    train_loss /= len(mnist_dataset)
    print("epoch:",epoch,"train_loss:", round(train_loss,2))
```
这段代码看起来没有问题，但是实际上在运行时会报错，这是由于数据在输出时是逐个输出的，模型在逐个数据计算损失函数时无法对其进行计算；同时，这样的计算方法会极大地限制PyTorch 2.0的计算性能。因此在此并不建议采用此方法直接对模型进行计算。 

















