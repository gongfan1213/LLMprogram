### 5.2 实战：基于卷积的MNIST手写体分类
前面我们实现了基于多层感知机的MNIST手写体，本章将实现以卷积神经网络完成的MNIST手写体识别。

#### 5.2.1 数据的准备
在本例中，我们依旧使用MNIST数据集，对这个数据集的数据和标签介绍在前面的章节中有较好的说明，相对于前面章节

直接对数据进行“折叠”处理，这里需要显式地标注出数据的通道，代码如下：
```python
import numpy as np
import einops.layers.torch as elt

#载入数据
x_train = np.load("../dataset/mnist/x_train.npy")
y_train_label = np.load("../dataset/mnist/y_train_label.npy")

x_train = np.expand_dims(x_train,axis=1)  #在指定维度上进行扩充
print(x_train.shape)
```

这里是对数据的修正，`np.expand_dims`的作用是在指定维度上进行扩充，在这里我们在第二维（也就是PyTorch的通道

维度）进行扩充，结果如下：

(60000, 1, 28, 28)

#### 5.2.2 模型的设计

本小节使用PyTorch 2.0框架对模型进行设计。在本例中，我们将使用卷积层对数据进行处理，完整的模型如下：
```python
import torch
import torch.nn as nn
import numpy as np
import einops.layers.torch as elt

class MnistNetword(nn.Module):
    def __init__(self):
        super(MnistNetword, self).__init__()
        #前置的特征提取模块
        self.convs_stack = nn.Sequential(
            nn.Conv2d(1,12,kernel_size=7),  #第一个卷积层
            nn.ReLU(),
            nn.Conv2d(12,24,kernel_size=5), #第二个卷积层
            nn.ReLU(),
            nn.Conv2d(24,6,kernel_size=3)  #第三个卷积层
        )
        #最终分类器层
        self.logits_layer = nn.Linear(in_features=1536,out_features=10)

    def forward(self,inputs):
        image = inputs
        x = self.convs_stack(image)

        #elt.Rearrange的作用是对输入数据维度进行调整，读者可以使用torch.nn.Flatten函数完成此工作
        x = elt.Rearrange("b c h w -> b (c h w)")(x)
        logits = self.logits_layer(x)
        return logits

model = MnistNetword()
torch.save(model,"model.pth")
```

在这里，我们首先设定了3个卷积层作为前置的特征提取层，最后一个全连接层作为分类器层。在这里需要注意的是，对

于分类器的全连接层，输入维度需要手动计算，当然读者可以一步一步尝试打印特征提取层的结果，使用`shape`函数打

印维度后计算。

最后对模型进行保存，这里可以调用前面章节中介绍的Netro软件对维度进行展示，结果如图5 - 12所示。

![image](https://github.com/user-attachments/assets/522fb821-68e2-43d5-8aeb-6b84c89dd32e)


在这里可以可视化地看到整体模型的结构与显示，这里对每个维度都进行了展示，感兴趣的读者可以自行查阅。

#### 5.2.3 基于卷积的MNIST分类模型

下面进入示例部分，也就是MNIST手写体的分类。完整的训练代码如下：

```python
import torch
import torch.nn as nn
import numpy as np
import einops.layers.torch as elt

#载入数据
x_train = np.load("../dataset/mnist/x_train.npy")
y_train_label = np.load("../dataset/mnist/y_train_label.npy")

x_train = np.expand_dims(x_train,axis=1)
print(x_train.shape)

class MnistNetword(nn.Module):
    def __init__(self):
        super(MnistNetword, self).__init__()
        self.convs_stack = nn.Sequential(
            nn.Conv2d(1,12,kernel_size=7),
            nn.ReLU(),
            nn.Conv2d(12,24,kernel_size=5),
            nn.ReLU(),
            nn.Conv2d(24,6,kernel_size=3)
        )
        self.logits_layer = nn.Linear(in_features=1536,out_features=10)

    def forward(self,inputs):
        image = inputs
        x = self.convs_stack(image)
        x = elt.Rearrange("b c h w -> b (c h w)")(x)
        logits = self.logits_layer(x)
        return logits

device = "cuda" if torch.cuda.is_available() else "cpu"
#注意需要将model发送到GPU计算
model = MnistNetword().to(device)
model = torch.compile(model)
loss_fn = nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)

batch_size = 128
for epoch in range(42):
    train_num = len(x_train)//128
    train_loss = 0.
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size

        x_batch = torch.tensor(x_train[start:end]).to(device)
        y_batch = torch.tensor(y_train_label[start:end]).to(device)

        pred = model(x_batch)
        loss = loss_fn(pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()  # 记录每个批次的损失值

    # 计算并打印损失值
    train_loss /= train_num
    accuracy = (pred.argmax(1) == y_batch).type(torch.float32).sum().item() / batch_size
    print("epoch:",epoch,"train_loss:",
          round(train_loss,2),"accuracy:",round(accuracy,2))
```
在这里，我们使用了本章新定义的卷积神经网络模块进行局部特征抽取，而对于其他的损失函数和优化函数，使用了与前期一样的模式进行模型训练。最终结果如图5 - 13所示。

![image](https://github.com/user-attachments/assets/0ba6bf15-91a2-44ca-ae5f-3114b798061a)


请读者自行尝试学习。

### 5.3 PyTorch的深度可分离膨胀卷积详解

在本章开始就说明了，相对于多层感知机来说，卷积神经网络能够对输入特征局部进行计算，同时能够节省大量的待训练

参数。基于此，本节将介绍更为深入的内容，即本章的进阶部分——深度可分离膨胀卷积。

需要说明的是，本例中深度可分离膨胀卷积可以按功能分为“深度”“可分离”“膨胀”“卷积”。

在讲解下面的内容之前，首先回顾PyTorch 2.0中的卷积定义类：
```python
class Conv2d(_ConvNd):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: _size_2_t,
        stride: _size_2_t = 1,
        padding: Union[str, _size_2_t] = 0,
        dilation: _size_2_t = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = 'zeros',  # TODO: refine this type
        device=None,
        dtype=None
    ) -> None:
        ...
```

前面讲解了卷积类中常用的输入输出维度（`in_channels`，`out_channels`）的定义，卷积核（`kernel_size`）以及步长（`stride`）大小的设置，而对于其他部分的参数定义却没有详细说明，本节将通过对深度可分离膨胀卷积的讲解更为细致地说明卷积类的定义与使用。

#### 5.3.1 深度可分离卷积的定义

在普通的卷积中，可以将其分为两个步骤来计算：

（1）跨通道计算。

（2）平面内计算。

这是由于卷积的局部跨通道计算的性质所形成的，一个非常简单的思想是，能否使用另一种 
