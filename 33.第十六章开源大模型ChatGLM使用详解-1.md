### 第16章 开源大模型ChatGLM使用详解

到目前为止，本书介绍的预训练语言模型，最大规模的是GPT-2。GPT-2在当时的人工智能生
成领域可以说是翘首，但是随着人们对人工智能研究的深入，以及计算机硬件水平的提高，人们尝
试使用更大、更强、更快的人工智能模型来生成任务，这也是现代科技发展所带来的必然结果。
本章将介绍目前市场上常用的深度学习模型——清华大学的ChatGLM，及其使用与自定义
的方法。

#### 16.1 为什么要使用大模型

随着OpenAI吹响了超大模型的使用号角，大模型技术发展迅速，每周甚至每天都有新的模型
在开源，并且大模型的精调训练成本大大降低。下面将目前大模型的一些分类和说明组织在一个完
整的框架中，如图16-1所示。

![image](https://github.com/user-attachments/assets/41be2365-84a8-4a86-8025-c06232f92246)



**图16-1 大模型的分类和说明**

LLM（大语言模型）分为主流大模型（如GLM-130B、PaLM、BLOOM等 ）、分布式训练（如张量并行、3D并行等 ）、微调（如FLAN、LoRA等 ）、应用（工具如Toolformer、AOT等 ）。

#### 16.1.1 大模型与普通模型的区别
顾名思义，大模型指网络规模巨大的深度学习模型，具体表现为多模型的参数量规模较大，
其规模通常在千亿级别。随着模型规模的提高，人们逐渐接受模型的参数越大，其性能越好，但是

大模型与普通深度学习模型有什么区别呢？

简单地解释，可以把普通模型比喻为一个小盒子，它的容量是有限的，只能存储和处理有限

数量的数据和信息。就像人类的大脑，只有有限的容量和处理能力，能完成的思考和决策有限。

表16-1列出了目前可以公开使用的大模型版本和参数量。

**表16-1 公开使用的大模型版本和参数量**

|模型|作者|参数量/Billion|类型|是否开源|
| ---- | ---- | ---- | ---- | ---- |
|LLaMa|Meta AI|65|Decoder|是|
|OPT|Meta AI|175|Decoder|是|
|T5|Google|11|Encoder-Decoder|是|
|mT5|Google|13|Encoder-Decoder|是|
|UL2|Google|20|Decoder|否|
|PaLM|Google|540|Decoder|否|
|LaMDA|Google|137|Encoder-Decoder|是|
|FLAN-T5|Google|11|Encoder-Decoder|是|
|FLAN-UL2|Google|20|Decoder|否|
|FLAN-PaLM|Google|540|Decoder|否|
|FLAN|Google|137|Decoder|是|
|BLOOM|BigScience|176|Decoder|是|
|GPT-Neo|EleutherAI|2.7|Decoder|是|
|GPT-NeoX|EleutherAI|20|Decoder|是|
|GPT-3|OpenAI|175|Decoder|否|
|InstructGPT|OpenAI|1.3|Decoder|否|


相比之下，大模型就像一个超级大的仓库，它能够存储和处理大量的数据和信息。它不仅可
以完成普通模型能够完成的任务，还能够处理更加复杂和庞大的数据集。这些大模型通常由数十亿
甚至上百亿个参数组成，需要大量的计算资源和存储空间才能运行。这类似于人类大脑（约有1000
亿个神经元细胞），在庞大的运算单元支撑下完成非常复杂和高级的思考和决策。

因此，大模型之所以被称为大模型，是因为其规模和能力相比于普通模型是巨大的。大模型
能够完成更加复杂和高级的任务，例如自然语言理解、语音识别、图像识别等，这些任务需要大量
的数据和计算资源才能完成。大模型可以被看作人工智能发展的一次飞跃，它的出现为我们提供了
更加强大的工具和技术来解决现实中一些复杂和具有挑战性的问题。

与普通模型相比，大模型具有更加复杂和庞大的网络结构、更多的参数和更深的层数，能够
处理和学习更加复杂和高级的模式和规律。这种架构差异类似于计算机和超级计算机之间的差异，
它们的性能和能力相差甚远。

#### 16.1.2 一个神奇的现象——大模型的涌现能力
本小节讨论一个神奇的现象——大模型的涌现能力。大模型的“大”体现在参数和存储空间
上，就是纸面上的数字表现特别巨大，而随之带来的是一个大模型特有的现象——涌现能力
（Emergent Ability），即：通过在大规模数据上进行训练，大型深度神经网络可以学习到更加复杂
和抽象的特征表示，这些特征表示可以在各种任务中产生出乎意料的上乘表现，具体可参考图16-2。

![image](https://github.com/user-attachments/assets/59d8f3d1-1433-4c2f-a854-31659e13ff2c)


**图16-2 大模型在不同任务中产生“涌现”现象的参数量比较**

展示了不同模型（LaMDA、GPT3等 ）在文本生成、文本摘要、文本分类任务中，随着模型参数增加，准确率的变化情况。可以看到，随着模型参数的增加，对于准确率的比较模型“突然”有了突飞猛进的增加。这里先简单解释一下，可以将其认为是从量变到质变的转化，即模型的规模增加时，精度的增速大于0的现象（通常增速曲线到后期，增速一般小于0，就像抛物线逐渐接近高点），于是可以看到模型规模与准确率的曲线上整体呈现非线性增长。

结果的展现形式就是精度准确率的“涌现”出现，使得模型能够表现出更高的抽象能力和泛

化能力。这种涌现能力的出现可以通过以下几个方面来解释。

- **更复杂的神经网络结构**：随着模型规模的增加，神经元之间的连接也会变得更加复杂。这使得模型能够更好地捕捉输入数据的高层次特征，从而提高模型的表现能力。

- **更多的参数**：大型模型通常具有更多的参数，这意味着它们可以对输入数据进行更复杂的非线性变换，从而更好地适应不同的任务。例如，在自然语言处理领域，大型语言模型可以通过对海量文本数据的训练，学习到更加抽象的语言特征，从而可以生成更加流畅、自然的文本。 

- **更强的数据驱动能力**：大型模型通常需要大量的数据来进行训练，这使得它们能够从数据中学习到更加普遍的特征和规律。这种数据驱动的能力可以帮助模型在面对新的任务时表现得更加出色。 


这里对涌现现象的讨论并不深入，有兴趣继续深入研究的读者，可以自行查找相关材料学习。

#### 16.2 ChatGLM使用详解

本节介绍生成模型GLM系列模型的新成员——中英双语对话模型ChatGLM。

ChatGLM分为6B和130B（默认使用ChatGLM-6B）两种，主要区别在于其模型参数不同。ChatGLM是一个开源的、支持中英双语问答的对话语言模型，并针对中文进行了优化。该模型基
于GLM（General Language Model）架构，如图16-3所示。

**图16-3 ChatGLM架构**

![image](https://github.com/user-attachments/assets/aa33ba28-0294-4012-b502-84ca7c00921e)


GLM是自回归空白填充的通用语言模型预训练，现有预训练架构分为自编码模型（如BERT，处理NLU任务、自然语言理解任务 ）、自回归模型（如GPT，适用于无条件生成任务 ）、自回归自编码模型（如T5，适用于有条件生成任务 ）、General Language Model（GLM）。GLM训练思路是从输入文本中随机空白出连续跨度的Token，并按照自回归预训练的思路，训练模型预测这些被Mask掉的Token。模型架构基于单一的Transformer，并将所有任务都融合成LM+Mask任务。

结合模型量化技术，使用ChatGLM-6B用户可以在消费级的显卡上进行本地部署（INT4量化级别下最低只需6GB显存 ）。表16-2展示了ChatGLM的硬件资源消耗。

**表16-2 ChatGLM的硬件资源消耗**

|量化等级|最低GPU显存（推理）|最低GPU显存（高效参数微调）|
| ---- | ---- | ---- |
|Half（半精度，无量化）|13 GB|14 GB|
|8 GB|8 GB|9 GB|
|INT8（8Bit量化）|6 GB|7 GB|
|INT4（4Bit量化）| - | - |

接下来将以ChatGLM-6B为基础进行讲解，在讲解过程中，如果没有特意注明，默认使用ChatGLM-6B。更大的模型GLM-130B在使用上与ChatGLM-6B类似，只是在参数量、训练层数以及落地的训练任务方面有所区别，有条件的读者可以自行尝试。

##### 16.2.1 ChatGLM简介及应用前景
ChatGLM基于GLM架构，针对中文问答和对话进行了优化。经过约1TB标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿个参数的ChatGLM-6B虽然规模不及千亿模型的ChatGLM-130B，但大大降低了推理成本，提升了效率，并且已经能生成相当符合人类偏好的回答。具体来说，ChatGLM-6B具备以下特点。
- **充分的中英双语预训练**：ChatGLM-6B在1:1比例的中英语料上训练了1TB的Token量，兼具双语能力。

- **优化的模型架构和大小**：吸取GLM-130B训练经验，修正了二维RoPE位置编码实现，使用传统FFN结构。6B（62亿）的参数大小，使得研究者和个人开发者自己微调和部署ChatGLM-6B成为可能。 

- **较低的部署门槛**：在FP16半精度下，ChatGLM-6B至少需要13GB的显存进行推理，结合模型量化技术，这一需求可以进一步降低到10GB(INT8)和6GB(INT4)，使得ChatGLM-6B可以部署在消费级显卡上。 

- **更长的序列长度**：相比GLM-10B（序列长度为1024），ChatGLM-6B的序列长度达2048，支持更长的对话和应用。 

- **人类意图对齐训练**：使用了监督微调（Supervised Fine-Tuning）、反馈自助（Feedback Bootstrap）、人工强化学习反馈（RLHF）等方式，使模型初具理解人类指令意图的能力。输出格式为Markdown，方便展示。 


因此，ChatGLM-6B在一定条件下具备较好的对话与问答能力。

在应用前景上，相对于宣传较多的ChatGPT，其实ChatGLM都适用。表面来看，ChatGPT无所不能，风光无限。但是对于绝大多数企业用户来说，和自身盈利方向有关的垂直领域才是最重要的。

在垂直领域，ChatGLM经过专项训练，可以做得非常好，甚至有网友想出了用收集ChatGPT不熟悉领域的内容，再由ChatGLM加载使用的策略。

比如智能客服，没几个人会在打客服电话的时候咨询相对论，而大型的ChatGPT的博学在单一领域就失去了绝对优势，如果把企业所在行业的问题训练好，那么就会是一个很好的人工智能应用。

比如将ChatGLM在语音方面的应用依托于大模型就很有想象力，有公司已经能很好地进行中外语言的文本转换了，和大模型结合后，很快就能生成专业的外文文档。

比如在人工智能投顾方面造诣颇深，接入大模型后进行私有语料库的训练，可以把自然语言轻松地转换成金融市场的底层数据库所能理解的复杂公式，小学文化水平理解这些复杂的炒股指标不再是梦想。

再比如工业机器人领域，初看起来和ChatGPT、ChatGLM没什么关联，但是机器人的操作本质上是代码驱动的，如果利用人工智能让机器直接理解自然语言，那么中间的调试过程将大大减少，工业机器人的迭代速度很可能呈指数级上升。


##### 16.2.2 下载ChatGLM

正如我们在本书开始的时候演示的，ChatGLM可以很轻松地部署在本地的硬件上，当时采用的是THUDM/chatglm-6b-int4。（使用的时候，需要安装一些特定的Python包，按提示安装即可。）

为了后续的学习和再训练，我们直接使用完整的ChatGLM存档结构，代码如下：
```python
from transformers import AutoTokenizer, AutoModel
names = ["THUDM/chatglm-6b-int4", "THUDM/chatglm-6b"]
tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
model = AutoModel.from_pretrained("THUDM/chatglm-6b",
trust_remote_code=True).half().cuda()
response, history = model.chat(tokenizer, "你好", history=[])
print(response)
print("-----------------------")
response, history = model.chat(tokenizer, "晚上睡不着该怎么办", history=history)
print(response)
```

从打印结果来看，此时的展示结果与chatglm-6b-int4没有太大差别。
合并，如图16-4所示。

**图16-4 下载过程展示**

显示了下载文件分部分下载的进度条，如Downloading (..)-00001-of-00008.bin: 57% 等。

需要注意的是，对于下载的存档文件还需要进行合并处理，展示如图16-5所示。

**图16-5 对下载的存档文件进行合并处理**

显示Downloading shards: 100% 等合并进度信息。

最终展示的结果如图16-6所示。

**图16-6 最终展示的结果**

![image](https://github.com/user-attachments/assets/17f9fc4c-c0b1-4c24-b48b-36217844bdc2)

![image](https://github.com/user-attachments/assets/a6b3d530-6cb7-4e3d-86d2-401ad35ae608)


ChatGLM-6B回复：你好~！我是人工智能助手ChatGLM-6B，很高兴见到你，欢迎问我任何问题。对于晚上睡不着，给出了创建舒适睡眠环境、建立固定睡眠时间表、放松自己等建议。


请读者自行打印验证这部分内容。需要注意的是，即使问题是一样的，但是回答也有可能不同，因为我们所使用的ChatGLM是生成式模型，前面的生成直接影响了后面的生成，而这一点也是生成模型不好的地方，前面的结果有了波动，后面就会发生很大的变化，会产生滚雪球效应。

##### 16.2.3 ChatGLM的使用与Prompt介绍

前面简单向读者介绍了ChatGLM的使用，除此之外，ChatGLM还有很多可以胜任的地方，例如进行文本内容的抽取，读者可以尝试如下任务：

```python
content="""ChatGLM-6B是一个开源的、支持中英双语的对话语言模型，基于General Language Model（GLM）架构，具有62亿参数。
手机号18888888888
ChatGLM-6B使用了较ChatGPT更为高级的技术，针对中文问答和对话进行了优化。
邮箱123456789@qq.com
经过约1TB标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答，更多信息请参考我们的博客。
账号:root 密码:xiaohua123
"""
prompt='从上文中，提取“信息”(keyword,content)，包括:"手机号"、"邮箱"、"账号"、"密码"等类型的实体，输出json格式内容'
input = '{}nn{}'.format(content,prompt)
print(input)
response, history = model.chat(tokenizer, input, history=[])
print(response)
```
这是一个经典的文本抽取任务，希望通过ChatGLM抽取其中的内容，在这里我们使用了一个Prompt（中文暂时称为“提示”），Prompt是研究者为了下游任务设计出来的一种输入形式或模板，它能够帮助ChatGLM“回忆”起自己在预训练时“学习”到的东西。
Prompt也可以帮助使用者更好地“提示”预训练模型所需要做的任务，在这里我们通过Prompt的方式向ChatGLM传达一个下游任务目标，即需要其对文本进行信息抽取，抽取其中蕴含的手机、邮箱、账号、密码等常用信息。最终显示结果如图16-7所示。

![image](https://github.com/user-attachments/assets/d57c63b9-9cd1-4c2d-b715-d016ec0719f3)


**图16-7 对文本进行信息抽取**
结果以JSON格式呈现：

```json
{
"keyword": "信息",
"content": {
"手机号": "18888888888",
"邮箱": "123456789@qq.com",
"账号": "root",
"密码": "xiaohua123"
}
}
```

可以看到，这是一个使用JSON格式表示的抽取结果，其中的内容根据Prompt中的定义提供了相应的键 - 值对，直接抽取了对应的信息。

除此之外，读者还可以使用ChatGLM进行一些常识性的文本问答和编写一些代码。当然，完成这些内容还需要读者设定好特定的Prompt，从而使得ChatGLM能够更好地理解读者所提出的问题和意思。

#### 16.3 本章小结
本章讲解了深度学习自然语言处理的一个重要的研究方向——自然语言处理的大模型ChatGLM，这是目前为止深度学习在自然语言处理中最前沿和最重要的方向之一。本章只做了抛砖引玉的工作，介绍了大模型的基本概念、分支并实现了一个基于ChatGLM的应用。从第17章开始将以此为基础完成ChatGLM的再训练和微调工作。 
