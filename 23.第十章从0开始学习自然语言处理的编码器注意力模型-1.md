### 第10章 从零开始学习自然语言处理的编码器

好吧，我们又要从0开始了。

前面的章节带领读者掌握了使用多种方式对字符进行表示的方法。例如原始的One - Hot方法，现在较为常用的Word2Vec和FastText方法等。这些都是将字符进行向量化处理的方法。

问题来了，无论是使用旧方法还是现在常用的方法，或者是将来出现的新方法，有没有一个统一的称谓？答案是有的，所有的这些处理方法都可以被简称为Encoder（编码器），如图10 - 1所示。

![image](https://github.com/user-attachments/assets/f39927cb-e97a-4ecc-9f6b-41f24ff4772c)



编码器的作用是构造一种能够存储字符（词）的若干个特征的表达方式（虽然这个特征具体是什么我们也不知道，但这样做就行了），这个就是前文讲的Embedding形式。

本章将从一个简单的编码器开始，介绍其核心架构、整体框架及其实现，并以此为基础引入编程实战，即一个对汉字和拼音转换的翻译。

但是编码器并不是简单地使用，其更重要的内容是在此基础上引入transform架构的基础概念，这是目前最为流行和通用的编码器架构，并在此基础上衍生出了更多的内容，这在第11章会详细介绍。本章着重讲解通用解码器，读者可以将其当成独立的内容来学习。


#### 10.1 编码器的核心——注意力模型

编码器的作用是对输入的字符序列进行编码处理，从而获得特定的词向量结果。为了简便起见，作者直接使用transformer的编码器方案，这也是目前最为常用的编码器架构方案。编码器的结构如图10 - 2所示。

![image](https://github.com/user-attachments/assets/d9fe6cb8-0186-48c2-9a74-fae222d57cac)



从图10 - 2可见，编码器由以下多个模块构成：
- 初始词向量（Input Embedding）层。
- 位置编码器（Positional Encoding）层。
- 多头自注意力（Multi - Head Attention）层。
- 前馈（Feed Forward）层。

实际上，编码器的构成模块并不是固定的，也没有特定的形式，transformer的编码器架构是目前最为常用的，因此接下来将以此为例进行介绍。首先介绍编码器的核心内容：注意力模型和架构，然后以此为基础完成整个编码器的介绍和编写。

#### 10.1.1 输入层——初始词向量层和位置编码器层
初始词向量层和位置编码器层是数据输入最初的层，作用是将输入的序列通过计算组合成向量矩阵，如图10 - 3所示。

![image](https://github.com/user-attachments/assets/2b5f9c06-ee18-4f99-966f-bad4442241e8)


下面对每一部分依次进行讲解。

1. **初始词向量层**

如同大多数的向量构建方法一样，首先将每个输入单词通过词嵌入算法转换为词向量。其中每个词向量被设定为固定的维度，本书后面将所有词向量的维度设置为312。具体代码如下：

```python
import torch
word_embedding_table = torch.nn.Embedding(num_embeddings=encoder_vocab_size, embedding_dim=312)
encoder_embedding = word_embedding_table(inputs)
```

这里对代码进行解释，首先使用torch.nn.Embedding函数创建了一个随机初始化的向量矩阵，encoder_vocab_size是字库的个数，一般在编码器中字库是包含所有可能出现的“字”的集合。而embedding_dim定义的Embedding向量维度，这里使用通用的312即可。

词向量初始化在PyTorch中只发生在最底层的编码器中。额外讲一下，所有的编码器都有一个相同的特点，即它们接收一个向量列表，列表中的每个向量大小为312维。在底层（最开始）编码器中，它就是词向量，但是在其他编码器中，它就是下一层编码器的输出（也是一个向量列表）。

2. **位置编码**

位置编码是一个既重要又有创新性的结构输入。一般自然语言处理使用的都是连续的长度序列，因此为了使用输入的顺序信息，需要将序列对应的相对位置和绝对位置信息注入模型中。

基于此目的，一个朴素的想法就是将位置编码设计成与词嵌入同样大小的向量维度，之后将其直接相加使用，从而使得模型能够既获取到词嵌入信息，也能获取到位置信息。

具体来说，位置向量的获取方式有以下两种：

- 通过模型训练所得。

- 根据特定的公式计算所得（用不同频率的sin和cos函数直接计算）。

因此，在实际操作中，模型插入位置编码可以设计一个可以随模型训练的层，也可以使用一个计算好的矩阵直接插入序列的位置函数，公式如下：

![image](https://github.com/user-attachments/assets/49254257-f0c6-485c-a421-a3995bb1a325)


\[
\begin{align*}
PE_{(pos,2i)}&=\sin(pos/10000^{2i/d_{model}})\\
PE_{(pos,2i + 1)}&=\cos(pos/10000^{2i/d_{model}})
\end{align*}
\]

序列中任意一个位置都可以用三角函数表示，pos是输入序列的最大长度，i是序列中依次的各个位置，$d_{model}$是设定的与词向量相同的位置312。代码如下：
```python
class PositionalEncoding(torch.nn.Module):
    def __init__(self, d_model = 312, dropout = 0.05, max_len=80):
        """
        :param d_model: pe编码维度，一般与Word Embedding相同，方便相加
        :param dropout: drop out
        :param max_len: 语料库中最长句子的长度，即Word Embedding中的L
        """
        super(PositionalEncoding, self).__init__()
        # 定义drop out
        self.dropout = torch.nn.Dropout(p=dropout)

        # 计算pe编码
        pe=torch.zeros(max_len, d_model)  # 建立空表，每行代表一个词的位置，每列代表一个编码位
        position = torch.arange(0, max_len).unsqueeze(1)  # 建个arange表示词的位置，以便使用公式计算，size=(max_len,1)
        div_term=torch.exp(torch.arange(0, d_model, 2) *-(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)  # 计算偶数维度的pe值
        pe[:, 1::2] = torch.cos(position * div_term)  # 计算奇数维度的pe值
        pe = pe.unsqueeze(0)  # size=(1, L, d_model)，为了后续与word_embedding相加，意为batch维度下的操作
        self.register_buffer('pe', pe)  # pe值是不参加训练的

    def forward(self, x):
        # 输入的最终编码 = word_embedding + positional_embedding
        x = x + self.pe[:, :x.size(1)].clone().detach().requires_grad_(False)
        return self.dropout(x)  # size = [batch, L, d_model]
```
这种位置编码函数的写法过于复杂，读者直接使用即可。最终将词向量矩阵和位置编码组合如图10 - 4所示。

![image](https://github.com/user-attachments/assets/37f68d6e-d13b-468a-8b0c-8597943c6120)


#### 10.1.2 自注意力层

自注意力层不但是本章的重点，而且是本书的重要内容（然而实际上非常简单）。

注意力层是使用注意力机构建的，是能够脱离距离的限制建立非相互关系的一种计算机机制。注意力机制最早是在视觉图像领域提出的，来自于2014年“谷歌大脑”团队的论文Recurrent Models of Visual Attention，其在RNN模型上使用了注意力机制来进行图像分类。

随后，Bahdanau等在论文Neural Machine Translation by Jointly Learning to Align and Translate中，使用类似注意力的机制在机器翻译任务上将翻译和对齐同时进行，这实际上是第一次将注意力机制应用到NLP领域中。

接下来，注意力机制被广泛应用于基于RNN/CNN等神经网络模型的各种NLP任务中。2017年，Google机器翻译团队发表的Attention is all you need中大量使用了自注意力（Self - Attention）机制来学习文本表示。自注意力机制也成为大家近期的研究热点，并在各种自然语言处理任务上进行探索。

自然语言中的自注意力机制通常指的是不使用其他额外的信息，只使用自我注意力的形式关注本身，进而从句子中抽取相关信息。自注意力又称作内部注意力，它在很多任务上都有十分出色的表现，比如阅读理解、文本继承、自动文本摘要等。

下面将介绍一个简单的自注意机制。

本章内容非常重要，建议读者第一次先通读一遍本章内容，再结合实战代码部分重新阅读2遍以上。

1. **自注意力中的Query、Key和Value**

自注意力机制是进行自我关注从而抽取相关信息的机制。从具体实现来看，注意力函数的本质可以被描述为一个查询（Query）到一系列键 - 值（key - value）对的映射，它们被作为一种抽象的向量，主要用于计算和辅助自注意力，如图10 - 5所示。

![image](https://github.com/user-attachments/assets/0f5b65b6-f344-4a0b-bfd9-dfb9b5fd4ac1)



如图10 - 5所示，一个单词Thinking经过向量初始化后，经过3个不同的全连接层重新计算后获取特定维度的值，即看到的$q_1$，而$q_2$的来历也是如此。单词Machines经过Embedding向量初始化后，经过与上一个单词相同的全连接层计算，之后依次将$q_1$和$q_2$连接起来，组成一个新的连接后的二维矩阵$W^Q$，被定义成Query。
```python
W^Q=concat([q_1,q_2],axis = 0)
```
而由于是自注意力机制，因此Key和Value和Query的值相同，如图10 - 6所示。

![image](https://github.com/user-attachments/assets/b82d7469-29e5-4c1a-be52-540812b79bef)



2. **使用Query、Key和Value计算自注意力的值**

下面使用Query、Key和Value计算自注意力的值，其过程如下：

（1）将Query和每个Key进行相似度计算得到权重，常用的相似度函数有点积、拼接、感知机等，这里使用的是点积计算，如图10 - 7所示。

![image](https://github.com/user-attachments/assets/31319bac-af81-49aa-a73c-49120aa5ff9e)


（2）使用一个Softmax函数对这些权重进行归一化。

Softmax函数的作用是计算不同输入之间的权重“分数”，又称为权重系数。例如，正在考虑Thinking这个词，就用它的$q_1$乘以每个位置的$k_i$，随后将得分加以处理再传递给Softmax，然后通过Softmax计算，其目的是使分数归一化，如图10 - 8所示。

这个Softmax计算分数决定了每个单词在该位置表达的程度。相关联的单词将具有相应位置上最高的Softmax分数。用这个得分乘以每个Value向量，可以增强需要关注单词的值，或者降低对不相关单词的关注度。

![image](https://github.com/user-attachments/assets/a2d87e72-7cd2-4b1c-b847-dc6c9ebd9891)



Softmax的分数决定了当前单词在每个句子中每个单词位置的表示程度。很明显，当前单词对应句子中此单词所在位置的Softmax的分数最高，但是有时attention机制也能关注到此单词外的其他单词。

（3）每个Value向量乘以Softmax后的得分，如图10 - 9所示。

累加计算相关向量。这会在此位置产生自注意力层的输出（对于第一个单词），即将权重和相应的键值Value进行加权求和，得到最后的注意力值。


![image](https://github.com/user-attachments/assets/2d1204f6-a28a-40ee-8958-ef84646d2b14)



总结自注意力的计算过程，根据输入的query与key计算两者之间的相似性或相关性，之后通过一个Softmax来对值进行归一化处理，获得注意力权重值，然后对Value进行加权求和，并得到最终的Attention数值。然而，在实际的实现过程中，该计算会以矩阵的形式完成，以便更快地处理。自注意力公式如下：

\[Attention(Query, Source)=\sum_{i = 1}^{L_x}Similarity(Query, key_i)\times Value_i\]


换成更为通用的矩阵点积的形式来实现，其结构和形式如图10 - 10所示。

![image](https://github.com/user-attachments/assets/a120bed2-a2fc-4119-99ad-b57a320c3008)




3. **自注意力的代码实现**

下面进行自注意力的代码实现，实际上通过上面两步的讲解，自注意力模型的基本架构其实并不复杂，基本代码如下（仅供演示）。

**【程序10 - 1】**
```python
import torch
import math
import einops.layers.torch as elt
# word_embedding_table =
torch.nn.Embedding(num_embeddings=encoder_vocab_size,embedding_dim=312)
# encoder_embedding = word_embedding_table(inputs)

vocab_size = 1024  #字符的种类
embedding_dim = 312
hidden_dim = 256
token = torch.ones(size=(5,80),dtype=int)
#创建一个输入Embedding值
input_embedding = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim=embedding_dim)(token)

#对输入的input_embedding进行修正，这里进行了简写
query = torch.nn.Linear(embedding_dim,hidden_dim)(input_embedding)
key = torch.nn.Linear(embedding_dim,hidden_dim)(input_embedding)
value = torch.nn.Linear(embedding_dim,hidden_dim)(input_embedding)

key = elt.Rearrange("b l d -> b d l")(key)
#计算query与key之间的权重系数
attention_prob = torch.matmul(query,key)

#使用softmax对权重系数进行归一化计算
attention_prob = torch.softmax(attention_prob,dim=-1)

#计算权重系数与value的值，从而获取注意力值
attention_score = torch.matmul(attention_prob,value)

print(attention_score.shape)
```

核心代码实现起来实际上很简单，这里读者先掌握这些核心代码即可。

换个角度，从概念上对注意力机制进行解释，注意力机制可以理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大，越聚焦于其对应的Value值上，即权重代表信息的重要性，而权重与Value的点积是其对应的最终信息。

完整的注意力层代码如下。这里读者需要注意的是，在实现Attention的完整代码中，相对于前面的代码段，在这里加入了mask部分，用于在计算时忽略为了将所有的序列padding成一样的长度而进行的掩码计算的操作，具体在10.1.3节会介绍。

**【程序10 - 2】**
```python
import torch
import math
import einops.layers.torch as elt

class Attention(torch.nn.Module):
    def __init__(self,embedding_dim = 312,hidden_dim = 256):
        super().__init__()
        self.query_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.key_layer = torch.nn.Linear(embedding_dim, hidden_dim)
        self.value_layer = torch.nn.Linear(embedding_dim, hidden_dim)

    def forward(self,embedding,mask):
        input_embedding = embedding

        query = self.query_layer(input_embedding)
        key = self.key_layer(input_embedding)
        value = self.value_layer(input_embedding)

        key = elt.Rearrange("b l d -> b d l")(key)
        # 计算query与key之间的权重系数
        attention_prob = torch.matmul(query, key)

        # 使用softmax对权重系数进行归一化计算
        attention_prob += mask * -1e5  # 在自注意力权重基础上加上掩码值
        attention_prob = torch.softmax(attention_prob, dim=-1)

        # 计算权重系数与value的值，从而获取注意力值
        attention_score = torch.matmul(attention_prob, value)
        return (attention_score)
```
具体结果请读者自行打印查阅。


#### 10.1.3 ticks和Layer Normalization

10.1.2节的最后，我们基于PyTorch 2.0自定义层的形式编写了注意力模型的代码。与演示的代码有区别的是，实战代码中在这一部分的自注意层中还额外加入了mask值，即掩码层。掩码层的作用是获取输入序列的“有意义的值”，而忽视本身就是用作填充或补全序列的值。一般用0表示有意义的值，而用1表示填充值（这点并不固定，0和1的意思可以互换）。

```js
[2,3,4,5,5,4,0,0,0]->[0,0,0,0,0,0,1,1,1]
```


掩码计算的代码如下：
```python
def create_padding_mask(seq):
    mask = torch.not_equal(seq, 0).float()
    mask = torch.unsqueeze(mask, dim=-1)
    return mask
```
此外，计算出的Query与Key的点积还需要除以一个常数，其作用是缩小点积的值以方便进行Softmax计算。这常被称为ticks，即采用一个小技巧使得模型训练能够更加准确和便捷。Layer Normalization函数也是如此。下面对其进行详细介绍。
Layer Normalization函数是专门用作对序列进行整形的函数，其目的是防止字符序列在计算过程中发散，从而使得神经网络在拟合的过程中受影响。PyTorch 2.0中对Layer Normalization的使用准备了高级API，调用如下：
```python
layer_norm = torch.nn.LayerNorm(normalized_shape=[80,312], eps=1e-05, elementwise_affine=True, device=None, dtype=None)
embedding = layer_norm(embedding)  #使用layer_norm对输入数据进行处理
```
图10 - 11展示了Layer Normalization函数与Batch Normalization函数的不同。从图10 - 11中可以看到，Batch Normalization是对一个batch中不同序列中处于同一位置的数据进行归一化计算，而Layer Normalization是对同一序列中不同位置的数据进行归一化处理。

![image](https://github.com/user-attachments/assets/625c23ba-48c8-4bb0-b208-1ff7cb793792)


有兴趣的读者可以展开学习，这里就不再过多阐述了。具体的使用如下（注意一定要显式声明归一化的维度）：
```python
embedding = torch.rand(size=(5,80,312))
print(torch.nn.LayerNorm(normalized_shape=[80,312])(embedding).shape) #显式声明归一化的维度
```

#### 10.1.4 多头
