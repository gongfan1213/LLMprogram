### 8.4 针对文本的卷积神经网络模型简介——词卷积
使用字符卷积对文本进行分类是可以的，但是相对于词来说，字符包含的信息没有词多，即使卷积神经网络能够较好地对数据信息进行学习，但是由于包含的内容关系，其最终效果也只是差强人意。

在字符卷积的基础上，研究人员尝试使用词为基础数据对文本进行处理，图8-29是使用CNN来构建词卷积模型。

![image](https://github.com/user-attachments/assets/339151b0-7c20-4e8b-88ad-57011b236164)



在实际读写中，短文本用于表达较为集中的思想，文本长度有限，结构紧凑，能够独立表达意思，因此可以使用基于词卷积的神经网络对数据进行处理。



#### 8.4.1 单词的文本处理

首先对文本进行处理，使用卷积神经网络对单词进行处理的基本要求就是将文本转换成计算机可以识别的数据。8.3节使用卷积神经网络对字符的One - Hot矩阵进行了分析处理，一个简单的想法是，是否可以将文本中的单词依旧处理成One - Hot矩阵进行处理，如图8-30所示。

![image](https://github.com/user-attachments/assets/6048e92c-143c-476b-a5df-8b17dd7479f1)



使用One - Hot对单词进行表示从理论上可行，但是事实上并不是一种可行方案，对于基于字符的One - Hot方案来说，所有的字符都会在一个相对合适的字库中选取，例如从26个字母或者一些常用的字符中选取，总量并不会很多（通常少于128个），因此组成的矩阵也不会很大。

但是对于单词来说，常用的英文单词或者中文词语一般在5000左右，因此建立一个稀疏的、庞大的One - Hot矩阵是一个不切实际的想法。

目前来说，一个较好的解决方案是使用Word2Vec的Word Embedding，这样可以通过学习将字库中的词转换成维度一定的向量，作为卷积神经网络的计算依据。本小节的处理和计算依旧使用文本标题作为处理的目标。单词的词向量的建立步骤如下。

1. **分词模型的处理**

对读取的数据进行分词处理，与One - Hot的数据读取类似，首先对文本进行清理，去除停用词和标准化文本，但是需要注意的是，对于Word2Vec训练模型来说，需要输入若干个词列表，因此对获取的文本要进行分词，转换成数组的形式存储。

```python
def text_clearTitle_word2vec(text):
    text = text.lower()  #将文本转换成小写
    text = re.sub(r"[^a - z]", " ", text)  #替换非标准字符，^是求反操作
    text = re.sub(r"+", " ", text)  #替换多重空格
    text = text.strip()  #取出首尾空格
    text = text + " eos"  #添加结束符，注意eos前有空格
    text = text.split(" ")  #对文本分词转成列表存储
    return text
```

请读者自行验证。

2. **分词模型的训练与载入**

对分词模型进行训练与载入，基于已有的分词数组对不同维度的矩阵分别进行处理。这里需要注意的是，对于Word2Vec词向量来说，简单地将待补全的矩阵用全0补全是不合适的，最好的方法是将全0矩阵修改为一个非常小的常数矩阵，代码如下：

```python
def get_word2vec_dataset(n = 12):
    agnews_label = []  #创建标签列表
    agnews_title = []  #创建标题列表
    agnews_train = csv.reader(open('../datasets/ag_news_dataset/train.csv', "r"))
    for line in agnews_train:
        agnews_label.append(np.int(line[0]))  #将数据读取到对应列表中
        agnews_title.append(text_clearTitle_word2vec(line[1]))  #对数据进行清洗之后再读取
    from gensim.models import word2vec  #导入Gensim包
    model = word2vec.Word2Vec(agnews_title, vector_size=64, min_count=0, window=5) #设置训练参数
    train_dataset = []
    for line in agnews_title:
        length = len(line)  #对长度进行判定
        if length > n:
            line = line[:n]  #获取列表长度
            word2vec_matrix = (model.wv[line])  #对列表长度进行判断
            train_dataset.append(word2vec_matrix)  #截取需要的长度列表
        else:
            word2vec_matrix = (model.wv[line])  #获取word2vec矩阵
            pad_length = n - length  #获取需要补全的长度
            pad_matrix = np.zeros((pad_length, 64)) + 1e - 10  #创建补全矩阵并增加一个小数值
            word2vec_matrix = np.concatenate((word2vec_matrix, pad_matrix), axis=0)  #将word2vec矩阵添加到训练集中
            train_dataset.append(word2vec_matrix)
    train_dataset = np.expand_dims(train_dataset, 3)  #对三维矩阵进行扩展
    label_dataset = get_label_one_hot(agnews_label)  #转换成one - hot矩阵
    return train_dataset, label_dataset
```
经过向量化处理后的AG_news数据集，最终结果如下：

(120000, 12, 64, 1)

(120000, 5)

注意：在代码的倒数第4行使用了np.expand_dims函数对三维矩阵进行扩展，在不改变具体数值大小的前提下扩展了矩阵的维度，这样是为下一步使用二维卷积对文本进行分类做数据准备。



#### 8.4.2 卷积神经网络文本分类模型的实现——Conv2d（二维卷积）

![image](https://github.com/user-attachments/assets/d0bca95b-9d58-480f-b33f-7d6bc460c42e)



如图8-31所示是对卷积神经网络进行设计。

模型的思想很简单，根据输入的已转化成word embedding形式的词矩阵，通过不同的卷积提取不同的长度进行二维卷积计算，对最终的计算值进行链接，然后经过池化层获取不同矩阵的均值，最后通过一个全连接层对其进行分类。

使用模型进行完整训练的代码如下：

```python
import torch
import einops.layers.torch as elt

def word2vec_CNN(input_dim = 28):
    model = torch.nn.Sequential(
        elt.Rearrange("b l d l -> b l 1 d"),
        #第一层卷积
        torch.nn.Conv2d(1,3,kernel_size=3),
        torch.nn.ReLU(),
        torch.nn.BatchNorm2d(num_features=3),

        #第二层卷积
        torch.nn.Conv2d(3, 5, kernel_size=3),
        torch.nn.ReLU(),
        torch.nn.BatchNorm2d(num_features=5),

        #flatten
        torch.nn.Flatten(), #[batch_size,64 * 28]
        torch.nn.Linear(2400,64),
        torch.nn.ReLU(),
        torch.nn.Linear(64,5),
        torch.nn.Softmax()
    )
    return model

"""----------------下面是模型训练部分----------------"""
import get_data_84 as get_data
from sklearn.model_selection import train_test_split
train_dataset,label_dataset = get_data.get_word2vec_dataset()
X_train,X_test,y_train,y_test = 
train_test_split(train_dataset,label_dataset,test_size=0.1, random_state=828) #将数据集划分为训练集和测试集

#获取device
device = "cuda" if torch.cuda.is_available() else "cpu"
model = word2vec_CNN().to(device)

#定义交叉熵损失函数
def cross_entropy(pred, label):
    res = -torch.sum(label * torch.log(pred)) / label.shape[0]
    return torch.mean(res)

optimizer = torch.optim.Adam(model.parameters(), lr=1e - 4)
batch_size = 128
train_num = len(X_test)//128

for epoch in range(99):
    train_loss = 0
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size
        x_batch = torch.tensor(X_train[start:end]).type(torch.float32).to(device)
        y_batch = torch.tensor(y_train[start:end]).type(torch.float32).to(device)
        pred = model(x_batch)
        loss = cross_entropy(pred, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item() # 记录每个批次的损失值

    #计算并打印损失值
    train_loss = train_loss / train_num
    accuracy = (pred.argmax(1) == y_batch.argmax(1)).type(torch.float32).sum().item() / batch_size
    print("epoch ",epoch,"train_loss:",
    round(train_loss,2),",accuracy:",round(accuracy,2))
```
模型使用不同的卷积核分别生成了3通道和5通道的卷积计算值，池化以后将数据拉伸并连接为平整结构，之后使用两个全连接层完成最终的矩阵计算与预测。

通过对模型的训练可以看到，最终测试集的准确率应该在80%左右，请读者根据配置自行完成。

### 8.5 使用卷积对文本分类的补充内容

在前面的章节中，通过不同的卷积（一维卷积和二维卷积）实现了文本的分类，并且通过使用Gensim掌握了对文本进行词向量转换的方法。词向量是目前常用的将文本转换成向量的方法，比较适合较为复杂的词袋中词组较多的情况。

使用one - hot方法对字符进行表示是一种非常简单的方法，但是由于其使用受限较大，产生的矩阵较为稀疏，因此实用性并不是很强，这里推荐使用词向量的方式对词进行处理。

可能有读者会产生疑问，使用Word2Vec的形式来计算字符的字向量是否可行？答案是完全可以，并且相对于单纯采用one - hot形式的矩阵表示，会有更好的表现和准确度。


#### 8.5.1 汉字的文本处理
对于汉字的文本处理，一个非常简单的方法是将汉字转换成拼音的形式，可以使用Python提供的拼音库包：
```bash
pip install pypinyin
```
使用方法如下：
```python
from pypinyin import pinyin, lazy_pinyin, Style
value = lazy_pinyin('你好') # 不考虑多音字的情况
print(value)
```
打印结果如下：

['ni', 'hao']

这里使用不考虑多音字的普通模式，除此之外，还有带拼音符号的多音字字母，有兴趣的读者可以自行学习。

较为常用的对汉字文本进行的方法是使用分词器对文本进行分词，将分词后的词数列去除停用词和副词之后，制作词向量，如图8-32所示。

![image](https://github.com/user-attachments/assets/f45e36e9-cac2-485c-a705-9966c3a0793b)



接下来对文本进行分词并将其转化成词向量的形式进行处理。

1. **读取数据**

对于数据的读取，这里为了演示，直接使用字符串作为数据的存储格式，而对于多行文本的读取，读者可以使用Python类库中的文本读取工具，这里不过多阐述。

“在上面的章节中，作者通过不同的卷积（一维卷积和二维卷积）实现了文本的分类，并且通过使用Gensim掌握了对文本进行词向量转换的方法。词向量Word Embedding是目前最常用的将文本转成向量的方法，比较适合较为复杂词袋中词组较多的情况。使用one - hot方法对字符进行表示是一种非常简单的方法，但是由于其使用受限较大，产生的矩阵较为稀疏，因此在实用性上并不是很强，作者在这里统一推荐使用Word Embedding的方式对词进行处理。可能有读者会产生疑问，如果使用Word2Vec的形式来计算字符的“字向量”是否可行。那么作者的答案是完全可以，并且准确度相对于单纯采用one - hot形式的矩阵表示，都能有更好的表现和准确度。”

2. **中文文本的清理与分词**

使用分词工具对中文文本进行分词计算。对于文本分词工具，Python类库中最为常用的是jieba分词，导入如下：

```python
import jieba #分词器
import re #正则表达式库包
```
对于正文的文本，首先需要对其进行清洗和提出非标准字符，这里采用re正则表达式对文本进行处理，部分处理代码如下：
```python
text = re.sub(r"[^a-zA-Z0-9-，。“”（）]", " ", text) #替换非标准字符，^是求反操作
text = re.sub(r"+", " ", text) #替换多重空格
text = re.sub(" ", "", text) #替换隔断空格
```

![image](https://github.com/user-attachments/assets/9b58e92b-1483-43a3-a100-1bb08aeb8a8b)



处理好的文本如图8-33所示。

可以看到，文本中的数字、非汉字字符以及标点符号已经被删除，并且其中由于删除不标准字符所遗留的空格也一一被删除，留下的是完整的待切分的文本内容。

jieba库可用于对中文文本进行分词，其分词函数如下：

```python
text_list = jieba.lcut_for_search(text)
```

这里使用jieba库对文本进行分词，然后将分词后的结果以数组的形式存储，打印结果如图8-34所示。

![image](https://github.com/user-attachments/assets/c266e71c-ffe1-4314-8a9e-8aa5e549b9a4)



3. **使用Gensim构建词向量**

读者应该比较熟悉Gensim构建词向量的方法，这里直接使用，代码如下：

```python

from gensim.models import word2vec #导入Gensim包
# 设置训练参数，注意方括号中的内容
model = word2vec.Word2Vec([text_list], size=50, min_count=1, window=3)
print(model["章节"])
```

有一个非常重要的细节，因为Word2Vec.Word2Vec函数接收的是一个二维数组，而本文通过jieba分词的结果是一个一维数组，因此需要在其上添加一个数组符号，人为地构建一个新的数据结构，否则在打印词向量时会报错。

执行代码，等待Gensim训练完成后，打印一个字符向量，如图8-35所示。

![image](https://github.com/user-attachments/assets/769f8e50-86bb-438d-8334-81f95128544d)


完整代码如下。

【程序8-10】
```python
import jieba
import re

text = re.sub(r"[^a-zA-Z0-9-，。“”（）]", " ", text) #替换非标准字符，^是求反操作
text = re.sub(r"+", " ", text) #替换多重空格
text = re.sub(" ", "", text) #替换隔断空格
print(text)
text_list = jieba.lcut_for_search(text)
from gensim.models import word2vec #导入Gensim包
model = word2vec.Word2Vec([text_list], size=50, min_count=1, window=3) # 设置训练参数
print(model["章节"])
```
后续工程读者可以参考二维卷积对文本处理的模型进行计算。

#### 8.5.2 其他细节

通过上一小节的演示读者可以看到，对于普通的文本完全可以通过一系列的清洗和向量化处理将其转换成矩阵的形式，之后通过卷积神经网络对文本进行处理。在8.5.1节中只是做了中文向量的词处理，缺乏主题的提取、去除停用词等操作，读者可以自行学习，根据需要进行补全。

一个非常重要的想法是，对于词嵌入构成的矩阵，能否使用已有的模型进行处理？例如在前面的章节中手把手带领读者实现的ResNet网络，以及加上了注意力机制的ResNet模型，如图8-36所示。

![image](https://github.com/user-attachments/assets/8246af42-cf0a-4e3d-b8b0-b8b8421d587d)


答案是可以的，作者在进行文本识别的过程中，使用过ResNet50作为文本模型识别器，同样可以获得不低于现有模型的准确率，有兴趣的读者可以自行验证。

### 8.6 本章小结

卷积神经网络并不是只能对图像进行处理，本章演示了使用卷积神经网络对文本进行分类的方法。对于文本处理来说，传统的基于贝叶斯分类和循环神经网络实现的文本分类方法，卷积神经网络同样可以实现，而且效果并不比前面的差。

卷积神经网络的应用非常广泛，通过正确的数据处理和建模，可以达到程序设计人员心中所要求的目标。更为重要的是，相对于循环神经网络来说，卷积神经网络在训练过程中，训练速度更快（并发计算），处理范围更大（图矩阵），能够获取更多的相互关联（感受野）。因此，卷积神经网络在机器学习中的作用越来越重要。

预训练词向量是本章新加入的内容，可能有读者会问使用Word Embedding等价于什么？等价于把Embedding层的网络用预训练好的参数矩阵初始化。但是只能初始化第一层网络参数，再高层的参数就无能为力了。

而下游NLP任务在使用词嵌入的时候一般有两种做法：一种是Frozen，就是词嵌入这层网络参数固定不动；另一种是Fine - Tuning（微调），就是词嵌入这层参数使用新的训练集合训练，也需要跟着训练过程更新词嵌入。 
