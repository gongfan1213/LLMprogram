### 第3章 从零开始学习PyTorch 2.0
第2章完成了第一个PyTorch深度学习示例程序——一个非常简单的MNIST手写体生成器，其作用是向读者演示一个PyTorch深度学习程序的基本构建与完整的训练过程。

PyTorch作为一个成熟的深度学习框架，对于使用者来说，即使是初学者，也能很容易地用其进行深度学习项目的训练，只要编写出简单的代码就可以构建相应的模型进行实验，但其缺点在于框架的背后内容都被隐藏起来了。

本章将使用Python实现一个轻量级的、易于扩展的深度学习框架，目的是希望读者从这一过程中了解深度学习的基本组件以及框架的设计和实现，从而为后续的学习打下基础。

本章首先使用PyTorch完成MNIST分类的练习，主要是为了熟悉PyTorch的基本使用流程；之后将实现一个自定义的深度学习框架，从基本的流程开始分析，对神经网络中的关键组件进行抽象，确定基本框架，然后对框架中的各个组件进行代码实现；最后基于自定义框架实现MNIST分类，并与PyTorch实现的MNIST分类进行简单的对比验证。

#### 3.1 实战MNIST手写体识别
第2章对MNIST数据集做了介绍，描述了其构成方式及其数据特征和标签含义等。了解这些信息有助于编写合适的程序来对MNIST数据集进行分析和分类识别。本节将实现MNIST数据集分类的任务。

**3.1.1 数据图像的获取与标签的说明**
第2章已经详细介绍了MNIST数据集，我们可以使用下面代码获取数据：
```python
import numpy as np
x_train = np.load("./dataset/mnist/x_train.npy")
y_train_label = np.load("./dataset/mnist/y_train_label.npy")
```
基本数据的获取在第2章也做了介绍，这里不再过多阐述。需要注意的是，我们在第2章介绍MNIST数据集时，只使用了图像数据，没有对标签进行说明，在这里重点对数据标签，也就是`y_train_label`进行介绍。

下面使用`print(y_train_label[:10])`打印出数据集的前10个标签，结果如下：
```
[5 0 4 1 9 2 1 3 1 4]
```
可以很清楚地看到，这里打印出了10个字符，每个字符对应相应数字的数据图像所对应的数字标签，即图像3的标签，对应的就是3这个数字字符。

可以说，训练集中每个实例的标签对应0 - 9的任意一个数字，用以对图片进行标注。另外，需要注意的是，对于提取出来的MNIST的特征值，默认使用一个0 - 9的数值进行标注，但是这种标注方法并不能使得损失函数获得一个好的结果，因此这里主要介绍将单一序列转换成one - hot的方法，将其具体落在某个标注区间中。

one - hot的标注方法请读者查找材料自行学习。这里主要介绍将单一序列转换成one - hot的方法，一般情况下，可以用NumPy实现one - hot的表示方法，但是这样转换成的是numpy.array格式的数据，并不适合直接输入到PyTorch中。

如果读者能够自行编写将序列值转换成one - hot的函数，那么你的编程功底真不错，不过PyTorch提供了已经编写好的转换函数：
```python
torch.nn.functional.one_hot
```
完整的one - hot使用方法如下：
```python
import numpy as np
import torch
x_train = np.load("./dataset/mnist/x_train.npy")
y_train_label = np.load("./dataset/mnist/y_train_label.npy")
x = torch.tensor(y_train_label[:5], dtype=torch.int64)
# 定义一个张量输入，因为此时有5个数值，且最大值为9，类别数为10
# 所以我们可以得到y的输出结果的形状为shape=(5,10)，5行10列
y = torch.nn.functional.one_hot(x, 10) # 一个参数张量x，10为类别数
```
运行结果如图3 - 1所示。

可以看到，one - hot的作用是将一个序列转换成以one - hot形式表示的数据集。所有的行或者列都被设置成0，而每个特定的位置都用一个1来表示，如图3 - 2所示。

![image](https://github.com/user-attachments/assets/7eba3f32-f5f3-4153-b3d0-7874e632e42f)


简单来说，MNIST数据集的标签实际上就是一个表示60,000幅图片的60,000×10大小的矩阵张量[60000,10]。前面的行数指的是数据集中的图片为60,000幅，后面的10是指10个列向量。

**3.1.2 实战基于PyTorch 2.0的手写体识别模型**

本小节使用PyTorch 2.0框架完成MNIST手写体数字的识别。

1. **模型的准备（多层感知机）**
第2章讲过了，PyTorch最重要的一项内容是模型的准备与设计，而模型的设计最关键的一点就是了解输出和输入的数据结构类型。

通过第2章图像降噪的演示，读者已经了解到我们输入的数据是一个[28,28]大小的二维图像。而通过对数据结构的分析可以得知，对于每个图形都有一个确定的分类结果，也就是一个0 - 9之间的确定数字。

因此为了实现对输入图像进行数字分类这个想法，必须设计一个合适的判别模型。而从上面对图像的分析来看，最直观的想法就将图形作为一个整体结构直接输入到模型中进行判断。基于这种思路，简单的模型设计就是同时对图像所有参数进行计算，即使用一个多层感知机（Multilayer Perceptron，MLP）来对图像进行分类。整体的模型设计结构如图3 - 3所示。

![image](https://github.com/user-attachments/assets/70c6a4c9-528c-4eb0-bf21-dd001d1f2c02)


从图3 - 3可以看到，一个多层感知机模型就是将数据输入后，分散到每个模型的节点（隐藏层），进行数据计算后，即可将计算结果输出到对应的输出层中。多层感知机的模型结构如下：

```python
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28,312),
            nn.ReLU(),
            nn.Linear(312, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits
```

2. **损失函数的表示与计算**

在第2章中，我们使用MSE作为目标图形与预测图形的损失值，而在本例中，我们需要预测的目标是图形的“分类”，而不是图形表示本身，因此我们需要寻找并使用一种新的能够对类别归属进行“计算”的函数。

![image](https://github.com/user-attachments/assets/b559b1d8-defe-49b4-ad1b-07369bc0cc7d)


本例所使用的损失函数为`torch.nn.CrossEntropyLoss`。PyTorch官网对其介绍如下：

```python
CLASS torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean', label_smoothing=0.0)
```
该损失函数计算输入值（Input）和目标值（Target）之间的交叉熵损失。交叉熵损失函数可用于训练一个单标签或者多标签类别的分类问题。给定参数weight时，其为分配给每个类别的权重的一维张量（Tensor）。当数据集分布不均衡时，这个参数很有用。

同样需要注意的是，因为`torch.nn.CrossEntropyLoss`内置了Softmax运算，而Softmax的作用是计算分类结果中最大的那个类。从图3 - 4所示的代码实现中可以看到，此时`CrossEntropyLoss`已经在实现计算时完成了Softmax计算，因此在使用`torch.nn.CrossEntropyLoss`作为损失函数时，不需要在网络的最后添加Softmax层。此外，label应为一个整数，而不是one - hot编码形式。



代码如下：
```python
import torch
y = torch.LongTensor([0])
z = torch.Tensor([[0.2,0.1,-0.1]])
criterion = torch.nn.CrossEntropyLoss()
loss = criterion(z,y)
print(loss)
```
目前读者需要掌握的就是这些内容，`CrossEntropyLoss`的数学公式较为复杂，建议学有余力的读者查阅相关资料进行学习。

3. **基于PyTorch的手写体数字识别**
   
下面开始实现基于PyTorch的手写体数字识别。通过前文的介绍，我们还需要定义深度学习的优化器部分，在这里采用Adam优化器，代码如下：

### 第3章 从零开始学习PyTorch 2.0

```python
model = NeuralNetwork()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5) #设定优化函数
```

在这里首先需要定义模型，然后将模型参数传入优化器中，lr是对学习率的设定，根据设定的学习率进行模型计算。完整的手写体数字识别模型如下：

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0' #指定GPU编码
import torch
import numpy as np
from tqdm import tqdm

batch_size = 320 #设定每次训练的批次数
epochs = 1024 #设定训练次数

#device = "cpu" #PyTorch的特性，需要指定计算的硬件，如果没有GPU，就使用CPU进行计算
device = "cuda" #在这里默认使用GPU模式，如果出现运行问题，可以将其改成CPU模式

#设定多层感知机网络模型
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(28*28,312),
            torch.nn.ReLU(),
            torch.nn.Linear(312, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )
    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
model = model.to(device) #将计算模型传入GPU硬件等待计算
model = torch.compile(model) #PyTorch 2.0的特性，加速计算速度
loss_fu = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-5) #设定优化函数

#载入数据
x_train = np.load("../../dataset/mnist/x_train.npy")
y_train_label = np.load("../../dataset/mnist/y_train_label.npy")
train_num = len(x_train)//batch_size

#开始计算
for epoch in range(20):
    train_loss = 0
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size
        train_batch = torch.tensor(x_train[start:end]).to(device)
        label_batch = torch.tensor(y_train_label[start:end]).to(device)

        pred = model(train_batch)
        loss = loss_fu(pred, label_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item() # 记录每个批次的损失值

    # 计算并打印损失值
    train_loss /= train_num
    accuracy = (pred.argmax(1) == label_batch).type(torch.float32).sum().item() / batch_size
    print("train_loss:", round(train_loss,2),"accuracy:",round(accuracy,2))
```

此时模型的训练结果如图3 - 5所示。

|epoch|train_loss|accuracy|
| ---- | ---- | ---- |
|0|2.18|0.78|
|1|1.64|0.87|
|2|1.04|0.91|
|3|0.73|0.92|
|4|0.58|0.93|
|5|0.49|0.93|
|6|0.44|0.93|
|7|0.4|0.94|
|8|0.38|0.94|
|9|0.36|0.95|
|10|0.34|0.95|

可以看到，随着模型循环次数的增加，模型的损失值在降低，而准确率在逐渐增高，具体请读者自行验证测试。

#### 3.1.3 基于Netron库的PyTorch 2.0模型可视化

前面章节带领读者完成了基于PyTorch 2.0的MNIST模型的设计，并基于此完成了MNIST手写体数字的识别。此时可能有读者对我们自己设计的模型结构感到好奇，如果能够可视化地显示模型结构就更好了。



读者可以自行在百度搜索Netron。Netron是一个深度学习模型可视化库，支持可视化地表示PyTorch 2.0的模型存档文件。因此，我们可以把3.1.2节中PyTorch的模型结构保存为文件，并通过Netron进行可视化展示。保存模型的代码如下：

```python
import torch
device = "cuda" #在这里默认使用GPU模式，如果出现运行问题，可以将其改成CPU模式

#设定多层感知机网络模型
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(28*28,312),
            torch.nn.ReLU(),
            torch.nn.Linear(312, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )
    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits

#进行模型的保存
model = NeuralNetwork()
torch.save(model, './model.pth') #将模型保存为pth文件
```
建议读者从GitHub上下载Netron，其主页提供了基于不同版本的安装方式，如图3 - 6所示。

![image](https://github.com/user-attachments/assets/8ada246e-5bba-4d07-be7c-89ff31597474)


读者可以依照操作系统的不同下载对应的文件，在这里安装的是基于Windows的.exe文件，安装后是一个图形界面，直接在界面上单击file操作符号打开我们刚才保存的.pth文件，显示结果如图3 - 7所示。

可以看到，此时我们定义的模型结构被可视化地展示出来了，每个模块的输入输出维度在图3 - 7上都展示出来了，单击深色部分可以看到每个模块更详细的说明，如图3 - 8所示。

![image](https://github.com/user-attachments/assets/a409a9a4-d901-4f35-90f3-38f3a038c648)


感兴趣的读者可以自行安装查看。

![image](https://github.com/user-attachments/assets/37461dd8-1066-4034-87b8-0c003c48e3f4)


#### 3.2 自定义神经网络框架的基本设计

本章学习自定义神经网络框架，稍微有点困难，建议有一定编程基础的读者掌握一下，其他读者了解一下即可。

对于一个普通的神经网络运算流程来说，最基本的过程包含两个阶段，即训练（training）和预测（predict）。而训练的基本流程包括输入数据、网络层前向传播、计算损失、网络层反向传播梯度、更新参数这一系列过程。对于预测来说，又分为输入数据、网络层前向传播和输出结果。


**3.2.1 神经网络框架的抽象实现**

神经网络的预测就是训练过程的一部分，因此，基于训练的过程，我们可以对神经网络中的基本组件进行抽象。在这里，神经网络的组件被抽象成4部分，分别是数据输入、计算层（包括激活层）、损失计算以及优化器，如图3 - 9所示。

![image](https://github.com/user-attachments/assets/8cfbdd51-e6c7-47c5-9585-4ebf8716ff5c)


各个部分的作用如下：

- **输入数据**：这个是神经网络中数据输入的基本内容，一般我们将其称为tensor。

- **计算层**：负责接收上一层的输入，进行该层的运算，并将结果输出给下一层，由于tensor的流动有前向和反向两个方向，因此对于每种类型的网络层，我们都需要同时实现forward和backward两种运算。

- **激活层**：通常与计算层结合在一起对每个计算层进行非线性分割。 

- **损失计算**：在给定模型预测值与真实值之后，使用该组件计算损失值以及关于最后一层的梯度。

- **优化器**：负责使用梯度更新模型的参数。


基于上面的分析，我们可以按照抽象的认识完成深度学习代码的流程设计，如下所示：

```python
# define model
net = Net(Activity([layer1, layer2,...]))  #数据的激活与计算
model = Model(net, loss_fn, optimizer)

# training
pred = model.forward(train_X)  #前向计算
loss, grads = model.backward(pred, train_Y)  #反向计算
model.apply_grad(grads)  #参数优化

# inference
test_pred = model.forward(test_X)  #预测过程
```
上面代码中，我们定义了一个net计算层，然后将net、loss - fn、optimizer一起传给model。model实现了forward、backward和apply_grad三个接口，分别对应前向传播、反向传播和参数更新三个功能。下面我们分别对这些内容进行实现。

**3.2.2 自定义神经网络框架的具体实现**

本小节演示自定义神经网络框架的具体实现，这个实现较为困难，请读者结合本书配套源码包中的train.py文件按下面的说明步骤进行学习。

1. **tensor数据包装**

根据前面的分析，首先需要实现数据的输入输出定义，即张量的定义类型。张量是神经网络中的基本数据单位，为了简化起见，这里直接使用numpy.ndarray类作为tensor类的实现。

```python
import numpy as np
tensor = np.random.random(size=(10,28,28,1))
```
上面代码中，我们直接使用NumPy包中的random函数生成数据。

2. **layer计算层的基类与实现**

计算层的作用是对输入的数据进行计算，在这一层中输入数据的前向计算在forward过程中完成，相对于普通的计算层来说，除了需要计算forward过程外，还需要实现一个参数更新的backward过程。因此，一个基本的计算层的基类如下：

```python
class Layer:
    """Base class for layers."""
    def __init__(self):
        self.params = {p: None for p in self.param_names}
        self.nt_params = {p: None for p in self.nt_param_names}
        self.initializers = {}
        self.grads = {}
        self.shapes = {}
        self._is_training = True  # used in BatchNorm/Dropout layers
        self._is_init = False
        self.ctx = {}

    def __repr__(self):
        shape = None if not self.shapes else self.shapes
        return f"layer: {self.name}\tshape: {shape}"

    def forward(self, inputs):
        raise NotImplementedError

    def backward(self, grad):
        raise NotImplementedError

    @property
    def is_init(self):
        return self._is_init

    @is_init.setter
    def is_init(self, is_init):
        self._is_init = is_init
        for name in self.param_names:
            self.shapes[name] = self.params[name].shape

    @property
    def is_training(self):
        return self._is_training

    @is_training.setter
    def is_training(self, is_train):
        self._is_training = is_train

    @property
    def name(self):
        return self.__class__.__name__

    @property
    def param_names(self):
        return ()

    @property
    def nt_param_names(self):
        return ()

    def _init_params(self):
        for name in self.param_names:
            self.params[name] = self.initializers[name](self.shapes[name])
        self._is_init = True
```
下面实现一个基本的神经网络计算层——全连接层。关于全连接层的详细介绍，我们在后续章节中会讲解，在这里主要将其作为一个简单的计算层来实现。

在全连接层的计算过程中，forward接受上层的输入inputs实现ωx + b的计算；backward正好相反，接受来自反向的梯度。具体实现如下：
```python
class Dense(Layer):
    """A dense layer operates 'outputs = dot(inputs, weight) + bias'
    :param num_out: A positive integer, number of output neurons
    :param w_init: Weight initializer
    :param b_init: Bias initializer
    """
    def __init__(self,
                 num_out,
                 w_init=XavierUniform(),
                 b_init=Zeros()):
        super().__init__()
        self.initializers = {"w": w_init, "b": b_init}
        self.shapes = {"w": [None, num_out], "b": [num_out]}

    def forward(self, inputs):
        if not self.is_init:
            self.shapes["w"][0] = inputs.shape[1]
            self._init_params()
        self.ctx = {"X": inputs}
        return inputs @ self.params["w"] + self.params["b"]

    def backward(self, grad):
        self.grads["w"] = self.ctx["X"].T @ grad
        self.grads["b"] = np.sum(grad, axis=0)
        return grad @ self.params["w"].T

    @property
    def param_names(self):
        return "w", "b"
``` 
