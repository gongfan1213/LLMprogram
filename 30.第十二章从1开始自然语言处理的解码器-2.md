### 12.2 解码器实战——拼音汉字翻译模型
经过前面章节的学习，本节进入解码器实战——拼音汉字翻译模型。

前面的章节带领读者学习了注意力模型、前馈层以及掩码相关知识。这3部分内容共同构成了编码器 - 解码器架构的主要内容，共同组成的就是transformer这个基本架构，如图12 - 9所示。 

![image](https://github.com/user-attachments/assets/99c49f76-9784-4f1d-ad6f-4188b961c233)


图12 - 9 解码器


本节带领读者利用前面学习的知识完成一个翻译系统。不过在开始之前，有以下两个问题留给读者：

（1）编码器 - 解码器的翻译模型与编码器的转换模型有什么区别？

（2）如果想做汉字→拼音的翻译系统，编码器和解码器的输入端分别输入什么内容？

接下来让我们开始吧。

## 12.2.1 数据集的获取与处理
首先是数据集的准备和处理，本小节准备了15万条汉字和拼音对应数据。
### 1. 数据集展示
本小节用于实战的汉字拼音数据集如下：
```js
A11_0
lv4 shi4 yang2 chun1 yan1 jing3 da4 kuai4 wen2 zhang1 de di3 se4 si4 yue4 de
lin2 luan2 geng4 shi4 lv4 de2 xian1 huo2 xiu4 mei4 shi1 yi4 ang4 ran2 绿是阳春烟景大
块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然
A11_1
tai jin3 ping2 yao1 bu4 de li4ang zhai4 yong3 dao4 shang4 xia4 fan1 teng2
dong3 she2 xing2 zhuang4 ru2 hai3 tun2 yi1 zhi2 yi3 yi1 tou2 de you1 shi4 ling3 xian1
他仅凭腰部的力量在泳道上下翻腾蠕动蛇行状如海豚一直以一头的优势领先
A11_10
pao4 yan3 da3 hao3 le zha4 yao4 zen3 me zhuang1 yue4 zheng4 cai2 yao3 le yao3
yao2 shui1 de tuo1 qu4 yi1 fu2 guang1 bang3 zi1 chong1 jin4 le shui3 cuan4 dong4 炮眼打好
了炸药怎么装吕正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞
A11_100
ke3 shei2 zhi1 wen2 wan2 hou4 tai1 yi1 zhao4 jing4 zi1 zhi3 jian4 zuo3 xia4 yan3
jian3 de xian4 you4 cui2 you4 hei1 yu3 you4 ce4 ming2 xian3 bu4 dui4 cheng1 可谁知纹完后
她一照镜子只见左下眼睑的线又粗又黑与右侧明显不对称
```

下面简单介绍一下。数据集中的数据分成3部分，每部分使用特定的空格键隔开。
```
A11_10 …… ke3 shei2 …… 可谁 ……
```

• 第一部分A11_i为序号，这里表示序列的条数和行号。
• 第二部分是拼音编号，是汉语拼音，与真实的拼音标注不同的是，去除了拼音
原始标注，而使用数字1、2、3、4替代，分别代表当前读音的第一声到第四声，这点请读
者注意。
• 最后一部分是汉字的序列，这里与第二部分的拼音部分一一对应。
## 2. 获取字库和训练数据
获取数据集中字库的个数很重要，这里使用set格式的数据对全部字库中的不同字符进行读取。
创建字库和训练数据的完整代码如下：
```python
import numpy as np
sentences = []
src_vocab = {'〇': 0, '>': 1, '<': 2}  #这个是汉字vocab
tgt_vocab = {'〇': 0, '>': 1, '<': 2}  #这个是拼音vocab

with open("../dataset/zh.tsv", errors="ignore", encoding="UTF-8") as f:
    context = f.readlines()
    for line in context:
        line = line.strip().split(" ")
        pinyin = line[1]
        hanzi = line[2]
        (hanzi_s) = hanzi.split(" ")
        (pinyin_s) = pinyin.split(" ")
        #[><]
        pinyin_inp = [">"] + pinyin_s
        pinyin_trg = pinyin_s + ["<"]
        line = [hanzi_s,pinyin_inp,pinyin_trg]
        for char in hanzi_s:
            if char not in src_vocab:
                src_vocab[char] = len(src_vocab)
        for char in pinyin_s:
            if char not in tgt_vocab:
                tgt_vocab[char] = len(tgt_vocab)

sentences.append(line)
```
这里做一个说明，首先context读取了全部数据集中的内容，之后根据空格将其分成3部分。对于拼音和汉字部分，将其转化成一个序列，并在前后分别加上起止符GO和终止符END。这实际上可以不用加，为了明确地描述起止关系，从而加上了起止标注。

实际上还需要加上一个特定符号PAD，这是为了对单行序列进行补全的操作，最终的数据如下：
```
['GO', 'liu2', 'yong3', ……, 'gan1', 'END', 'PAD', 'PAD', ……]
['GO', '柳', '永', ……, '感', 'END', 'PAD', 'PAD', ……]
```
pinyin_list和hanzi_list是两个列表，分别用来存放对应的拼音和汉字训练数据。最后不要忘记在字库中加上PAD符号。
```python
pinyin_vocab = ["PAD"] + list(sorted(pinyin_vocab))
hanzi_vocab = ["PAD"] + list(sorted(hanzi_vocab))
```
## 3. 根据字库生成Token数据
获取的拼音标注和汉字标注的训练数据并不能直接用于模型训练，模型需要转化成Token的系列数字列表，代码如下：
```python
enc_inputs, dec_inputs, dec_outputs = [], [], []
for line in sentences:
    enc = line[0];dec_in = line[1];dec_tgt = line[2]
    if len(enc) <= src_len and len(dec_in) <= tgt_len and len(dec_tgt) <= tgt_len:
        enc_token = [src_vocab[char] for char in enc];enc_token = enc_token + [0] * (src_len - len(enc_token))
        dec_in_token = [tgt_vocab[char] for char in dec_in];dec_in_token = dec_in_token + [0] * (tgt_len - len(dec_in_token))
        dec_tgt_token = [tgt_vocab[char] for char in dec_tgt];dec_tgt_token = dec_tgt_token + [0] * (tgt_len - len(dec_tgt_token))
enc_inputs.append(enc_token);dec_inputs.append(dec_in_token);dec_outputs.append(dec_tgt_token)
```
代码中创建了两个新的列表，分别对拼音和汉字的Token进行存储，从而获取根据字库序号编号后新的序列Token。
# 12.2.2 翻译模型
翻译模型就是经典的编码器 - 解码器模型，整体代码如下。
【程序12 - 5】
```python
# 导入库
import math
import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
import einops.layers.torch as elt

import get_dataset_v2
from tqdm import tqdm

sentences = get_dataset_v2.sentences
src_vocab = get_dataset_v2.src_vocab
tgt_vocab = get_dataset_v2.tgt_vocab

src_vocab_size = len(src_vocab)  #4462
tgt_vocab_size = len(tgt_vocab)  #1154

src_len = 48
tgt_len = 47  #由于输出比输入多一个符号，因此就用这个
# *********************************************
# Transformer的参数
# Transformer Parameters
d_model = 512  # 每一个词的Word Embedding用多少位表示
# （包括positional encoding应该用多少位表示，因为这两个维度要相加，应该是一样的维度）
d_ff = 2048  # FeedForward dimension
# forward线性层变成多少位（d_model->d_ff->d_model）
d_k = d_v = 64  # dimension of K(=Q)，V
# K、Q、V矩阵的维度（K和Q一定是一样的，因为要用K乘以Q的转置），V不一定
# 换一种说法，就是在进行self-attention的时候，
# 从Input（当然是加了位置编码之后的input）线性变换之后的3个向量K、Q、V的维度
#
n_layers = 6  # encoder和decoder各有多少层
n_heads = 8  # multi-head attention有几个头
# *********************************************

# 数据预处理
# 将encoder_input、decoder_input和decoder_output进行id化
enc_inputs, dec_inputs, dec_outputs = [], [], []
for line in sentences:
    enc = line[0];dec_in = line[1];dec_tgt = line[2]
    if len(enc) <= src_len and len(dec_in) <= tgt_len and len(dec_tgt) <= tgt_len:
        enc_token = [src_vocab[char] for char in enc];enc_token = enc_token + [0] * (src_len - len(enc_token))
        dec_in_token = [tgt_vocab[char] for char in dec_in];dec_in_token = dec_in_token + [0] * (tgt_len - len(dec_in_token))
        dec_tgt_token = [tgt_vocab[char] for char in dec_tgt];dec_tgt_token = dec_tgt_token + [0] * (tgt_len - len(dec_tgt_token))
enc_inputs.append(enc_token);dec_inputs.append(dec_in_token);dec_outputs.append(dec_tgt_token)

enc_inputs = torch.LongTensor(enc_inputs)
dec_inputs = torch.LongTensor(dec_inputs)
dec_outputs = torch.LongTensor(dec_outputs)
# print(enc_inputs[0])
# print(dec_inputs[0])
# print(dec_outputs[0])

# *********************************************
print(enc_inputs.shape,dec_inputs.shape,dec_outputs.shape)

class MyDataSet(Data.Dataset):
    def __init__(self, enc_inputs, dec_inputs, dec_outputs):
        super(MyDataSet, self).__init__()
        self.enc_inputs = enc_inputs
        self.dec_inputs = dec_inputs
        self.dec_outputs = dec_outputs
    def __len__(self):
        return self.enc_inputs.shape[0]
    # 有几个sentence
    def __getitem__(self, idx):
        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]
    # 根据索引查找encoder_input,decoder_input,decoder_output

loader = Data.DataLoader(
    MyDataSet(enc_inputs, dec_inputs, dec_outputs),
    batch_size=512,
    shuffle=True
)

# *********************************************
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        # max_length（一个sequence的最大长度）
        pe = torch.zeros(max_len, d_model)
        # pe [max_len,d_model]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        # position [max_len, 1]

        div_term = torch.exp(
            torch.arange(0, d_model, 2).float()
            * (-math.log(10000.0) / d_model)
        )
        # div_term:[d_model/2]
        # e^(-i*log10000/d_model)=10000^(-i/d_model)
        # d_model为embedding dimension

        # 两个相乘的维度为[max_len,d_model/2]
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # 计算position encoding
        # pe的维度为[max_len,d_model]，每一行的奇数和偶数分别取sin和cos(position * div_term)
        # 中的值
        pe = pe.unsqueeze(0).transpose(0, 1)
        # 维度变成(max_len,1,d_model)
        # 所以直接用pe=pe.unsqueeze(1)也可以
        self.register_buffer('pe', pe)
        # 放入buffer中，参数不会训练
    def forward(self, x):
        """
        x: [seq_len, batch_size, d_model]
        """
        x = x + self.pe[:x.size(0), :, :]
        # 选取和x一样维度的seq_length，将pe加到x上
        return self.dropout(x)

# 由于在Encoder和Decoder中都需要进行掩码操作，
# 因此无法确定这个函数的参数中seq_len的值
# 如果是在Encoder中调用的，seq_len就等于src_len
# 如果是在Decoder中调用的，seq_len就有可能等于src_len
# 也有可能等于tgt_len（因为Decoder有两次掩码操作）
# src_len是在encoder-decoder中的mask
# tgt_len是decdoer mask

def creat_self_mask(from_tensor, to_tensor):
    """
    这里需要注意，
    from_tensor是输入的文本序列，即input_word_ids，应该是2D的，即[1,2,3,4,5,6,0,0,0,0]
    to_tensor是输入的的input_word_ids，应该是2D的，即[1,2,3,4,5,6,0,0,0,0]
    最终的结果是输出2个3D的相乘
    注意：后面如果需要4D的，则使用expand添加一个维度即可
    """
    batch_size, from_seq_length = from_tensor.shape
    # 这里只能做self attention，不能做交互
    # assert from_tensor == to_tensor,print("输入from_tensor与to_tensor不一致，检查mask创建部分，需要自己完成")

    to_mask = torch.not_equal(from_tensor, 0).int()
    to_mask = elt.Rearrange('b l -> b l 1')(to_mask)  # 这里扩充了数据类型

    broadcast_ones = torch.ones_like(to_tensor)
    broadcast_ones = torch.unsqueeze(broadcast_ones, dim=-1)

    mask = broadcast_ones * to_mask
    mask.to("cuda")
    return mask

def create_mask(from_tensor, to_tensor):
    corss_mask = creat_self_mask(from_tensor, to_tensor)
    look_ahead_mask = torch.tril(torch.ones(to_tensor.shape[1], from_tensor.shape[1]))
    look_ahead_mask = look_ahead_mask.to("cuda")
    corss_mask = look_ahead_mask * corss_mask
    return corss_mask

class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()
    def forward(self, Q, K, V, attn_mask):
        """
        Q: [batch_size, n_heads, len_q, d_k]
        K: [batch_size, n_heads, len_k, d_k]
        V: [batch_size, n_heads, len_v(=len_k), d_v]
        attn_mask: [batch_size, n_heads, seq_len, seq_len]
        """
        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)
        # scores : [batch_size, n_heads, len_q, len_k]
        scores.masked_fill_(attn_mask == 0, -1e9)
        # 有attn_mask所有为True的部分（有Padding的部分），scores填充为负无穷，也就是这个位置的值对于Softmax没有影响
        attn = nn.Softmax(dim=-1)(scores)
        # attn: [batch_size, n_heads, len_q, len_k]
        # 对每一行进行Softmax计算
        context = torch.matmul(attn, V)
        # [batch_size, n_heads, len_q, d_v]
        return context, attn

# 这里要做的是，通过Q和K计算出scores，然后将scores和V相乘，得到每个单词的context vector。第一步是将Q和K的转置相乘，没什么好讲的，相乘之后得到的scores还不能立刻进行softmax，需要和attn_mask相加，把一些需要屏蔽的信息屏蔽掉，attn_mask是一个仅由True和False组成的tensor，并且一定会让attn_mask和scores的维度4个值相同（不然无法对应位置相加）。
# 掩码操作完成之后，就可以对Scores进行Softmax计算了。然后与V相乘，得到context。


class MultiHeadAttention(nn.Module):
    def __init__(self):
        super(MultiHeadAttention, self).__init__()
        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)
        # 3个矩阵，分别对输入进行3次线性变化
        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)
        # 变换维度

    def forward(self, input_Q, input_K, input_V, attn_mask):
        """
        input_Q: [batch_size, len_q, d_model]
        input_K: [batch_size, len_k, d_model]
        input_V: [batch_size, len_v(=len_k), d_model]
        attn_mask: [batch_size, seq_len, seq_len]
        """
        residual, batch_size = input_Q, input_Q.size(0)
        # [batch_size, len_q, d_model]
        # (W)-> [batch_size, len_q,d_k * n_heads]
        # (view)->[batch_size, len_q,n_heads,d_k]
        # (transpose)-> [batch_size,n_heads,len_q,d_k ]
        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)
        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)
        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)
        # 生成Q、K、V矩阵

        attn_mask = attn_mask.unsqueeze(1)
        # attn_mask : [batch_size,n_heads, seq_len, seq_len]

        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)
        # context: [batch_size, n_heads, len_q, d_v],
        # attn: [batch_size, n_heads, len_q, len_k]
        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v)
        # context: [batch_size, len_q, n_heads * d_v]
        output = self.fc(context)
        # [batch_size, len_q, d_model]
        return nn.LayerNorm(d_model).cuda()(output + residual), attn


# 完整代码中，一定会有三处调用MultiHeadAttention()，Encoder Layer调用一次，传入的input_Q、input_K、input_V全部都是enc_inputs；Decoder Layer中调用两次，第一次都是decoder_inputs，第二次是两个encoder_outputs和一个decoder_input。

class PoswiseFeedForwardNet(nn.Module):
    def __init__(self):
        super(PoswiseFeedForwardNet, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_ff, bias=False),
            nn.ReLU(),
            nn.Linear(d_ff, d_model, bias=False)
        )

    def forward(self, inputs):
        """
        inputs: [batch_size, seq_len, d_model]
        """
        residual = inputs
        output = self.fc(inputs)
        return nn.LayerNorm(d_model).cuda()(output + residual)  #[batch_size, seq_len, d_model]

# 这段代码非常简单，就是做两次线性变换，残差连接后再跟一个Layer Norm

class EncoderLayer(nn.Module):
    def __init__(self):
        super(EncoderLayer, self).__init__()
        self.enc_self_attn = MultiHeadAttention()
        # 多头注意力机制
        self.pos_ffn = PoswiseFeedForwardNet()
        # 提取特征

    def forward(self, enc_inputs, enc_self_attn_mask):
        """
        enc_inputs: [batch_size, src_len, d_model]
        enc_self_attn_mask: [batch_size, src_len, src_len]
        """

        # enc_outputs: [batch_size, src_len, d_model],
        # attn: [batch_size, n_heads, src_len, src_len] 每一个投一个注意力矩阵
        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)
        # 乘以WQ、WK、WV生成QKV矩阵（由于此时是自注意力模型，因此传入的数据是相同内容。）
        # 但在decoder-encoder的mulit-head中，
        # 由于此时是交互注意力，因此需要传入的是解码器输入向量与编码器输出向量。
        # 为了使用方便，我们在定义enc_self_attn函数的时候就定义的是有3个形参的

        enc_outputs = self.pos_ffn(enc_outputs)
        # enc_outputs: [batch_size, src_len, d_model]
        # 输入和输出的维度是一样的
        return enc_outputs, attn

# 将上述组件拼起来，就是一个完整的Encoder Layer

class DecoderLayer(nn.Module):
    def __init__(self):
        super(DecoderLayer, self).__init__()
        self.dec_self_attn = MultiHeadAttention()
        self.dec_enc_attn = MultiHeadAttention()
        self.pos_ffn = PoswiseFeedForwardNet()

    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):
        """
        dec_inputs: [batch_size, tgt_len, d_model]
        enc_outputs: [batch_size, src_len, d_model]
        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]
        dec_enc_attn_mask: [batch_size, tgt_len, src_len]
        """

        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]
        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)
        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]
        # 先是decoder的self-attention

        # print(dec_outputs.shape)
        # print(enc_outputs.shape)
        # print(dec_enc_attn_mask.shape)

        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)
        # 再是encoder-decoder attention部分

        dec_outputs = self.pos_ffn(dec_outputs)  # [batch_size, tgt_len, d_model]
        # 特征提取

        return dec_outputs, dec_self_attn, dec_enc_attn

# 在Decoder Layer中会调用两次MultiHeadAttention，第一次是计算Decoder Input的self-attention，得到输出dec_outputs
# 然后将dec_outputs作为生成Q的元素，enc_outputs作为生成K和V的元素，再调用一次MultiHeadAttention，得到的是Encoder和Decoder Layer之间的context vector。最后将dec_outputs做一次维度变换，最后返回最终的解码器输出结果

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.src_emb = nn.Embedding(src_vocab_size, d_model)
        self.pos_emb = PositionalEncoding(d_model)
        # 计算位置向量

        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])
        # 将6个Encoder Layer组成一个module

    def forward(self, enc_inputs):
        """
        enc_inputs: [batch_size, src_len]
        """
        enc_outputs = self.src_emb(enc_inputs)
        # 对每个单词进行词向量计算
        # enc_outputs [batch_size, src_len, d_model]

        enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)
        # 添加位置编码
        # enc_outputs [batch_size, src_len, d_model]

        enc_self_attn_mask = creat_self_mask(enc_inputs, enc_inputs)
        # enc_self_attn: [batch_size, src_len, src_len]


# 计算得到输入到编码器注意力中的掩码矩阵
enc_self_attns = []
# 创建一个列表，保存接下来计算的attention score（query与key的相关性计算结果）
for layer in self.layers:
    # enc_outputs: [batch_size, src_len, d_model]
    # enc_self_attn: [batch_size, n_heads, src_len, src_len]
    enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)
    enc_self_attns.append(enc_self_attn)
    # 再传进来就不用positional encoding了
    # 记录下每一次的attention
return enc_outputs, enc_self_attns

# nn.ModuleList()中的参数是列表，列表里面存了n_layers个Encoder Layer
# 由于我们控制好了Encoder Layer的输入维度和输出维度相同，因此可以直接用for循环以嵌套的方式
# 将上一次Encoder Layer的输出作为下一次Encoder Layer的输入
# *********************************************
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)
        self.pos_emb = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])

    def forward(self, dec_inputs, enc_inputs, enc_outputs):
        """
        dec_inputs: [batch_size, tgt_len]
        enc_inputs: [batch_size, src_len]
        enc_outputs: [batch_size, src_len, d_model] 经过6次encoder之后得到的东西
        """
        dec_outputs = self.tgt_emb(dec_inputs)
        # [batch_size, tgt_len, d_model]
        # 同样地，对decoder_layer进行词向量的生成
        dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1)).transpose(0, 1).cuda()
        # 计算其位置向量
        # [batch_size, tgt_len, d_model]

        dec_self_attn_mask = creat_self_mask(dec_inputs, dec_inputs)
        # [batch_size, tgt_len, tgt_len]

        # dec_self_attn_subsequence_mask = create_look_ahead_mask(dec_inputs).cuda()
        # [batch_size, tgt_len, tgt_len]
        # 当前时刻看不到未来时刻的东西

        dec_enc_attn_mask = create_look_ahead_mask(enc_inputs,dec_inputs)
        # [batch_size, tgt_len, tgt_len]
        # 布尔 + int false 0 true 1, gt 大于 True
        # 这样把dec_self_attn_pad_mask和dec_self_attn_subsequence_mask中为True的部分都剔
        # 除掉了
        # 也就是说，屏蔽掉Padding，也屏蔽掉Mask
        # 在decoder的第二个attention中使用
        dec_self_attns, dec_enc_attns = [], []
        for layer in self.layers:
            # dec_outputs: [batch_size, tgt_len, d_model],
            # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len],
            # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]
            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs,
                                                             dec_self_attn_mask, dec_enc_attn_mask)
            dec_self_attns.append(dec_self_attn)
            dec_enc_attns.append(dec_enc_attn)
        return dec_outputs, dec_self_attns, dec_enc_attns

# *********************************************
class Transformer(nn.Module):
    def __init__(self):
        super(Transformer, self).__init__()
        self.encoder = Encoder().cuda()
        self.decoder = Decoder().cuda()
        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False).cuda()
        # 对decoder的输出转换维度
        # 从隐藏层维数 -> 英语单词词典大小（选取概率最大的那个作为我们的预测结果）

    def forward(self, enc_inputs, dec_inputs):
        """
        enc_inputs维度: [batch_size, src_len]
        对于encoder-input，一个batch中有几个sequence，一个sequence有几个字
        dec_inputs: [batch_size, tgt_len]
        对于decoder-input，一个batch中有几个sequence，一个sequence有几个字
        """
        # enc_outputs: [batch_size, src_len, d_model],
        # d_model是每一个字的Word Embedding长度
        """
        enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]
        注意力矩阵，对于encoder和decoder，每一层、每一句、每一个头、每两个字之间都有一个权重系数，
        这些权重系数组成了注意力矩阵(之后的dec_self_attns同理，当然decoder还有一个decoder-encoder矩阵)
        """
        enc_outputs, enc_self_attns = self.encoder(enc_inputs)

        """
        dec_outputs: [batch_size, tgt_len, d_model],
        dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len],
        dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]
        """
        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs,
                                                                  enc_outputs)

        dec_logits = self.projection(dec_outputs)
        # 将输出的维度从[batch_size, tgt_len, d_model]变成[batch_size, tgt_len, tgt_vocab_size]
        # dec_logits: [batch_size, tgt_len, tgt_vocab_size]

        return dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns

# dec_logits的维度是[batch_size * tgt_len, tgt_vocab_size]，可以理解为
# 这个句子有batch_size*tgt_len个单词，每个单词有tgt_vocab_size种情况，取概率最大者
# transformer主要就是调用Encoder和Decoder。最后返回
# *********************************************
save_path = "./saver/transformer.pt"
device = "cuda"
model = Transformer()
model.to(device)
# model.load_state_dict(torch.load(save_path))
criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = optim.AdamW(model.parameters(), lr=2e-5)
# *********************************************
for epoch in range(1024):
    pbar = tqdm(loader, total=len(loader))  # 显示进度条
    for enc_inputs, dec_inputs, dec_outputs in pbar:
        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), \
                                              dec_inputs.to(device), dec_outputs.to(device)
        # outputs: [batch_size * tgt_len, tgt_vocab_size]
        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)
        loss = criterion(outputs, dec_outputs.view(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        pbar.set_description(f"epoch {epoch + 1} : train loss {loss.item():.6f} ")  # : learn_rate {lr_scheduler.get_last_lr()[0]:.6f}

    torch.save(model.state_dict(), save_path)

idx2word = {i: w for i, w in enumerate(tgt_vocab)}
enc_inputs, dec_inputs, dec_outputs = next(iter(loader))
predict, e_attn, d2_attn = model(enc_inputs[0].view(1, -1).cuda(), dec_inputs[0].view(1, -1).cuda())
predict = predict.data.max(1, keepdim=True)[1]
predict = [idx2word[n.item()] for n in predict.squeeze()]
print(enc_inputs[0], '->', predict)
```
以上代码就是transformer的结构代码，实际上就是综合前面所学的全部知识，结合编码器和解码器。读者可以使用以下程序对代码进行测试。
```
if __name__ == "__main__":
    encoder_input = tf.keras.Input(shape=(None,))
    decoder_input = tf.keras.Input(shape=(None,))

    output = Transformer(1024,1024)([encoder_input,decoder_input])
    model = tf.keras.Model([encoder_input,decoder_input],output)
    print(model.summary())
```
打印结果请读者自行验证。

### 12.2.3 拼音汉字模型的训练
本小节进行transformer的训练。需要注意的是，相对于第11章的学习，transformer的训练过程需要特别注意编码器的输出和解码器输入的错位计算。
- 第1次输入：编码器输入完整的序列[GO]ni hao ma[END]。与此同时，解码器的输入端输入的是解码开始符GO，经过交互计算后，解码器的输出为“你” 。
- 第2次输入：编码器输入完整的序列[GO]ni hao ma[END]。与此同时，解码器的输入端输入的是解码开始符GO和字符“你” ，经过交互计算后，解码器的输出为“你好” 。

这样依次进行输出。

然后依次进行错位输入。

最后一次输入：编码器输入的还是完整序列，此时在解码器的输出端会输出带有结束符的序列，表明解码结束。

- 第1次输入：
    - 编码器输入: [GO]ni hao ma[END]
    - 解码器输入: [GO]
    - 解码器输出: 你
- 第2次输入：
    - 编码器输入: [GO]ni hao ma[END]
    - 解码器输入: [GO]你
    - 解码器输出: 你 好
- 第3次输入：
    - 编码器输入: [GO]ni hao ma[END]
    - 解码器输入: [GO]你 好
    - 解码器输出: 你 好 吗
- 最后一次输入：
    - 编码器输入: [GO]ni hao ma[END]
    - 解码器输入: [GO]你 好 吗
    - 解码器输出: 你 好 吗 [END]

计算步骤如图12 - 10所示。

![image](https://github.com/user-attachments/assets/7d61537c-ba4d-4ac4-8cab-dc70fe0911fe)

如编码器读取数据一样，由于硬件设备的原因，需要使用数据生成器循环生成数据，并且在生成器当中进行错位输入，具体请读者自行完成

### 12.2.4 拼音汉字模型的使用

相信读者一定发现了，相对于拼音汉字转换模型，拼音汉字翻译模型并不是整体一次性输出的，而是根据在编码器中的输入内容生成特定的输出内容。

根据这个特性，如果想获取完整的解码器生成的数据内容，则需要采用循环输入的方式完成模型的使用，代码如下。

```python
idx2pinyin = {i: w for i, w in enumerate(tgt_vocab)}
idx2hanzi = {i: w for i, w in enumerate(src_vocab)}

context = "你好吗"
token = [src_vocab[char] for char in context]
token = torch.tensor(token)
sentence_tensor = torch.LongTensor(token).unsqueeze(0).to(device)
outputs = []
for i in range(tgt_len):
    trg_tensor = torch.LongTensor(outputs).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(sentence_tensor, trg_tensor)
    best_guess = torch.argmax(output,dim=-1).detach().cpu()
    outputs.append(best_guess[-1])

    if best_guess[-1] == 2:
        break
print([idx2pinyin[id.item()] for id in outputs[1:]])
```

以上代码演示了循环输出预测结果，这里使用了一个for循环对预测进行输入，具体请读者自行验证。

### 12.3 本章小结
首先回答12.2节提出的两个问题。
- **（1）编码器 - 解码器的翻译模型与编码器的转换模型有什么区别？**
答：对于转换模型来说，模型在工作时不需要进行处理，默认所有的信息都包含在编码器编码的词嵌入中，最后直接进行Softmax计算即可。而编码器 - 解码器的翻译模型需要综合编码器的编码内容和解码器的原始输入共同完成后续的交互计算。
- **（2）如果想做汉字→拼音的翻译系统，编码器和解码器的输入端分别输入什么内容？**
答：编码器的输入端是汉字，解码器的输入端是错位的拼音。 

本章和第10章是相互衔接的，主要介绍了当前非常流行的transformer深度学习模型，从其架构入手详细介绍其主要架构部分：编码器和解码器，并且还介绍了各种ticks和小细节，有针对性地对模型优化做了说明。对于解决自然语言处理问题，目前transformer架构是最重要的方法。读者在学习这两章的时候一定要多次阅读，尽量掌握全部内容。 
