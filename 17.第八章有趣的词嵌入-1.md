### 第8章 有趣的词嵌入
词嵌入（Word Embedding）是什么？为什么要词嵌入？在深入了解这个概念之前，先看几个例子：
- 在购买商品或者入住酒店后，会邀请顾客填写相关的评价表来表明对服务的满意程度。
- 使用几个词在搜索引擎上搜索一下。
- 有些博客网站会在博客下面标记一些相关的tag标签。

那么问题来了，这些是怎么做到的呢？
实际上这是文本处理后的应用，目的是用这些文本进行情绪分析、同义词聚类、文章分类和打标签。

读者在阅读文章或者评论服务的时候，可以准确地说出这个文章大致讲了什么、评论的倾向如何，但是计算机是怎么做的呢？计算机可以匹配字符串，然后告诉你是否与所输入的字符串相同，但是我们怎么能让计算机在你搜索梅西的时候，告诉你有关足球或者皮耶罗的事情？

词嵌入由此诞生，它就是对文本的数字表示。通过其表示和计算可以使得计算机很容易得到如下公式：
梅西 - 阿根廷 + 巴西 = 内马尔

本章将着重介绍词嵌入的相关内容，首先通过多种计算词嵌入的方式循序渐进地讲解如何获取对应的词嵌入，然后使用词嵌入进行文本分类。

#### 8.1 文本数据处理
无论是使用深度学习还是传统的自然语言处理方式，一个非常重要的内容就是将自然语言转换成计算机可以识别的特征向量。文本的预处理就是如此，通过文本分词、词向量训练、特征词抽取这3个主要步骤组建能够代表文本内容的矩阵向量。

##### 8.1.1 Ag_news数据集介绍和数据清洗
新闻分类数据集AG是由学术社区ComeToMyHead提供的，该数据集包括从2000多个不同的新闻来源搜集的超过100万篇新闻文章 ，用于研究分类、聚类、信息获取等非商业活动。在AG语料库的基础上，Xiang Zhang为了研究需要，从中提取了127600样本作为Ag_news数据集，其中抽出120000作为训练集，而7600作为测试集。分为以下4类：
- World
- Sports
- Business
- Sci/Tec

Ag_news数据集使用CSV文件格式存储。

![image](https://github.com/user-attachments/assets/7b33719f-1a58-4ba6-8e8b-78971f9d39ed)


第1列是新闻分类，第2列是新闻标题，第3列是新闻的正文部分，使用“,”和“.”作为断句的符号。

由于获取的数据集是由社区自动化存储和收集的，无可避免地存有大量的数据杂质：
- Reuters - Was absenteeism a little high\on Tuesday among the guys at the office? EA Sports would like\to think it was because "Madden NFL 2005" came out that day,\and some fans of the football simulation are rabid enough\to take a sick day to play it.
- Reuters - A group of technology companies\(including Texas Instruments Inc. (TXN.N), STMicroelectronics (STM.PA) and Broadcom Corp. (BRCM.O), on Thursday said they will propose a new wireless networking standard up to 10 times\the speed of the current generation.


因此，第一步是对数据进行清洗，步骤如下。

1. **数据的读取与存储**

数据集的存储格式为CSV，需要按列队数据进行读取，代码如下。

```python
import csv
agnews_train = csv.reader(open('./dataset/train.csv',"r"))
for line in agnews_train:
    print(line)
```
输出结果（示例）：
```
['2', 'Sharapova wins in fine style', 'Maria Sharapova and Amelie Mauresmo opened their challenges at the WTA Champions...']
['2', 'Leeds deny Sainsbury deal elite', 'Leeds chairman Gerald Krasner has laughed off suggestions that he is...']
['2', 'Washington - bound Expos Hire Ticket Agency', 'It is doubtful whether Alex McLeish will be given time to oversee...']
['2', 'NHL #39;s losses not as bad as they say: Forbes mag', 'WASHINGTON Nov 9 - The Expos cleared another step to Devil...']
['1', 'Resistance Rages as Lift Pressure Off Fallujah', 'RAMADI, November 12 (IslamOnline.net - amp; News Agencies) -...']
```
读取的train中的每行数据内容被默认以逗号分隔、按列依次存储在序列不同的位置中。为了分类方便，可以使用不同的数组将数据按类别进行存储。当然，也可以根据需要使用Pandas，但是为了后续操作和运算速度，这里主要使用Python原生函数和NumPy进行计算。

![image](https://github.com/user-attachments/assets/3440657f-4d59-499e-a807-f068485a6667)


```python
import csv
agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('./dataset/train.csv',"r"))
for line in agnews_train:
    agnews_label.append(line[0])
    agnews_title.append(line[1].lower())
    agnews_text.append(line[2].lower())
```

可以看到，不同的内容被存储在不同的数组中，并且为了统一执行，将所有的字母转换成小写以便于后续的计算。

2. **文本的清洗**

文本中除了常用的标点符号外，还包含着大量的特殊字符，因此需要对文本进行清洗。

文本清洗的方法一般是使用正则表达式，可以匹配小写`a-z`、大写`A-Z`或者数字`0-9`之外的所有字符，并用空格代替，这个方法无须指定所有标点符号，代码如下：

```python
import re
text = re.sub(r"[^a-zA-Z0-9]", " ",text)
```
这里`re`是Python中对应正则表达式的Python包，字符串“^”的意义是求反，即只保留要求的字符，而替换不要求保留的字符。进一步分析可以知道，文本清洗中除了将不需要的符号使用空格替换外，还会产生一个问题，即空格数目过多或在文本的首尾有空格残留，同样会影响文本的读取，因此还需要对替换符号后的文本进行二次处理。
```python
import re
def text_clear(text):
    text = text.lower()  # 将文本转换成小写
    text = re.sub(r"[^a-z0-9]", " ", text)  # 替换非标准字符，“^”是求反操作
    text = re.sub(r"  +", " ", text)  # 替换多重空格
    text = text.strip()  # 取出首尾空格
    text = text.split(" ")
    return text
```
由于加载了新的数据清洗工具，因此在读取数据时可以使用自定义的函数将文本信息处理后存储，代码如下：
```python
import csv
import tools
import numpy as np
agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('./dataset/train.csv',"r"))
for line in agnews_train:
    agnews_label.append(np.float32(line[0]))
    agnews_title.append(tools.text_clear(line[1]))
    agnews_text.append(tools.text_clear(line[2]))
```
这里使用了额外的包和NumPy函数对数据进行处理，因此可以获得处理后较为干净的数据。

![image](https://github.com/user-attachments/assets/251ba806-2aa6-4db6-a52a-74c625089444)


##### 8.1.2 停用词的使用
观察分好词的文本集，每组文本中除了能够表达含义的名词和动词外，还有大量没有意义的副词，例如is、are、the等。这些词的存在并不会给句子增加太多含义，反而由于频率非常高，会影响后续的词向量分析。因此，为了减少我们要处理的词汇量，降低后续程序的复杂度，需要清除停用词。清除停用词一般用的是NLTK工具包。安装代码如下：
```bash
conda install nltk
```
然后，只是安装NLTK并不能够使用停用词，还需要额外下载NLTK停用词包，建议读者通过控制端进入NLTK，之后运行如下代码，打开NLTK下载控制台。


![image](https://github.com/user-attachments/assets/b19fcf1c-3a0f-416d-a21a-64e941a3523c)


```python
import nltk
nltk.download()
```

![image](https://github.com/user-attachments/assets/929cddb4-63e2-42ae-a8d9-f8f96a14f412)


在Corpora标签下选择stopwords，单击Download按钮下载数据。下载后验证方法如下：
```python
stoplist = stopwords.words('english')
print(stoplist)
```
`stoplist`将停用词获取到一个数组列表中。

![image](https://github.com/user-attachments/assets/dcbbe6b9-93f2-4b51-908d-721163e8d0b4)


下面将停用词数据加载到文本清洁器中，除此之外，由于英文文本的特殊性，单词会具有不同的变化和变形，例如后缀`ing`和`ed`丢弃、`ies`用`y`替换等。这样可能会变成不是完整词的词干，但是只要这个词的所有形式都还原成同一个词干即可。NLTK中对这部分词根还原的处理使用的函数如下：

```python
PorterStemmer().stem(word)
```

整体代码如下：

```python
def text_clear(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9]", " ", text)  # 将文本转化成小写
    text = re.sub(r"  +", " ", text)  # 替换非标准字符，“^”是求反操作
    text = text.strip()  # 替换多重空格
    text = text.split(" ")  # 取出首尾空格
    text = [word for word in text if word not in stoplist]  # 去除停用词
    text = [PorterStemmer().stem(word) for word in text]  # 还原词干部分
    text.append('eos')  # 添加结束符
    text = ["bos"] + text  # 添加开始符
    return text
```

![image](https://github.com/user-attachments/assets/75e1f0ba-b57d-4740-a253-b6b33d596f3c)


这样生成的最终结果（示例）：
```
['bos', 'leagued', 'ruter', 'daily','sugges', 'dodg', 'bullet', 'bombing', 'enough','many', 'kneel', 'face', 'freez', 'abuj', 'ruter', 'african', 'union','sudan','start', 'withdraw', 'peopl', 'belie', 'peop', 'civili', 'intens', 'pressure', 'guy', 'juda','start', 'withdraw', 'krachi', 'ruter', 'pakistan', 'president', 'pervez','musharraf', 'pull','security', 'force', 'three', 'peopl', 'die', 'ruter', 'united','state', 'enviroment', 'protect', 'agency', 'army', 'chief','renew', 'pled', 'eos']
```
可以看到，相对于未处理过的文本，获取的是相对干净的文本数据。下面对文本的清洁处理步骤做个总结。
- **Tokenization**：对句子进行拆分，以单个词或者字符的形式进行存储，文本清洁函数中的text.split函数执行的就是这个操作。
- **Normalization**：将词语正则化，lower函数和PorterStemmer函数做了此方面的工作，用于将数据转为小写和还原词干。 
- **Rare Word Replacement**：对稀疏性较低的词进行替换，一般将词频小于5的替换成一个特殊的Token`<UNK>`。通常把Rare Word视为噪声，故此法可以降噪并减小字典的大小。 
- **Add <BOS> <EOS>**：添加每个句子的开始和结束标识符。 
- **Long Sentence Cut-Off or Short Sentence Padding**：对于过长的句子进行截取，对于过短的句子进行补全。

由于模型的需要，在处理的时候并没有完整地执行以上步骤。在不同的项目中，读者可以自行斟酌使用。

##### 8.1.3 词向量训练模型Word2Vec使用介绍
Word2Vec是Google在2013年推出的一个NLP（Natural Language Processing，自然语言处理）工具，它的特点是将所有的词向量化，这样词与词之间就可以定量地度量它们之间的关系，挖掘词之间的联系。

![image](https://github.com/user-attachments/assets/376db02f-2837-4111-889a-836a0c0a3a0b)



用词向量来表示词并不是Word2Vec的首创，在很久以前就出现了。最早的词向量是很冗长的，它使用词向量维度大小为整个词汇表的大小，对于每个具体的词汇表中的词，将对应的位置置为1。
例如5个词组成的词汇表，词"Queen"的序号为2，那么它的词向量就是(0,1,0,0,0)(0,1,0,0,0)，同样的道理，词"Woman"的词向量就是(0,0,0,1,0)(0,0,0,1,0)。这种词向量的编码方式一般叫作One-of-N Representation或者One-Hot。

One-Hot用来表示词向量非常简单，但是却有很多问题。最大的问题是词汇表一般都非常大，比如达到百万级别，这样每个词都用百万维的向量来表示基本是不可能的。而且这样的向量除了一个位置是1，其余的位置全部是0，表达的效率不高。将其使用在卷积神经网络中可以使得网络难以收敛。

Word2Vec是一种可以解决One-Hot的方法，它的思路是通过训练将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学方法来研究词与词之间的关系。

Word2Vec具体的训练方法主要有两部分：CBOW模型和Skip-Gram模型。
（1）**CBOW模型**：CBOW（Continuous Bag-Of-Word，连续词袋）模型是一个三层神经网络。该模型的特点是输入已知的上下文，输出对当前单词的预测。

![image](https://github.com/user-attachments/assets/ee278ae6-7ed3-4539-817f-5c59a0494bdd)


（2）**Skip-Gram模型**：Skip-Gram模型与CBOW模型正好相反，由当前词预测上下文词。

Word2Vec更为细节的训练模型和训练方式这里不做讨论。本小节将主要介绍训练一个可以获得词向量的模型训练提出了很多方法，最为简单的是使用Python工具包中的Gensim包对数据进行训练。

1. **训练Word2Vec模型**

对词模型进行训练的代码非常简单：

```python
from gensim.models import Word2Vec
model = Word2Vec(agnews_text,size=64, min_count = 0,window = 5)  # 设置训练参数
model_name = "corpusWord2Vec.bin"  # 模型存储名
model.save(model_name)  # 将训练好的模型存储
```
首先在代码中导入Gensim包，之后使用Word2Vec函数根据设定的参数对Word2Vec模型进行训练，这里解释一下主要参数。Word2Vec函数的主要参数如下：
```python
Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling,iter = 5)
```

其中，sentences是输入数据，worker是并行运行的线程数，size是词向量的维数，min_count是最小的词频，window是上下文窗口大小，sample是对频繁词汇下采样设置，iter是循环的次数。一般不是有特殊要求，按默认值设置即可。

save函数用于将生成的模型进行存储供后续使用。

2. **Word2Vec模型的使用**

模型的使用非常简单，代码如下：

```python
text = "Prediction Unit Helps Forecast Wildfires"
text = tools.text_clear(text)
print(model[text].shape)
```
其中text是需要转换的文本，同样调用text_clear函数对文本进行清理。之后使用已训练好的模型对文本进行转换。转换后的文本内容如下：
```
['bos', 'predict', 'unit', 'help', 'forecast', 'wildfir', 'eos']
```
计算后的Word2Vec文本向量实际上是一个[764]大小的矩阵。

![image](https://github.com/user-attachments/assets/3253c3d7-8ade-4893-bb03-94f121e21d97)


3. **对已有模型补充训练**

模型训练完毕后，可以对其存储，但是随着要训练的文档的增加，Gensim同样提供了持续性训练模型的方法，代码如下：

```python
from gensim.models import word2vec  # 导入Gensim包
model = word2vec.Word2Vec.load('./corpusWord2Vec.bin')  # 载入存储的模型
model.train(agnews_title, epochs=model.epochs, total_examples=model.corpus_count)  # 继续模型训练
```
可以看到，Word2Vec提供了加载存储模型的函数。之后train函数继续对模型进行训练，可以看到在最初的训练集中，agnews_text作为初始的训练文档，而agnews_title是后续训练部分，这样合在一起可以作为更多的训练文件进行训练。完整代码如下：
```python
import csv
import tools
import numpy as np
agnews_label = []
agnews_title = []
agnews_text = []
agnews_train = csv.reader(open('./dataset/train.csv',"r"))
for line in agnews_train:
    agnews_label.append(np.float32(line[0]))
    agnews_title.append(tools.text_clear(line[1]))
    agnews_text.append(tools.text_clear(line[2]))

print("开始训练模型")
from gensim.models import word2vec
model = word2vec.Word2Vec(agnews_text,size=64, min_count = 0,window = 5,iter=128)
model_name = "corpusWord2Vec.bin"
model.save(model_name)
from gensim.models import word2vec
model = word2vec.Word2Vec.load('./corpusWord2Vec.bin')
model.train(agnews_title, epochs=model.epochs, total_examples=model.corpus_count)
```
模型的使用在前面已经做了介绍，请读者自行完成。

对于需要训练的数据集和需要测试的数据集，建议读者在使用的时候一起进行训练，这样才能够获得最好的语义标注。在现实工程中，对数据的训练往往都有着极大的训练样本，文本容量能够达到几十甚至上百吉字节（GB）的数据，因而不会产生词语缺失的问题，所以在实际工程中只需要在训练集上对文本进行训练即可。

##### 8.1.4 文本主题的提取：基于TF-IDF
使用卷积神经网络对文本进行分类，文本主题的提取并不是必需的。

