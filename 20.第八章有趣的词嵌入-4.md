字符转换矩阵示意图
然后将字符串按字符表中的顺序转换成数字序列，代码如下：
```python
def get_char_list(string):
    alphabet_title = "abcdefghijklmnopqrstuvwxyz"
    char_list = []
    for char in string:
        num = alphabet_title.index(char)
        char_list.append(num)
    return char_list
```

这样生成的结果如下：

hello -> [7, 4, 11, 11, 14]

将代码整合在一起，最终结果如下：

```python
def get_one_hot(list,alphabet_title = None):  #设置字符集
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz"
    else:alphabet_title = alphabet_title
    values = np.array(list)  #获取字符数列
    n_values = len(alphabet_title) + 1  #获取字符表长度
    return np.eye(n_values)[values]
def get_char_list(string,alphabet_title = None):
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz"
    else:alphabet_title = alphabet_title
    char_list = []
    for char in string:  #获取字符串中的字符
        num = alphabet_title.index(char)  #获取对应位置
        char_list.append(num)  #组合位置编码
    return char_list
#主代码
def get_string_matrix(string):
    char_list = get_char_list(string)
    string_matrix = get_one_hot(char_list)
    return string_matrix
```
这样生成的结果如图8 - 27所示。
```
[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
```

![image](https://github.com/user-attachments/assets/9cae5496-8221-4390-92b7-2531103b8aea)



图8 - 27 转换字符串并进行One - Hot处理

可以看到，单词hello被转换成一个[5,26]大小的矩阵，供下一步处理。但是这里又产生一个新的问题，对于不同长度的字符串，组成的矩阵行长度不同。虽然卷积神经网络可以处理具有不同长度的字符串，但是在本例中还是以相同大小的矩阵作为数据输入进行计算。

#### 3. 生成文本矩阵的细节处理——矩阵补全

根据文本标题生成One - Hot矩阵，对于第2步中的矩阵生成的One - Hot矩阵函数，读者可以自行将其变更成类使用，这样能够在使用时更简便，同时我们也会将已完成的其他函数直接导入使用，这一点请读者注意。

```python
import csv
import numpy as np
import tools
agnews_title = []
agnews_train = csv.reader(open("./dataset/train.csv","r"))
for line in agnews_train:
    agnews_title.append(tools.text_clearTitle(line[1]))
for title in agnews_title:
    string_matrix = tools.get_string_matrix(title)
    print(string_matrix.shape)
```

补全后的矩阵维度，打印结果如下：

(51, 28)

(59, 28)

(44, 28)

(47, 28)

(51, 28)

(91, 28)

(54, 28)

(42, 28)

可以看到，生成的文本矩阵被整形成一个有一定大小规则的矩阵输出。但是这里出现了一个新的问题，对于不同长度的文本，单词和字母的多少并不是固定的，虽然对于全卷积神经网络来说，输入的数据维度可以不统一和固定，但是本部分还是对其进行处理。

对于不同长度的矩阵，一个简单的思路就是对其进行规范化处理，即长的截短，短的补长。

这里的思路也是如此，代码如下：

```python
def get_handle_string_matrix(string,n = 64):  #n为设定的长度，可以根据需要修正
    string_length= len(string)  #获取字符串长度
    if string_length > 64:  #判断是否大于64
        string = string[:64]  #长度大于64的字符串予以截短
        string_matrix = get_string_matrix(string) #获取文本矩阵
        return string_matrix
    else:  #对于长度不够的字符串
        string_matrix = get_string_matrix(string) #获取字符串矩阵
        handle_length = n - string_length  #获取需要补全的长度
        pad_matrix = np.zeros((handle_length,28)) #使用全0矩阵进行补全
        string_matrix = np.concatenate((string_matrix,pad_matrix),axis=0) #将字符矩阵和全0矩阵进行叠加，将全0矩阵叠加到字符矩阵后面
        return string_matrix
```

代码分成两部分，首先对不同长度的字符进行处理，对于长度大于64的字符串，只保留前64位字符，这个64是人为设定的截取或保留长度，可以根据需要对其进行修改。

而对于长度不到64的字符串，则需要对其进行补全，生成由余数构成的全0矩阵对生成矩阵进行处理。

经过修饰后的代码如下：

```python
import csv
import numpy as np
import tools
agnews_title = []
agnews_train = csv.reader(open("./dataset/train.csv","r"))
for line in agnews_train:
    agnews_title.append(tools.text_clearTitle(line[1]))
for title in agnews_title:
    string_matrix = tools.get_handle_string_matrix(title)
    print(string_matrix.shape)
```

标准化补全后的矩阵维度，打印结果如下：

(64, 28)

(64, 28)

(64, 28)

(64, 28)

(64, 28)

(64, 28)

(64, 28)

(64, 28)

#### 4. 标签的One - Hot矩阵构建

对于分类的表示，这里同样可以使用One - Hot方法对其分类进行重构，代码如下：

```python
def get_label_one_hot(list):
    values = np.array(list)
    n_values = np.max(values) + 1
    return np.eye(n_values)[values]
```

仿照文本的One - Hot函数，根据传进来的序列化参数对列表进行重构，形成一个新的One - Hot矩阵，从而能够反映出不同的类别。
#### 5. 数据集的构建

通过准备文本数据集，对文本进行清洗，去除不相干的词，提取主干，并根据需要设定矩阵维度和大小，全部代码如下（tools代码为上文的分布代码，在主代码后面）：

```python
import csv
import numpy as np
import tools
agnews_label = []  #空标签列表
agnews_title = []  #空文本标题文档
agnews_train = csv.reader(open("./dataset/train.csv","r"))  #读取数据集
for line in agnews_train:  #分行迭代文本数据
    agnews_label.append(np.int(line[0]))  #将标签读入标签列表
    agnews_title.append(tools.text_clearTitle(line[1]))  #将文本读入
train_dataset = []
for title in agnews_title:
    string_matrix = tools.get_handle_string_matrix(title)  #构建文本矩阵
    train_dataset.append(string_matrix)
train_dataset = np.array(train_dataset)  #将文本矩阵读取训练列表
label_dataset = tools.get_label_one_hot(agnews_label) #将label列表转换成One - Hot格式
```

这里首先通过CSV库获取全文本数据，之后逐行将文本和标签读入，分别将其转换成One - Hot矩阵后，利用NumPy库将对应的列表转换成NumPy格式。标准化转换后的AG_news结果如下：

(120000, 64, 28)

(120000, 5)

这里分别生成了训练集数量数据和标签数据的One - Hot矩阵列表，训练集的维度为[12000,64,28]，第一个数字是总的样本数，第2个和第3个数字分别为生成的矩阵维度。

标签数据为一个二维矩阵，12000是样本的总数，5是类别。这里读者可能会提出疑问，明明只有4个类别，为什么会出现5个？因为One - Hot是从0开始的，而标签的分类是从1开始的，所以会自动生成一个0的标签，这点请读者自行处理。全部tools函数如下，读者可以将其改成类的形式进行处理。

```python
import re
import csv
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
import numpy as np
#对英文文本进行数据清洗
#stoplist = stopwords.words('english')
def text_clear(text):
    text = text.lower()  #将文本转换成小写
    text = re.sub(r"[^a-z]", " ", text)  #替换非标准字符，^是求反操作
    text = re.sub(r" +", " ", text)  #替换多重空格
    text = text.strip()  #取出首尾空格
    text = text.split(" ")
    #text = [word for word in text if word not in stoplist]  #去除停用词
    text = [PorterStemmer().stem(word) for word in text]  #还原词干部分
    text.append("eos")  #添加结束符
    text = ["bos"] + text  #添加开始符
    return text
#对标题进行处理
def text_clearTitle(text):
    text = text.lower()  #将文本转换成小写
    text = re.sub(r"[^a-z]", " ", text)  #替换非标准字符，^是求反操作
    text = re.sub(r" +", " ", text)  #替换多重空格
    text = re.sub("  ", " ", text)  #替换隔断空格
    #text = text.split(" ")
    text = text.strip()  #取出首尾空格
    text = text + " eos"  #添加结束符
    return text
#生成标题的One - Hot标签
def get_label_one_hot(list):
    values = np.array(list)
    n_values = np.max(values) + 1
    return np.eye(n_values)[values]
#生成文本的One - Hot矩阵
def get_one_hot(list,alphabet_title = None):  #设置字符集
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz "
    else:alphabet_title = alphabet_title
    values = np.array(list)  #获取字符数列
    n_values = len(alphabet_title) + 1  #获取字符表长度
    return np.eye(n_values)[values]
#获取文本在词典中的位置列表
def get_char_list(string,alphabet_title = None):
    if alphabet_title == None:
        alphabet_title = "abcdefghijklmnopqrstuvwxyz "
    else:alphabet_title = alphabet_title
    char_list = []
    for char in string:  #获取字符串中的字符
        num = alphabet_title.index(char)  #获取对应位置
        char_list.append(num)  #组合位置编码
    return char_list
#生成文本矩阵
def get_string_matrix(string):
    char_list = get_char_list(string)
    string_matrix = get_one_hot(char_list)
    return string_matrix
#获取补全后的文本矩阵
def get_handle_string_matrix(string,n = 64):
    string_length= len(string)
    if string_length > 64:
        string = string[:64]
        string_matrix = get_string_matrix(string)
        return string_matrix
    else:
        string_matrix = get_string_matrix(string)
        handle_length = n - string_length
        pad_matrix = np.zeros((handle_length,28))
        string_matrix = np.concatenate((string_matrix,pad_matrix),axis=0)
        return string_matrix
#获取数据集
def get_dataset():
    agnews_label = []
    agnews_title = []
    agnews_train = csv.reader(open("./dataset/ag_news数据集/dataset/train.csv","r"))
    for line in agnews_train:
        agnews_label.append(np.int(line[0]))
        agnews_title.append(text_clearTitle(line[1]))
    train_dataset = []
    for title in agnews_title:
        string_matrix = get_handle_string_matrix(title)
        train_dataset.append(string_matrix)
    train_dataset = np.array(train_dataset)
    label_dataset = get_label_one_hot(agnews_label)
    return train_dataset,label_dataset
if __name__ == '__main__':
    get_dataset()
```
## 8.3.2 卷积神经网络文本分类模型的实现——Conv1d（一维卷积）

对文本的数据集部分处理完毕后，接下来需要设计基于卷积神经网络的分类模型。模型的构成包括多个部分，如图8 - 28所示。

![image](https://github.com/user-attachments/assets/a7220b77-c54b-4b69-b534-acbd729cbf9d)


图8 - 28 使用CNN处理字符文本分类

如图8 - 31所示的结构，作者根据类似的模型设计了一个由5层神经网络构成的文本分类模型，如表8 - 1所示。


表8 - 1 5层神经网络构成的文本分类模型

| 层数 | 层名 |
| ---- | ---- |
| 1 | Conv 3×3 1×1 |
| 2 | Conv 5×5 1×1 |
| 3 | Conv 3×3 1×1 |
| 4 | full_connect 512 |
| 5 | full_connect 5 |

这里使用的是5层神经网络，前3个基于一维的卷积神经网络，后两个全连接层用于分类任务，代码如下：

```python
import torch
import einops.layers.torch as elt
def char_CNN(input_dim = 28):
    model = torch.nn.Sequential(
        #第一层卷积
        elt.Rearrange("b l c -> b c l"),
        torch.nn.Conv1d(input_dim,32,kernel_size=3,padding=1),
        elt.Rearrange("b c l -> b l c"),
        torch.nn.ReLU(),
        torch.nn.LayerNorm(32),
        #第二层卷积
        elt.Rearrange("b l c -> b c l"),
        torch.nn.Conv1d(32, 28, kernel_size=3, padding=1),
        elt.Rearrange("b c l -> b l c"),
        torch.nn.ReLU(),
        torch.nn.LayerNorm(28),
        #flatten
        torch.nn.Flatten(),  #[batch_size,64 * 28]
        torch.nn.Linear(64 * 28,64),
        torch.nn.ReLU(),
        torch.nn.Linear(64,5),
        torch.nn.Softmax()
    )
    return model
if __name__ == '__main__':
    embedding = torch.rand(size=(5,64,28))
    model = char_CNN()
    print(model(embedding).shape)
```

这里是完整的训练模型，训练代码如下：
```python
import get_data
from sklearn.model_selection import train_test_split
train_dataset,label_dataset = get_data.get_dataset()
X_train,X_test, y_train, y_test = train_test_split(train_dataset,label_dataset,test_size=0.1, random_state=828) #将数据集划分为训练集和测试集
#获取device
device = "cuda" if torch.cuda.is_available() else "cpu"
model = char_CNN().to(device)
#定义交叉熵损失函数
def cross_entropy(pred, label):
    res = -torch.sum(label * torch.log(pred)) / label.shape[0]
    return torch.mean(res)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
batch_size = 128
train_num = len(X_train)
for epoch in range(99):
    train_loss = 0.
    for i in range(train_num):
        start = i * batch_size
        end = (i + 1) * batch_size
        x_batch = torch.tensor(X_train[start:end]).type(torch.float32).to(device)
        y_batch = torch.tensor(y_train[start:end]).type(torch.float32).to(device)
        pred = model(x_batch)
        loss = cross_entropy(pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()  # 记录每个批次的损失值
    # 计算并打印损失值
    train_loss /= train_num
    accuracy = (pred.argmax(1) == y_batch.argmax(1)).type(torch.float32).sum().item() / batch_size
    print("epoch: ",epoch,"train_loss:", round(train_loss,2),"accuracy:", round(accuracy,2))
```
首先获取完整的数据集，之后通过train_test_split函数对数据集进行划分，将数据分为训练集和测试集，而模型的计算和损失函数的优化与前面的PyTorch方法类似，这里就不过多阐述了。最终结果请读者自行完成。需要说明的是，这里的模型是一个较为简易的基于短文本分类的文本分类模型，8.4节将用另一种方式对这个模型进行修正。 
       
