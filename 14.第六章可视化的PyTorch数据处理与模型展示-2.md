### 6.1.3 批量输出数据的DataLoader类详解

本小节讲解torch.utils.data工具箱中最后一个工具，即用于批量输出数据的DataLoader类。

首先需要说明的是，DataLoader可以解决使用Dataset自定义封装的数据时无法对数据进行批量化处理的问题，其用法非常简单，只需要将其包装在使用Dataset封装好的数据集外即可，代码如下：

```python
mnist_dataset = MNIST_Dataset(transform=ToTensor())  #通过Dataset获取数据集
from torch.utils.data import DataLoader  #导入DataLoader
train_loader = DataLoader(mnist_dataset, batch_size=batch_size, shuffle=True)  #包装已封装好的数据集
```
事实上使用起来就是这么简单，我们对DataLoader的使用，首先导入对应的包，然后用其包装封装好的数据即可。DataLoader的定义如下：
```python
class DataLoader(object):
    __initialized = False
    def __init__(self, dataset, batch_size=1, shuffle=False, sampler=None,
                 batch_sampler=None, num_workers=0, collate_fn=None,
                 pin_memory=False, drop_last=False, timeout=0,
                 worker_init_fn=None, *, prefetch_factor=2,
                 pin_memory_device='', device=None):
        pass
    def __getattr__(self, name):
        pass
    def __iter__(self):
        pass
    def __len__(self):
        pass
```
与前面我们实现Dataset的不同之处在于：
- 我们一般不需要自己实现DataLoader的方法，只需要在构造函数中指定相应的参数即可，比如常见的batch_size、shuffle等参数。所以使用DataLoader十分简洁方便，都是通过指定构造函数的参数来实现。
- DataLoader实际上是一个较为高层的封装类，它的功能都是通过更底层的_DataLoader来完成的，这里就不再展开讲解了。DataLoaderIter就是_DataLoader的一个框架，用来传给_DataLoaderIter一堆参数，并把自己装进DataLoaderIter中。

对于DataLoader的使用现在只介绍这么多。基于PyTorch 2.0数据处理工具箱对数据进行识别和训练的完整代码如下：
```python
import numpy as np
import torch

#device = "cpu"  #PyTorch的特性，需要指定计算的硬件，如果没有GPU的存在，就使用CPU进行计算
device = "cuda"  #在这里默认使用GPU，如果出现运行问题，可以将其改成CPU模式

class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        inputs = np.reshape(inputs,[28*28])
        return torch.tensor(inputs), torch.tensor(targets)

class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self,transform = None):  #在定义时需要定义transform的参数
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")
        self.transform = transform  #需要显式地提供transform类

    def __getitem__(self, index):
        image = (self.x_train[index])
        label = (self.y_train_label[index])
        #通过判定transform类的存在对其进行调用
        if self.transform:
            image,label = self.transform(image,label)
        return image,label

    def __len__(self):
        return len(self.y_train_label)

import torch
import numpy as np

batch_size = 320  #设定每次训练的批次数
epochs = 42  #设定训练次数

mnist_dataset = MNIST_Dataset(transform=ToTensor())
from torch.utils.data import DataLoader
train_loader = DataLoader(mnist_dataset, batch_size=batch_size)

#设定的多层感知机网络模型
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(28*28, 312),
            torch.nn.ReLU(),
            torch.nn.Linear(312, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )

    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
model = model.to(device)  #将计算模型传入GPU硬件等待计算
torch.save(model, './model.pth')
model = torch.compile(model)  #PyTorch 2.0的特性，加速计算速度
loss_fu = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)  #设定优化函数

#开始计算
for epoch in range(epochs):
    train_loss = 0
    for image,label in (train_loader):
        train_image = image.to(device)
        train_label = label.to(device)

        pred = model(train_image)
        loss = loss_fu(pred,train_label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()  # 记录每个批次的损失值

    # 计算并打印损失值
    train_loss = train_loss/batch_size
    print("epoch: ", epoch, "train_loss:", round(train_loss, 2))
```
最终结果请读者自行打印完成。

### 6.2 实战：基于tensorboardX的训练可视化展示

6.1节带领读者完成了对于PyTorch 2.0中数据处理工具箱的使用，相信读者已经可以较好地对PyTorch 2.0的数据进行处理。本节对PyTorch 2.0进行数据可视化。

#### 6.2.1 可视化组件tensorboardX的简介与安装


前面介绍了Netron的安装与使用，这是一种可视化PyTorch模型的方法，其优点是操作简单，可视性强。但是随之而来的是，Netron组件对模型的展示效果并不是非常准确，只能大致地展示出模型的组件与结构。

tensorboardX就是专门为PyTorch 2.0进行模型展示与训练可视化的组件，可以记录模型训练过程的数字、图像等内容，以方便研究人员观察神经网络训练过程。

可以使用以下代码安装tensorboardX：

```
pip install tensorboardX
```
注意，这部分操作一定要在Anaconda或者Miniconda终端中进行，基于pip的安装和后续操作都是这样。
#### 6.2.2 tensorboardX可视化组件的使用
tensorboardX最重要的作用之一是对模型的展示，读者可以遵循以下步骤获得模型的展示效果。

1. **存储模型的计算过程**

首先使用tensorboardX模拟一次模型的运算过程，代码如下：

```python
#创建模型
model = NeuralNetwork()
# 模拟输入数据
input_data = (torch.rand(5, 784))
from tensorboardX import SummaryWriter
writer = SummaryWriter()
with writer:
    writer.add_graph(model,(input_data,))
```

可以看到，首先载入已设计好的模型，之后模拟输入数据，在载入tensorboardX并建立读写类之后，将模型带参数的运算过程加载到运行图中。

2. **查看默认位置的run文件夹**


运行第1步的代码后，程序会在当前平行目录下生成一个新的runs目录，这是存储和记录模型展示的文件夹。


![image](https://github.com/user-attachments/assets/c2c5dcaa-f318-4afb-b9b5-6c3f29752e4f)


可以看到，该文件夹是以日期的形式生成新目录的。

3. **使用Anaconda或者Miniconda终端打开对应的目录**

使用Anaconda或者Miniconda终端打开刚才生成的目录：

```
(base) C:\Users\xiaohua>cd C:\Users\xiaohua\Desktop\jupyter_book\src\第六章
```
此时需要注意的是，我们在这里打开的是runs文件夹的上一级目录，而不是runs文件夹本身。
之后调用tensorboardX对模型进行展示，读者需要在刚才打开的文件夹中执行以下命令：
```
tensorboard --logdir runs
```
执行结果如下：

![image](https://github.com/user-attachments/assets/cdd77e93-f219-498f-886e-cf932b79056f)


```
(base) C:\Users\xiaohua\Desktop\jupyter_book\src\第六章>tensorboard --logdir runs
WARNING:tensorflow:From C:\miniforge3\lib\site-packages\tensorboard\backend\application.py:190: The TensorBoard version you're using is 2.10.0, which is not compatible with TensorFlow 2.13.1. You may experience unexpected behavior or bugs. Please consider upgrading TensorBoard to a version compatible with TensorFlow 2.13.1.
WARNING:tensorflow:From C:\miniforge3\lib\site-packages\tensorboard\backend\application.py:192: The TensorBoard version you're using is 2.10.0, which is not compatible with TensorFlow 2.13.1. You may experience unexpected behavior or bugs. Please consider upgrading TensorBoard to a version compatible with TensorFlow 2.13.1.
Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all
TensorBoard 2.10.0 at http://localhost:6006/ (Press CTRL+C to quit)
```
可以看到，此时程序在执行，并提供了一个HTTP地址。至此，使用tensorboardX展示模型的步骤第一阶段完成。

4. **使用浏览器打开模型展示页面**

查看模型展示页面，在这里使用Windows自带的Edge浏览器，读者也可以尝试不同的浏览器，这里只需要在地址栏中输入http://localhost:6006即可进入tensorboardX的本地展示页面。


![image](https://github.com/user-attachments/assets/a5ae595f-d165-4934-a097-e907d94f8b2c)


可以看到，这是记录了模型的基本参数、输入输出以及基本模块的展示，之后读者可以双击模型主题部分，展开模型进行进一步的说明。更多操作建议读者自行尝试。


![image](https://github.com/user-attachments/assets/549fe9d4-6190-4eae-ad22-de18e75e6cf4)


#### 6.2.3 tensorboardX对模型训练过程的展示
模型结构的展示是很重要的内容，而有的读者还希望了解模型在训练过程中出现的一些问题和参数变化，tensorboardX同样提供了此功能，可以记录并展示模型在训练过程中损失值的变化，代码如下：
```python
from tensorboardX import SummaryWriter
writer = SummaryWriter()
#开始计算
for epoch in range(epochs):
    ...
    # 计算并打印损失值
    train_loss = train_loss/batch_size
    writer.add_scalars('evl', {'train_loss': train_loss}, epoch)
writer.close()
```
这里可以看到，使用tensorboardX对训练过程的参数记录非常简单，直接记录损失过程即可，而epoch作为横坐标标记也被记录。完整的代码如下（作者故意调整了损失函数学习率）：
```python
import torch
#device = "cpu"  #PyTorch的特性，需要指定计算的硬件，如果没有GPU的存在，就使用CPU进行计算
device = "cuda"  #在这里默认使用GPU，如果出现运行问题，可以将其改成CPU模式

class ToTensor:
    def __call__(self, inputs, targets):  #可调用对象
        inputs = np.reshape(inputs,[28*28])
        return torch.tensor(inputs), torch.tensor(targets)

class MNIST_Dataset(torch.utils.data.Dataset):
    def __init__(self,transform = None):  #在定义时需要定义transform的参数
        super(MNIST_Dataset, self).__init__()
        # 载入数据
        self.x_train = np.load("../dataset/mnist/x_train.npy")
        self.y_train_label = np.load("../dataset/mnist/y_train_label.npy")
        self.transform = transform  #需要显式地提供transform类

    def __getitem__(self, index):
        image = (self.x_train[index])
        label = (self.y_train_label[index])
        #通过判定transform类的存在对其进行调用
        if self.transform:
            image,label = self.transform(image,label)
        return image,label

    def __len__(self):
        return len(self.y_train_label)

import torch
import numpy as np

batch_size = 320  #设定每次训练的批次数
epochs = 320  #设定训练次数

mnist_dataset = MNIST_Dataset(transform=ToTensor())
from torch.utils.data import DataLoader
train_loader = DataLoader(mnist_dataset, batch_size=batch_size)

#设定的多层感知机网络模型
class NeuralNetwork(torch.nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = torch.nn.Flatten()
        self.linear_relu_stack = torch.nn.Sequential(
            torch.nn.Linear(28*28, 312),
            torch.nn.ReLU(),
            torch.nn.Linear(312, 256),
            torch.nn.ReLU(),
            torch.nn.Linear(256, 10)
        )

    def forward(self, input):
        x = self.flatten(input)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork()
model = model.to(device)  #将计算模型传入GPU硬件等待计算
model = torch.compile(model)  #PyTorch 2.0的特性，加速计算速度
loss_fu = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=2e-6)  #设定优化函数
from tensorboardX import SummaryWriter
writer = SummaryWriter()
#开始计算
for epoch in range(epochs):
    train_loss = 0
    for image,label in (train_loader):
        train_image = image.to(device)
        train_label = label.to(device)

        pred = model(train_image)
        loss = loss_fu(pred,train_label)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        train_loss += loss.item()  # 记录每个批次的损失值

    # 计算并打印损失值
    train_loss = train_loss/batch_size
    print("epoch: ", epoch, "train_loss:", round(train_loss, 2))
    writer.add_scalars('evl', {'train_loss': train_loss}, epoch)
writer.close()
```

![image](https://github.com/user-attachments/assets/9ef4ce58-edbc-40ed-99bd-ed2f9f530cba)


完成训练后，我们可以使用上一步的HTTP地址，此时单击TIME SERIES标签，对存储的模型变量进行验证。

### 6.3 本章小结
本章主要讲解了PyTorch 2.0数据处理与模型训练可视化方面的内容。本章介绍了数据处理的步骤，读者可能会有这样的印象，即PyTorch 2.0中的数据处理是依据一个个“管套”进行的。事实上也是这样，PyTorch 2.0通过管套模型对数据进行一步一步地加工，这是一种常用的设计模型，请读者注意。

![image](https://github.com/user-attachments/assets/dd591aec-99ab-4634-8e2e-5a1f648b95e7)


同时，本章还讲解了基于PyTorch 2.0原生的模型训练可视化组件tensorboardX的用法，除了对模型本身的展示外，tensorboardX更侧重对模型的训练过程的展示，记录了模型的损失值等信息，读者还可以进一步尝试加入对准确率的记录。 
