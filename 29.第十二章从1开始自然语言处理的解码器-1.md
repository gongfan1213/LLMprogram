
### 第12章 从1开始自然语言处理的解码器
本章从1开始。

第10章介绍了编码器的架构和实现代码。如果读者按本章内容的学习建议阅读了3遍以上，那么相信你对编码器的编写已经很熟悉了。

解码器是在编码器的基础上对模型进行少量修正，在不改变整体架构的基础上进行模型设计，可以说，如果读者掌握了编码器的原理，那么学习解码器的概念、设计和原理一定易如反掌。

本章首先介绍解码器的原理和程序编写，然后着重解决一个非常大的问题——文本对齐。这是自然语言处理中一个不可轻易逾越的障碍，本章将以翻译模型为例，系统地讲解文本对齐的方法，并实现一个基于汉字和拼音的“翻译系统”。本章是对第11章的继承，在阅读时，如果有读者想先完整地体验编码器 - 解码器系统，可以先查看12.1.4节，这是对解码器的完整实现，并详细学习12.2节的实战部分。待程序运行畅通之后，返回参考12.1节，重新学习解码器相关内容，加深印象。如果读者想了解更多细节，建议按本章讲解的顺序循序渐进地学习。



#### 12.1 解码器的核心——注意力模型

解码器在深度学习模型中具有非常重要的作用，即对传送过来的数据进行解码，生成具有特定格式的、内容可被理解的模型组件。解码器的结构如图12 - 1所示。

解码器的架构总体上与编码器类似，但还是有一部分区别，下面进行说明。

- 相对于编码器的单一输入（无论是叠加还是单独的词向量Embedding），解码器的输入有两部分，分别是编码器的输入和目标的Embedding输入。

- 相对于编码器中的多头注意力模型，解码器中的多头注意力模型分成两种，分别是多头自注意力层和多头交互注意力层。


总而言之，相对于编码器中的“单一模块”，解码器中更多的是“双模块”，即需要编码器的输入和解码器本身的输入协同处理。本节对这些内容进行详细介绍。

![image](https://github.com/user-attachments/assets/e8407070-d31a-4064-bd80-f2f3d21de729)


##### 12.1.1 解码器的输入和交互注意力层的掩码
如果换一种编码器和解码器的表示方法，如图12 - 2所示，可以清楚地看到，经过多层编码器的输出被输入多层解码器中。但是需要注意的是，编码器的输出对于解码器来说并不是直接使用，而是解码器本身先进行一次自注意力编码。下面就这两部分进一步说明。

![image](https://github.com/user-attachments/assets/3768c658-4c4b-430d-a4af-5268d6e5b827)


### 1. 解码器的词嵌入输入

与编码器的词嵌入输入方式一样，解码器本身的词嵌入处理也是由初始化的词向量和位置向量构成的，结构如图12 - 3所示。

### 2. 解码器的自注意力层（重点学习掩码的构建）
解码器的自注意力层是对输入的词嵌入进行编码的部分，这里的构造与编码器中的构造相同，不再过多阐述。
相对于编码器的掩码部分，解码器的掩码操作有其特殊的要求。
事实上，解码器的输入和编码器在处理上不太一样，一般认为编码器的输入是一个完整的序列，而解码器在训练和数据的生成过程中是逐个进行Token的生成的。因此，为了防止“偷看”，解码器的自注意力层只能够关注输入序列当前位置以及之前的字，不能够关注之后的字。因此，需要将当前输入的字符Token之后的内容都进行掩码（mask）处理，使其在经过Softmax计算之后的权重变为0，不参与后续模型损失函数的计算，从而强制使得模型仅仅依靠之前输入的序列内容生成后续的“下一个字符序号”。代码如下：
```python
def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask
```
如果单独打印代码如下：
```python
mask = create_look_ahead_mask(4)
print(mask)
```
这里的参数size设置成4，则打印结果如图12 - 4所示。
```
tf.Tensor(
[[0. 1. 1. 1.]
 [0. 0. 1. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)
```
可以看到，函数的实际作用是生成一个三角掩码，对输入的值生成逐行递增的梯度序列，这样可以保证数据在输入模型的过程中，数据的接收也是依次增加的，当前的Token只与其本身和其前面的Token进行注意力计算，而不会与后续的Token进行注意力计算。这段内容的图形化效果如图12 - 5所示。

此外，对于解码器自注意力层的输入，即Query、Key、Value的定义和设定，在解码器的自注意力层的输入都是由叠加后的词嵌入输入的，因此与编码器类似，可以将其设置成同一个。

### 3. 解码器和编码器的交互注意力层（重点学习Query、Key和Value的定义）
编码器和解码器处理后的数据需要“交融”，从而进行新的数据整合和生成，而进行数据整合和生成的架构和模块在本例中所处的位置是交互注意力层。
编码器中的交互注意力层的架构和同处于编码器中的自注意力层没有太大的差别，其差距主要是输入的不同以及使用掩码对数据的处理不同。下面分别进行阐述。

#### 1）交互注意力层
交互注意力层的作用是将编码器输送的“全部”词嵌入与解码器获取的“当前”的词嵌入进行“融合”计算，使得当前的词嵌入“对齐”编码器中对应的信息，从而获取解码后的信息。
下面从解码器的角度进行讲解，如图12 - 6所示。
从图12 - 6可以看到，对于“交互注意力”的输入，从编码器中输入的是两个，而解码器自注意力层中输入的是一个，读者可能会有疑问，对于注意力层的Query、Key和Value，到底是如何安排和处理的？
问题的解答还是要回归注意力层的定义：
\[ 
\begin{align*}
attention((K,V),q)&=\sum_{i = 1}^{N}a_{i}v_{i}\\
&=\sum_{i = 1}^{N}\frac{\exp(s(k_{i},q))}{\sum_{j = 1}^{N}\exp(s(k_{j},q))}v_{i}
\end{align*}
\]
实际上，就是使用Query首先计算与Key的权重，之后使用权重与Value携带的信息进行比较，从而将Value中的信息“融合”到Query中。
可以非常简单地得到，在交互注意力层中，解码器的自注意力词嵌入首先与编码器的输入词嵌入计算权重，然后使用计算出来的权重来计算编码器中的信息。即：
```python
query = 解码器词输入向量
key = 编码器词输出向量
value = 编码器词输出向量
```

#### 2）交互注意力中的掩码层（对谁进行掩码处理）
下面处理的是解码器中多头注意力的掩码层，相对于单一的自注意力层来说，一个非常显著的问题是对谁进行掩码处理。
对这个问题的解答需要重新回到注意力模型的定义：
\[ z_{i}=\text{Softmax}(scores)v \]
从权重的计算来看，解码器的词嵌入（Query）与编码器输入词嵌入（Key和Value）进行权重计算，从而将Query的值与Key和Value进行“融合”。基于这点考虑，选择对编码器输入的词嵌入进行掩码处理。
如果读者对此不理解，现在请记住：
```python
mask the encoder input embedding（对解码器中的编码器输出向量进行掩码操作）
```
有兴趣的读者可以自行查阅更多的资料进行了解。
下面两个函数分别展示普通掩码处理和在解码器中自注意力层掩码的写法：
```python
#创建解码器中的交互注意力掩码
def creat_self_mask(from_tensor, to_tensor):
    """
    这里需要注意，from_tensor是输入的文本序列，即input_word_ids，
    应该是2D的，即[1,2,3,4,5,6,0,0,0]
    to_tensor是输入的input_word_ids，应该是2D的，即[1,2,3,4,5,6,0,0,0]
    而经过本函数的扩充维度操作后，最终是输出两个3D的相乘后的结果
    注意：后面如果需要4D的，则使用expand添加一个维度即可
    """
    batch_size, from_seq_length = from_tensor.shape
    to_mask = torch.not_equal(from_tensor, 0).int()
    to_mask = elt.Rearrange('b l -> b l 1')(to_mask)  # 这里扩充了数据维度
    broadcast_ones = torch.ones_like(to_tensor)
    broadcast_ones = torch.unsqueeze(broadcast_ones, dim=-1)
    mask = broadcast_ones * to_mask
    mask.to("cuda")
    return mask
```
打印结果和演示请读者自行完成。
然而，如果需要进一步提高准确率的话，还需要对掩码进行处理：
```python
def create_look_ahead_mask(from_tensor, to_tensor):
    corss_mask = creat_self_mask(from_tensor, to_tensor)
    look_ahead_mask = torch.tril(torch.ones(to_tensor.shape[1], from_tensor.shape[1]))
    look_ahead_mask = look_ahead_mask.to("cuda")
    corss_mask = look_ahead_mask * corss_mask
    return corss_mask
```
下面的代码段合成了pad_mask和look_ahead_mask，并通过maximum函数建立与或门，将其合成为一体，即：
```
tf.Tensor(
[[[1. 0. 0. 0.]]
 [[1. 1. 0. 0.]]
 [[1. 1. 1. 0.]]
 [[1. 1. 1. 1.]]], shape=(4, 1, 1, 4), dtype=float32)
+
tf.Tensor(
[[0. 1. 1. 1.]
 [0. 0. 1. 1.]
 [0. 0. 0. 1.]
 [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)
=
tf.Tensor(
[[[1. 1. 1. 1.]
  [1. 0. 0. 1.]
  [1. 0. 0. 0.]]
 [[1. 1. 1. 1.]
  [1. 1. 0. 1.]
  [1. 1. 0. 0.]]
 [[1. 1. 1. 1.]
  [1. 1. 1. 1.]
  [1. 1. 0. 0.]]
 [[1. 1. 1. 1.]
  [1. 1. 1. 1.]
  [1. 1. 1. 1.]]]
```
这样的处理可以最大限度地对无用部分进行掩码操作，从而使得解码器的输入（Query）与编码器的输入（Key，Value）能够最大限度地融合在一起，减少干扰。

##### 12.1.2 为什么通过掩码操作能够减少干扰
为什么在注意力层中，通过掩码操作能够减少干扰？这是由于Query和Value在进行点积计算时会产生大量的负值，而负值在进行Softmax计算时，由于Softmax的计算特性，会对平衡产生影响，代码如下。
```python
【程序12-1】
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(self, Q, K, V, attn_mask):
        """
        Q: [batch_size, n_heads, len_q, d_k]
        K: [batch_size, n_heads, len_k, d_k]
        V: [batch_size, n_heads, len_v(=len_k), d_v]
        attn_mask: [batch_size, n_heads, seq_len, seq_len]
        """
        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)
        # scores : [batch_size, n_heads, len_q, len_k]
        scores.masked_fill_(attn_mask == 0, -1e9)
        # 对于Softmax没有影响的部分（即被掩码操作的部分），scores填充为负无穷，也就是这个位置的值对Softmax没有影响
        attn = nn.Softmax(dim=-1)(scores)
        # attn: [batch_size, n_heads, len_q, len_k]
        # 对每一行进行Softmax
        context = torch.matmul(attn, V)
        # [batch_size, n_heads, len_q, d_v]
        return context, attn
```
结果如图12 - 7所示。
```
tf.Tensor(
[[-2.149965   0.12186236 -0.9287065  0.58555082]
 [ 0.3933525 -1.19042299 -0.5511145  0.66039836]
 [-1.1108926  0.9996936 -0.12755463 0.37680746]
 [ 1.6570117 -0.46462783 0.10604692 -0.8762158 ]], shape=(4, 4), dtype=float32)
softmax
tf.Tensor(
[[0.0338944  0.32864466 0.1149399  0.52252906]
 [0.3425522  0.0709658  0.13455153 0.45189962]
 [0.02230938 0.50029165 0.20917896 0.26823465]
 [0.7085763  0.08469124 0.15024884 0.06526261]], shape=(4, 4), dtype=float32)
```
实际上是不需要这些负值的，因此需要在计算时加上一个“负无穷”的值降低负值对Softmax计算的影响（一般使用-1e5即可）。

##### 12.1.3 解码器的输出（移位训练方法）
前面两个小节介绍了解码器的一些基本操作，本小节将主要介绍解码器在最终阶段解码的变化和一些相关的细节，如图12 - 8所示。
解码器通过交互注意力的计算选择将当前的解码器词嵌入关注到编码器词嵌入中，选择生成一个新的词嵌入。
这是整体的步骤，当程序开始启动时，首先将编码器中的词嵌入全部输入，解码器首先接收一个起始符号的词嵌入，从而生成第一个解码的结果。
这种输入和输出错位的训练方法是“移位训练”方法。
接下来重复这个过程，每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果。直到到达一个特殊的终止符号，它表示编码器 - 解码器架构已经完成了它的输出。
还有一点需要补充，解码器栈输出一个词嵌入，那如何将其变成一个输出词呢？这是最后一个全连接层的工作，并使用Softmax对输出进行归类计算。
全连接层是一个简单的全连接神经网络，它将解码器栈产生的向量投影到另一个向量维度，维度的大小对应生成字库的个数。之后的Softmax层将维度数值转换为概率。选择概率最大的维度，并对应地生成与之关联的字或者词作为此时间步的输出。

##### 12.1.4 解码器的实现
本小节介绍解码器的实现。
首先，多注意力层实际上是通用的，代码如下。
```python
【程序12-2】
class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self):
        super(MultiHeadAttention, self).__init__()

    def build(self, input_shape):
        self.dense_query = tf.keras.layers.Dense(units=embedding_size, activation=tf.nn.relu)
        self.dense_key = tf.keras.layers.Dense(units=embedding_size, activation=tf.nn.relu)
        self.dense_value = tf.keras.layers.Dense(units=embedding_size, activation=tf.nn.relu)
        self.dense = tf.keras.layers.Dense(units=embedding_size, activation=tf.nn.relu)
        super(MultiHeadAttention, self).build(input_shape)  # 一定要在最后调用它

    def call(self, inputs):
        query, key, value, mask = inputs
        shape = tf.shape(query)

        query_dense = self.dense_query(query)
        key_dense = self.dense_query(key)
        value_dense = self.dense_query(value)

        query_dense = splite_tensor(query_dense)
        key_dense = splite_tensor(key_dense)
        value_dense = splite_tensor(value_dense)

        attention = tf.matmul(query_dense, key_dense, transpose_b=True) / tf.math.sqrt(tf.cast(embedding_size, tf.float32))

        attention += (mask* -1e

9)
attention = tf.nn.softmax(attention)
attention = tf.matmul(attention, value_dense)
attention = tf.transpose(attention, [0,2,1,3])
attention = tf.reshape(attention, [shape[0], -1, embedding_size])
attention = self.dense(attention)
return attention
```
其次，前馈层也可以通用，代码如下。
```python
【程序12-3】
class FeedForward(tf.keras.layers.Layer):
    def __init__(self):
        super(FeedForward, self).__init__()

    def build(self, input_shape):
        self.conv_1 = tf.keras.layers.Conv1D(embedding_size*4,1,activation=tf.nn.relu)
        self.conv_2 = tf.keras.layers.Conv1D(embedding_size,1,activation=tf.nn.relu)
        super(FeedForward, self).build(input_shape)  # 一定要在最后调用它

    def call(self, inputs):
        output = self.conv_1(inputs)
        output = self.conv_2(output)
        return output
```
综合利用多层注意力层和前馈层，实现了专用的解码器的程序设计，代码如下。
```python
【程序12-4】
class DecoderLayer(nn.Module):
    def __init__(self):
        super(DecoderLayer, self).__init__()
        self.dec_self_attn = MultiHeadAttention()
        self.dec_enc_attn = MultiHeadAttention()
        self.pos_ffn = PoswiseFeedForwardNet()

    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):
        """
        dec_inputs: [batch_size, tgt_len, d_model]
        enc_outputs: [batch_size, src_len, d_model]
        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]
        dec_enc_attn_mask: [batch_size, tgt_len, src_len]
        """
        # dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]
        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)
        # dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, n_heads, tgt_len, src_len]
        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)
        # encoder-decoder attention部分
        dec_outputs = self.pos_ffn(dec_outputs)  # [batch_size, tgt_len, d_model]
        # 特征提取
        return dec_outputs, dec_self_attn, dec_enc_attn
```

### 12.2 解码器实战——拼音汉字翻译模型
经过前面章节的学习，本节进入解码器实战——拼音汉字翻译模型。
前面的章节带领读者学习了注意力模型、前馈层以及掩码相关知识。这3部分内容共同构成了编码器 - 解码器架构的主要内容，共同组成的就是transformer这个基本架构，如图12 - 9所示。 
